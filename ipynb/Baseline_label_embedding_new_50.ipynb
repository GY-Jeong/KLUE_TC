{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Baseline_label_embedding_new_50.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1T-3_PT6SHtYJAhOiHLldUeMKWk9LaGWz","authorship_tag":"ABX9TyOgH1ZIXkSpHUXAn9T9acrh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d795e4c3384a4bf09055b77e8d4c9cc0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_30028b2da94b49539ef42ec4202288a1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2f5b62333d0f4025aad0f13f473b00c0","IPY_MODEL_63ca311bf8cf40d7904c26b6aff7d9ec"]}},"30028b2da94b49539ef42ec4202288a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2f5b62333d0f4025aad0f13f473b00c0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d4ddb55e05a847968ae2edc1589733c1","_dom_classes":[],"description":"Downloading:  15%","_model_name":"FloatProgressModel","bar_style":"","max":1346854671,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":196842496,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a4e6911897fc442184bc07e50f31916c"}},"63ca311bf8cf40d7904c26b6aff7d9ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7696320cf0ea41908b660aae9edc65d7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 197M/1.35G [00:09&lt;01:12, 15.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bb678cf2d31f46b79d4118b9eb2af751"}},"d4ddb55e05a847968ae2edc1589733c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a4e6911897fc442184bc07e50f31916c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7696320cf0ea41908b660aae9edc65d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bb678cf2d31f46b79d4118b9eb2af751":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbWmvczoKOwQ","executionInfo":{"status":"ok","timestamp":1627667137195,"user_tz":-540,"elapsed":355,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"9a06f379-7b41-4f16-dc32-5c2a4d43c434"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fri Jul 30 17:45:35 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    25W / 300W |      2MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4kthgVPqr5VC"},"source":["## Directory 설정, 구글 드라이브 import"]},{"cell_type":"code","metadata":{"id":"FOkMqa8hrHl_"},"source":["cur_dir = '/content/drive/MyDrive/KLUE_TC'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jx_d93v9rzQI"},"source":["## Utils"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2KbbvLPitR1N","executionInfo":{"status":"ok","timestamp":1627667155757,"user_tz":-540,"elapsed":17486,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"ed81d15d-3245-442c-e10c-bf46e6274119"},"source":["!pip install adamp\n","!pip install git+https://github.com/GY-Jeong/transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: adamp in /usr/local/lib/python3.7/dist-packages (0.3.0)\n","Collecting git+https://github.com/GY-Jeong/transformers\n","  Cloning https://github.com/GY-Jeong/transformers to /tmp/pip-req-build-6pr583nl\n","  Running command git clone -q https://github.com/GY-Jeong/transformers /tmp/pip-req-build-6pr583nl\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2019.12.20)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (0.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (0.0.45)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.6.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (0.10.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (5.4.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers==4.10.0.dev0) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.10.0.dev0) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0.dev0) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6oadxEi9rZ7x"},"source":["import os\n","import random\n","import torch\n","import numpy as np\n","from torch import nn\n","\n","from torch.optim import Adam, AdamW, SGD\n","from adamp import AdamP\n","from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR, ExponentialLR, CosineAnnealingWarmRestarts\n","\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup\n","\n","\n","def set_seeds(seed=42):\n","    # 랜덤 시드를 설정하여 매 코드를 실행할 때마다 동일한 결과를 얻게 합니다.\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","def save_checkpoint(state, model_dir, model_filename):\n","    print('saving model ...')\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","    # torch.save(state, os.path.join(model_dir, model_filename))\n","    torch.save(state, os.path.join(model_filename))\n","\n","\n","def get_optimizer(model, args):\n","    if args.optimizer == 'adam':\n","        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'adamW':\n","        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'adamP':\n","        optimizer = AdamP(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'SGD':\n","        optimizer = SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n","\n","    # 모든 parameter들의 grad값을 0으로 초기화\n","    optimizer.zero_grad()\n","\n","    return optimizer\n","\n","\n","def get_scheduler(optimizer, args):\n","    if args.scheduler == 'plateau':\n","        scheduler = ReduceLROnPlateau(optimizer, patience=args.plateau_patience, factor=args.plateau_factor, mode='max',\n","                                      verbose=True)\n","    elif args.scheduler == 'linear_warmup':\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n","                                                    num_training_steps=args.total_steps)\n","    elif args.scheduler == 'step_lr':\n","        scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n","    elif args.scheduler == 'exp_lr':\n","        scheduler = ExponentialLR(optimizer, gamma=args.gamma)\n","    elif args.scheduler == 'cosine_annealing':\n","        scheduler = CosineAnnealingLR(optimizer, T_max=args.t_max, eta_min=args.eta_min)\n","    elif args.scheduler == 'cosine_annealing_warmstart':\n","        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=args.T_0, T_mult=args.T_mult, eta_min=args.eta_min,\n","                                                last_epoch=-1)\n","\n","    return scheduler\n","\n","\n","def update_params(loss, model, optimizer, batch_idx, max_len, args):\n","    if args.gradient_accumulation:\n","        # normalize loss to account for batch accumulation\n","        loss = loss / args.accum_iter \n","\n","        # backward pass\n","        loss.backward()\n","\n","        # weights update\n","        if ((batch_idx + 1) % args.accum_iter == 0) or (batch_idx + 1 == max_len):\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","    else:\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","\n","def load_tokenizer(args):\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.tokenizer_name\n","        if args.tokenizer_name\n","        else args.model_name_or_path,\n","        use_fast=True,\n","    )\n","\n","    return tokenizer\n","\n","\n","def load_model(args, model_name=None):\n","    if not model_name:\n","        model_name = args.model_name\n","    model_path = os.path.join(args.model_dir, model_name)\n","    print(\"Loading Model from:\", model_path)\n","    # load_state = torch.load(model_path)\n","    load_state = torch.load(model_name)\n","\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        args.config_name\n","        if args.config_name\n","        else args.model_name_or_path,\n","    )\n","\n","    config.num_labels = 7\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","    ).to(args.device)\n","\n","    # model.classifier = nn.Sequential(\n","    #     nn.Dropout(p=0.3, inplace=False),\n","    #     nn.Linear(1024, 1024),\n","    #     nn.Tanh(),\n","    #     nn.Dropout(p=0.3, inplace=False),\n","    #     nn.Linear(1024, 512),\n","    #     nn.Tanh(),\n","    #     nn.Dropout(p=0.3, inplace=False),\n","    #     nn.Linear(512, 7),\n","    # )\n","\n","    model.load_state_dict(load_state['state_dict'], strict=True)\n","\n","    # model = model.to(args.device)\n","\n","    print(\"Loading Model from:\", model_path, \"...Finished.\")\n","\n","    return model\n","\n","\n","def get_model(args):\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        args.config_name\n","        if args.config_name\n","        else args.model_name_or_path,\n","    )\n","\n","    config.num_labels = 7\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","    ).to(args.device)\n","\n","    # model.classifier = nn.Sequential(\n","    #     nn.Dropout(p=0.3, inplace=False),\n","    #     nn.Linear(1024, 1024),\n","    #     nn.Tanh(),\n","    #     nn.Dropout(p=0.3, inplace=False),\n","    #     nn.Linear(1024, 512),\n","    #     nn.Tanh(),\n","    #     nn.Dropout(p=0.3, inplace=False),\n","    #     nn.Linear(512, 7),\n","    # )\n","\n","    # print(model)\n","    #model.classifier.dropout = nn.Dropout(p=0.3, inplace = False)\n","\n","    model = model.to(args.device)\n","\n","    return model\n","\n","\n","def get_loaders(args, train, valid, is_inference=False):\n","    pin_memory = True\n","    train_loader, valid_loader = None, None\n","\n","    if is_inference:\n","        test_dataset = YNAT_dataset(args, valid, is_inference)\n","        test_loader = torch.utils.data.DataLoader(test_dataset, num_workers=args.num_workers, shuffle=False,\n","                                                  batch_size=args.batch_size, pin_memory=pin_memory)\n","        return test_loader\n","\n","    if train is not None:\n","        train_dataset = YNAT_dataset(args, train, is_inference)\n","        train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=args.num_workers, shuffle=True,\n","                                                   batch_size=args.batch_size, pin_memory=pin_memory)\n","    if valid is not None:\n","        valid_dataset = YNAT_dataset(args, valid, is_inference)\n","        valid_loader = torch.utils.data.DataLoader(valid_dataset, num_workers=args.num_workers, shuffle=False,\n","                                                   batch_size=args.batch_size, pin_memory=pin_memory)\n","\n","    return train_loader, valid_loader\n","\n","\n","# loss계산하고 parameter update!\n","def compute_loss(preds, targets, args):\n","    \"\"\"\n","    Args :\n","        preds   : (batch_size, max_seq_len)\n","        targets : (batch_size, max_seq_len)\n","    \"\"\"\n","    # print(preds, targets)\n","    loss = get_criterion(preds, targets, args)\n","    # 마지막 시퀀스에 대한 값만 loss 계산\n","    # loss = loss[:, -1]\n","    # loss = torch.mean(loss)\n","    return loss\n","\n","\n","def get_criterion(pred, target, args):\n","    if args.criterion == 'BCE':\n","        loss = nn.BCELoss(reduction=\"none\")\n","    elif args.criterion == \"BCELogit\":\n","        loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    elif args.criterion == \"MSE\":\n","        loss = nn.MSELoss(reduction=\"none\")\n","    elif args.criterion == \"L1\":\n","        loss = nn.L1Loss(reduction=\"none\")\n","    elif args.criterion == \"CE\":\n","        #weights = [1,1,2,1,1,1,1] #as class distribution\n","        #class_weights = torch.FloatTensor(weights).cuda()\n","        #loss = nn.CrossEntropyLoss(weight=class_weights)\n","        loss = nn.CrossEntropyLoss()\n","    # NLL, CrossEntropy not available\n","    return loss(pred, target)\n","\n","\n","def make_vocab(args):\n","    print(\"============ READ VOCABS ============\")\n","    vocabs = []\n","    for i in range(7):\n","        vocab = set()\n","        f = open(args.vocab_dir + str(i) + '.txt', 'r')\n","        while True:\n","            line = f.readline()\n","            if not line: break\n","            vocab.add(line[:-1])\n","        f.close()\n","        vocabs.append(vocab)\n","        print(f\"category {i} reading end, size : {len(vocab)}\")\n","    return vocabs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mLM-aadds6H9"},"source":["## Dataloader"]},{"cell_type":"code","metadata":{"id":"2s9RxMi7rlfb"},"source":["import os\n","import torch\n","import pandas as pd\n","\n","\n","class Preprocess:\n","    def __init__(self, args):\n","        self.args = args\n","        self.train_data = None\n","        self.test_data = None\n","\n","    def load_data(self, file_name):\n","        csv_file_name = os.path.join(self.args.data_dir, file_name)\n","        df = pd.read_csv(csv_file_name)\n","        #del df['Unnamed: 0']\n","        return df.values\n","\n","    def load_train_data(self):\n","        self.train_data = self.load_data('train_data.csv')\n","\n","    def load_test_data(self):\n","        self.test_data = self.load_data('test_data.csv')\n","\n","\n","class YNAT_dataset(torch.utils.data.Dataset):\n","    def __init__(self, args, data, is_inference):\n","        self.args = args\n","        self.data = data\n","        self.is_inference = is_inference\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        row = self.data[index]\n","        element = [row[i] for i in range(len(row))]\n","        #print(type(row))\n","        # np.array -> torch.tensor 형변환\n","        #for i, col in enumerate(row):\n","        #    if type(col) == str:\n","        #        pass\n","        #    else:\n","        #        row[i] = torch.tensor(col)\n","\n","        return element\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2g9iLEBtDnJ"},"source":["## Trainer"]},{"cell_type":"code","metadata":{"id":"Xq3sntmNtErb"},"source":["from sklearn.metrics import accuracy_score\n","from torch.nn.functional import one_hot\n","from tqdm import tqdm\n","from sklearn import metrics\n","\n","\n","def run(args, tokenizer, train_data, valid_data, cv_count):\n","    train_loader, valid_loader = get_loaders(args, train_data, valid_data)\n","\n","    # only when using warmup scheduler\n","    # args.total_steps = int(len(train_loader.dataset) / args.batch_size) * args.n_epochs\n","    # args.warmup_steps = int(args.total_steps * args.warmup_ratio)\n","\n","    model = get_model(args)\n","    optimizer = get_optimizer(model, args)\n","    scheduler = get_scheduler(optimizer, args)\n","\n","    best_acc = -1\n","    early_stopping_counter = 0\n","    for epoch in range(args.n_epochs):\n","\n","        print(f\"Start Training: Epoch {epoch + 1}\")\n","\n","        if not args.cv_strategy:\n","            model_name = args.run_name\n","        else:\n","            model_name = f\"{args.run_name.split('.pt')[0]}_{cv_count}.pt\"\n","\n","        # TRAIN\n","        train_acc, train_loss = train(args, model, tokenizer, train_loader, optimizer)\n","\n","        # VALID\n","        acc, val_loss = validate(args, model, tokenizer, valid_loader)\n","\n","        # TODO: model save or early stopping\n","        if args.scheduler == 'plateau':\n","            last_lr = optimizer.param_groups[0]['lr']\n","        else:\n","            last_lr = scheduler.get_last_lr()[0]\n","\n","        print({\"epoch\": epoch, \"train_loss\": train_loss, \"train_acc\": train_acc,\n","                   \"valid_acc\": acc, \"val_loss\": val_loss, \"learning_rate\": last_lr})\n","\n","        if acc > best_acc:\n","            best_acc = acc\n","            # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n","            model_to_save = model.module if hasattr(model, 'module') else model\n","            save_checkpoint({\n","                'epoch': epoch + 1,\n","                'state_dict': model_to_save.state_dict(),\n","            },\n","                args.model_dir, model_name,\n","            )\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","            if early_stopping_counter >= args.patience:\n","                print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n","                break\n","\n","        # scheduler\n","        if args.scheduler == 'plateau':\n","            scheduler.step(best_acc)\n","        else:\n","            scheduler.step()\n","\n","    return best_acc\n","\n","\n","def inference(args, test_data):\n","    # ckpt_file_names = []\n","    all_fold_preds = []\n","    all_fold_argmax_preds = []\n","\n","    if not args.cv_strategy:\n","        ckpt_file_names = [args.model_name]\n","    else:\n","        ckpt_file_names = [f\"{args.model_name.split('.pt')[0]}_{i + 1}.pt\" for i in range(args.fold_num)]\n","\n","    tokenizer = load_tokenizer(args)\n","\n","    for fold_idx, ckpt in enumerate(ckpt_file_names):\n","        model = load_model(args, ckpt)\n","        model.eval()\n","        test_loader = get_loaders(args, None, test_data, True)\n","\n","        total_preds = []\n","        total_argmax_preds = []\n","        total_ids = []\n","\n","        for step, batch in tqdm(enumerate(test_loader), desc='Inferencing', total=len(test_loader)):\n","            idx, text = batch\n","            tokenized_examples = tokenizer(\n","                text,\n","                max_length=args.max_seq_len,\n","                padding=\"max_length\",\n","                return_tensors=\"pt\"\n","            ).to(args.device)\n","\n","            token_label_0_type_ids = []\n","            token_label_1_type_ids = []\n","            token_label_2_type_ids = []\n","            token_label_3_type_ids = []\n","            token_label_4_type_ids = []\n","            token_label_5_type_ids = []\n","            token_label_6_type_ids = []\n","\n","            for row in tokenized_examples['input_ids']:\n","                row = tokenizer.convert_ids_to_tokens(row)\n","                # print(row)\n","                label_0 = [1 if token in args.vocab[0] else 0 for token in row]\n","                label_1 = [1 if token in args.vocab[1] else 0 for token in row]\n","                label_2 = [1 if token in args.vocab[2] else 0 for token in row]\n","                label_3 = [1 if token in args.vocab[3] else 0 for token in row]\n","                label_4 = [1 if token in args.vocab[4] else 0 for token in row]\n","                label_5 = [1 if token in args.vocab[5] else 0 for token in row]\n","                label_6 = [1 if token in args.vocab[6] else 0 for token in row]\n","                # print(label_2)\n","                token_label_0_type_ids.append(label_0)\n","                token_label_1_type_ids.append(label_1)\n","                token_label_2_type_ids.append(label_2)\n","                token_label_3_type_ids.append(label_3)\n","                token_label_4_type_ids.append(label_4)\n","                token_label_5_type_ids.append(label_5)\n","                token_label_6_type_ids.append(label_6)\n","                # temp = []\n","                # row = tokenizer.convert_ids_to_tokens(row)\n","                # for token in row:\n","                #     for i, element in enumerate(args.vocab):\n","                #         if token in element:\n","                #             temp.append(i+1)\n","                #             break\n","                #     else:\n","                #         temp.append(0)\n","                # #print(temp)\n","                # token_label_type_ids.append(temp)\n","\n","            # token_label_type_ids = torch.tensor(token_label_type_ids).to(args.device)\n","            # tokenized_examples['token_label_type_ids'] = token_label_type_ids\n","\n","            token_label_0_type_ids = torch.tensor(token_label_0_type_ids).to(args.device)\n","            token_label_1_type_ids = torch.tensor(token_label_1_type_ids).to(args.device)\n","            token_label_2_type_ids = torch.tensor(token_label_2_type_ids).to(args.device)\n","            token_label_3_type_ids = torch.tensor(token_label_3_type_ids).to(args.device)\n","            token_label_4_type_ids = torch.tensor(token_label_4_type_ids).to(args.device)\n","            token_label_5_type_ids = torch.tensor(token_label_5_type_ids).to(args.device)\n","            token_label_6_type_ids = torch.tensor(token_label_6_type_ids).to(args.device)\n","\n","            tokenized_examples['token_label_0_type_ids'] = token_label_0_type_ids\n","            tokenized_examples['token_label_1_type_ids'] = token_label_1_type_ids\n","            tokenized_examples['token_label_2_type_ids'] = token_label_2_type_ids\n","            tokenized_examples['token_label_3_type_ids'] = token_label_3_type_ids\n","            tokenized_examples['token_label_4_type_ids'] = token_label_4_type_ids\n","            tokenized_examples['token_label_5_type_ids'] = token_label_5_type_ids\n","            tokenized_examples['token_label_6_type_ids'] = token_label_6_type_ids\n","\n","            # token_label_type_ids = []\n","            # for row in tokenized_examples['input_ids']:\n","            #     temp = []\n","            #     row = tokenizer.convert_ids_to_tokens(row)\n","            #     for token in row:\n","            #         for i, element in enumerate(args.vocab):\n","            #             if token in element:\n","            #                 temp.append(i+1)\n","            #                 break\n","            #         else:\n","            #             temp.append(0)\n","            #     #print(temp)\n","            #     token_label_type_ids.append(temp)\n","\n","            # token_label_type_ids = torch.tensor(token_label_type_ids).to(args.device)\n","            # tokenized_examples['token_label_type_ids'] = token_label_type_ids\n","\n","            preds = model(**tokenized_examples)\n","            logits = preds['logits']\n","            #logits = logits[:,0,:]\n","            argmax_logits = torch.argmax(logits, dim=1)\n","\n","            if args.device == 'cuda':\n","                argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","                label = label.to('cpu').detach().numpy()\n","                loss = loss.to('cpu').detach().numpy()\n","                # token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","                token_label_0_type_ids = token_label_0_type_ids.to('cpu').detach().numpy()\n","                token_label_1_type_ids = token_label_1_type_ids.to('cpu').detach().numpy()\n","                token_label_2_type_ids = token_label_2_type_ids.to('cpu').detach().numpy()\n","                token_label_3_type_ids = token_label_3_type_ids.to('cpu').detach().numpy()\n","                token_label_4_type_ids = token_label_4_type_ids.to('cpu').detach().numpy()\n","                token_label_5_type_ids = token_label_5_type_ids.to('cpu').detach().numpy()\n","                token_label_6_type_ids = token_label_6_type_ids.to('cpu').detach().numpy()\n","            else:  # cpu\n","                argmax_logits = argmax_logits.detach().numpy()\n","                label = label.detach().numpy()\n","                loss = loss.detach().numpy()\n","                # token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","                token_label_0_type_ids = token_label_0_type_ids.detach().numpy()\n","                token_label_1_type_ids = token_label_1_type_ids.detach().numpy()\n","                token_label_2_type_ids = token_label_2_type_ids.detach().numpy()\n","                token_label_3_type_ids = token_label_3_type_ids.detach().numpy()\n","                token_label_4_type_ids = token_label_4_type_ids.detach().numpy()\n","                token_label_5_type_ids = token_label_5_type_ids.detach().numpy()\n","                token_label_6_type_ids = token_label_6_type_ids.detach().numpy()\n","\n","            total_preds += list(preds)\n","            total_argmax_preds += list(argmax_preds)\n","            total_ids += list(idx)\n","\n","        all_fold_preds.append(total_preds)\n","        all_fold_argmax_preds.append(total_argmax_preds)\n","\n","        output_file_name = \"output.csv\" if not args.cv_strategy else f\"output_{fold_idx + 1}.csv\"\n","        write_path = os.path.join(args.output_dir, output_file_name)\n","        if not os.path.exists(args.output_dir):\n","            os.makedirs(args.output_dir)\n","        with open(write_path, 'w', encoding='utf8') as w:\n","            print(\"writing prediction : {}\".format(write_path))\n","            w.write(\"index,topic_idx\\n\")\n","            for index, p in zip(total_ids, total_argmax_preds):\n","                w.write('{},{}\\n'.format(index, p))\n","\n","    if len(all_fold_preds) > 1:\n","        # Soft voting ensemble\n","        votes = np.sum(all_fold_preds, axis=0)\n","        votes = np.argmax(votes, axis=1)\n","\n","        write_path = os.path.join(args.output_dir, \"output_softvote.csv\")\n","        #write_path = \"output_softvote.csv\"\n","        if not os.path.exists(args.output_dir):\n","            os.makedirs(args.output_dir)\n","        with open(write_path, 'w', encoding='utf8') as w:\n","            print(\"writing prediction : {}\".format(write_path))\n","            w.write(\"index,topic_idx\\n\")\n","            for id, p in zip(total_ids, votes):\n","                w.write('{},{}\\n'.format(id, p))\n","\n","\n","def train(args, model, tokenizer, train_loader, optimizer):\n","    model.train()\n","\n","    total_preds = []\n","    total_targets = []\n","    losses = []\n","    for step, batch in tqdm(enumerate(train_loader), desc='Training', total=len(train_loader)):\n","        idx, text, label = batch\n","        label = label.to(args.device)\n","        # print(idx[:10])\n","        # print(text[:10])\n","        # print(label[:10])\n","        tokenized_examples = tokenizer(\n","            text,\n","            max_length=args.max_seq_len,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        ).to(args.device)\n","        \n","        # tokenize\n","        # 모델의 입력으로\n","        # label은 one-hot?\n","        # loss 주고\n","        # argmax를 golden\n","\n","        token_label_0_type_ids = []\n","        token_label_1_type_ids = []\n","        token_label_2_type_ids = []\n","        token_label_3_type_ids = []\n","        token_label_4_type_ids = []\n","        token_label_5_type_ids = []\n","        token_label_6_type_ids = []\n","\n","        for row in tokenized_examples['input_ids']:\n","            row = tokenizer.convert_ids_to_tokens(row)\n","            # print(row)\n","            label_0 = [1 if token in args.vocab[0] else 0 for token in row]\n","            label_1 = [1 if token in args.vocab[1] else 0 for token in row]\n","            label_2 = [1 if token in args.vocab[2] else 0 for token in row]\n","            label_3 = [1 if token in args.vocab[3] else 0 for token in row]\n","            label_4 = [1 if token in args.vocab[4] else 0 for token in row]\n","            label_5 = [1 if token in args.vocab[5] else 0 for token in row]\n","            label_6 = [1 if token in args.vocab[6] else 0 for token in row]\n","            # print(label_2)\n","            token_label_0_type_ids.append(label_0)\n","            token_label_1_type_ids.append(label_1)\n","            token_label_2_type_ids.append(label_2)\n","            token_label_3_type_ids.append(label_3)\n","            token_label_4_type_ids.append(label_4)\n","            token_label_5_type_ids.append(label_5)\n","            token_label_6_type_ids.append(label_6)\n","            # temp = []\n","            # row = tokenizer.convert_ids_to_tokens(row)\n","            # for token in row:\n","            #     for i, element in enumerate(args.vocab):\n","            #         if token in element:\n","            #             temp.append(i+1)\n","            #             break\n","            #     else:\n","            #         temp.append(0)\n","            # #print(temp)\n","            # token_label_type_ids.append(temp)\n","\n","        # token_label_type_ids = torch.tensor(token_label_type_ids).to(args.device)\n","        # tokenized_examples['token_label_type_ids'] = token_label_type_ids\n","\n","        token_label_0_type_ids = torch.tensor(token_label_0_type_ids).to(args.device)\n","        token_label_1_type_ids = torch.tensor(token_label_1_type_ids).to(args.device)\n","        token_label_2_type_ids = torch.tensor(token_label_2_type_ids).to(args.device)\n","        token_label_3_type_ids = torch.tensor(token_label_3_type_ids).to(args.device)\n","        token_label_4_type_ids = torch.tensor(token_label_4_type_ids).to(args.device)\n","        token_label_5_type_ids = torch.tensor(token_label_5_type_ids).to(args.device)\n","        token_label_6_type_ids = torch.tensor(token_label_6_type_ids).to(args.device)\n","\n","        tokenized_examples['token_label_0_type_ids'] = token_label_0_type_ids\n","        tokenized_examples['token_label_1_type_ids'] = token_label_1_type_ids\n","        tokenized_examples['token_label_2_type_ids'] = token_label_2_type_ids\n","        tokenized_examples['token_label_3_type_ids'] = token_label_3_type_ids\n","        tokenized_examples['token_label_4_type_ids'] = token_label_4_type_ids\n","        tokenized_examples['token_label_5_type_ids'] = token_label_5_type_ids\n","        tokenized_examples['token_label_6_type_ids'] = token_label_6_type_ids\n","\n","        preds = model(**tokenized_examples, labels = label)\n","        # print(preds)\n","        logits = preds['logits']\n","        # logits = logits[:,0,:]\n","        softmax_logits = nn.Softmax(dim=1)(logits)\n","        argmax_logits = torch.argmax(logits, dim=1)\n","\n","        # one_hot_logits = one_hot(argmax_logits, num_classes=7).float()\n","        # print(one_hot(argmax_logits, num_classes=7).type(torch.FloatTensor))\n","        # loss = compute_loss(logits,\n","        #                     label, args)\n","        loss = preds['loss']\n","        # print(loss)\n","\n","        update_params(loss, model, optimizer, step, len(train_loader), args)\n","\n","        if step % args.log_steps == 0:\n","            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n","\n","        if args.device == 'cuda':\n","            argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","            label = label.to('cpu').detach().numpy()\n","            loss = loss.to('cpu').detach().numpy()\n","            # token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","            token_label_0_type_ids = token_label_0_type_ids.to('cpu').detach().numpy()\n","            token_label_1_type_ids = token_label_1_type_ids.to('cpu').detach().numpy()\n","            token_label_2_type_ids = token_label_2_type_ids.to('cpu').detach().numpy()\n","            token_label_3_type_ids = token_label_3_type_ids.to('cpu').detach().numpy()\n","            token_label_4_type_ids = token_label_4_type_ids.to('cpu').detach().numpy()\n","            token_label_5_type_ids = token_label_5_type_ids.to('cpu').detach().numpy()\n","            token_label_6_type_ids = token_label_6_type_ids.to('cpu').detach().numpy()\n","        else:  # cpu\n","            argmax_logits = argmax_logits.detach().numpy()\n","            label = label.detach().numpy()\n","            loss = loss.detach().numpy()\n","            # token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","            token_label_0_type_ids = token_label_0_type_ids.detach().numpy()\n","            token_label_1_type_ids = token_label_1_type_ids.detach().numpy()\n","            token_label_2_type_ids = token_label_2_type_ids.detach().numpy()\n","            token_label_3_type_ids = token_label_3_type_ids.detach().numpy()\n","            token_label_4_type_ids = token_label_4_type_ids.detach().numpy()\n","            token_label_5_type_ids = token_label_5_type_ids.detach().numpy()\n","            token_label_6_type_ids = token_label_6_type_ids.detach().numpy()\n","\n","        total_preds.append(argmax_logits)\n","        total_targets.append(label)\n","        losses.append(loss)\n","\n","    total_preds = np.concatenate(total_preds)\n","    total_targets = np.concatenate(total_targets)\n","\n","    # Train AUC / ACC\n","    acc = accuracy_score(total_targets, total_preds)\n","    loss_avg = sum(losses) / len(losses)\n","    print(f'TRAIN ACC : {acc}, TRAIN LOSS : {loss_avg}')\n","    return acc, loss_avg\n","\n","\n","def validate(args, model, tokenizer, valid_loader):\n","    model.eval()\n","\n","    total_preds = []\n","    total_targets = []\n","    losses = []\n","    for step, batch in tqdm(enumerate(valid_loader), desc='Training', total=len(valid_loader)):\n","        idx, text, label = batch\n","        label = label.to(args.device)\n","        tokenized_examples = tokenizer(\n","            text,\n","            max_length=args.max_seq_len,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        ).to(args.device)\n","\n","        # tokenize\n","        # 모델의 입력으로\n","        # label은 one-hot?\n","        # loss 주고\n","        # argmax를 golden\n","\n","        token_label_0_type_ids = []\n","        token_label_1_type_ids = []\n","        token_label_2_type_ids = []\n","        token_label_3_type_ids = []\n","        token_label_4_type_ids = []\n","        token_label_5_type_ids = []\n","        token_label_6_type_ids = []\n","\n","        for row in tokenized_examples['input_ids']:\n","            row = tokenizer.convert_ids_to_tokens(row)\n","            # print(row)\n","            label_0 = [1 if token in args.vocab[0] else 0 for token in row]\n","            label_1 = [1 if token in args.vocab[1] else 0 for token in row]\n","            label_2 = [1 if token in args.vocab[2] else 0 for token in row]\n","            label_3 = [1 if token in args.vocab[3] else 0 for token in row]\n","            label_4 = [1 if token in args.vocab[4] else 0 for token in row]\n","            label_5 = [1 if token in args.vocab[5] else 0 for token in row]\n","            label_6 = [1 if token in args.vocab[6] else 0 for token in row]\n","            # print(label_2)\n","            token_label_0_type_ids.append(label_0)\n","            token_label_1_type_ids.append(label_1)\n","            token_label_2_type_ids.append(label_2)\n","            token_label_3_type_ids.append(label_3)\n","            token_label_4_type_ids.append(label_4)\n","            token_label_5_type_ids.append(label_5)\n","            token_label_6_type_ids.append(label_6)\n","            # temp = []\n","            # row = tokenizer.convert_ids_to_tokens(row)\n","            # for token in row:\n","            #     for i, element in enumerate(args.vocab):\n","            #         if token in element:\n","            #             temp.append(i+1)\n","            #             break\n","            #     else:\n","            #         temp.append(0)\n","            # #print(temp)\n","            # token_label_type_ids.append(temp)\n","\n","        # token_label_type_ids = torch.tensor(token_label_type_ids).to(args.device)\n","        # tokenized_examples['token_label_type_ids'] = token_label_type_ids\n","\n","        token_label_0_type_ids = torch.tensor(token_label_0_type_ids).to(args.device)\n","        token_label_1_type_ids = torch.tensor(token_label_1_type_ids).to(args.device)\n","        token_label_2_type_ids = torch.tensor(token_label_2_type_ids).to(args.device)\n","        token_label_3_type_ids = torch.tensor(token_label_3_type_ids).to(args.device)\n","        token_label_4_type_ids = torch.tensor(token_label_4_type_ids).to(args.device)\n","        token_label_5_type_ids = torch.tensor(token_label_5_type_ids).to(args.device)\n","        token_label_6_type_ids = torch.tensor(token_label_6_type_ids).to(args.device)\n","\n","        tokenized_examples['token_label_0_type_ids'] = token_label_0_type_ids\n","        tokenized_examples['token_label_1_type_ids'] = token_label_1_type_ids\n","        tokenized_examples['token_label_2_type_ids'] = token_label_2_type_ids\n","        tokenized_examples['token_label_3_type_ids'] = token_label_3_type_ids\n","        tokenized_examples['token_label_4_type_ids'] = token_label_4_type_ids\n","        tokenized_examples['token_label_5_type_ids'] = token_label_5_type_ids\n","        tokenized_examples['token_label_6_type_ids'] = token_label_6_type_ids\n","\n","        # token_label_type_ids = []\n","        # for row in tokenized_examples['input_ids']:\n","        #     temp = []\n","        #     row = tokenizer.convert_ids_to_tokens(row)\n","        #     for token in row:\n","        #         for i, element in enumerate(args.vocab):\n","        #             if token in element:\n","        #                 temp.append(i+1)\n","        #                 break\n","        #         else:\n","        #             temp.append(0)\n","        #     #print(temp)\n","        #     token_label_type_ids.append(temp)\n","\n","        # token_label_type_ids = torch.tensor(token_label_type_ids).to(args.device)\n","        # tokenized_examples['token_label_type_ids'] = token_label_type_ids\n","\n","        preds = model(**tokenized_examples, labels = label)\n","        logits = preds['logits']\n","        # logits = logits[:,0,:]\n","        softmax_logits = nn.Softmax(dim=1)(logits)\n","        argmax_logits = torch.argmax(logits, dim=1)\n","\n","        # one_hot_logits = one_hot(argmax_logits, num_classes=7).float()\n","        # print(one_hot(argmax_logits, num_classes=7).type(torch.FloatTensor))\n","        # loss = compute_loss(logits,\n","        #                     label, args)\n","        loss = preds['loss']\n","\n","        if step % args.log_steps == 0:\n","            print(f\"Validation steps: {step} Loss: {str(loss.item())}\")\n","\n","        # if args.device == 'cuda':\n","        #     argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","        #     label = label.to('cpu').detach().numpy()\n","        #     loss = loss.to('cpu').detach().numpy()\n","        #     token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","        # else:  # cpu\n","        #     argmax_logits = argmax_logits.detach().numpy()\n","        #     label = label.detach().numpy()\n","        #     loss = loss.detach().numpy()\n","        #     token_label_type_ids = token_label_type_ids.detach().numpy()\n","\n","        if args.device == 'cuda':\n","            argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","            label = label.to('cpu').detach().numpy()\n","            loss = loss.to('cpu').detach().numpy()\n","            # token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","            token_label_0_type_ids = token_label_0_type_ids.to('cpu').detach().numpy()\n","            token_label_1_type_ids = token_label_1_type_ids.to('cpu').detach().numpy()\n","            token_label_2_type_ids = token_label_2_type_ids.to('cpu').detach().numpy()\n","            token_label_3_type_ids = token_label_3_type_ids.to('cpu').detach().numpy()\n","            token_label_4_type_ids = token_label_4_type_ids.to('cpu').detach().numpy()\n","            token_label_5_type_ids = token_label_5_type_ids.to('cpu').detach().numpy()\n","            token_label_6_type_ids = token_label_6_type_ids.to('cpu').detach().numpy()\n","        else:  # cpu\n","            argmax_logits = argmax_logits.detach().numpy()\n","            label = label.detach().numpy()\n","            loss = loss.detach().numpy()\n","            # token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","            token_label_0_type_ids = token_label_0_type_ids.detach().numpy()\n","            token_label_1_type_ids = token_label_1_type_ids.detach().numpy()\n","            token_label_2_type_ids = token_label_2_type_ids.detach().numpy()\n","            token_label_3_type_ids = token_label_3_type_ids.detach().numpy()\n","            token_label_4_type_ids = token_label_4_type_ids.detach().numpy()\n","            token_label_5_type_ids = token_label_5_type_ids.detach().numpy()\n","            token_label_6_type_ids = token_label_6_type_ids.detach().numpy()\n","        total_preds.append(argmax_logits)\n","        total_targets.append(label)\n","        losses.append(loss)\n","\n","    total_preds = np.concatenate(total_preds)\n","    total_targets = np.concatenate(total_targets)\n","\n","    # Train AUC / ACC\n","    target_names = ['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치']\n","    print(metrics.classification_report(total_targets, total_preds, target_names=target_names))\n","    matrix = metrics.confusion_matrix(total_targets, total_preds)\n","    print(matrix.diagonal()/matrix.sum(axis=1))\n","\n","    acc = accuracy_score(total_targets, total_preds)\n","    loss_avg = sum(losses) / len(losses)\n","    print(f'VALID ACC : {acc}, VALID LOSS : {loss_avg}')\n","    return acc, loss_avg\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5CS14MP9tFCQ"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"K9DdutwStF4B"},"source":["import torch\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","from datetime import datetime\n","from pytz import timezone\n","\n","\n","def main(args):\n","    if not args.run_name:\n","        args.run_name = datetime.now(timezone(\"Asia/Seoul\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n","\n","    set_seeds(args.seed)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    args.device = device\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.tokenizer_name\n","        if args.tokenizer_name\n","        else args.model_name_or_path,\n","        use_fast=True,\n","    )\n","\n","    preprocess = Preprocess(args)\n","    preprocess.load_train_data()\n","    train_data_origin = preprocess.train_data\n","\n","    print(f\"Size of train data : {len(train_data_origin)}\")\n","    # print(f\"size of test data : {len(test_data)}\")\n","\n","    if args.cv_strategy == 'random':\n","        kf = KFold(n_splits=args.fold_num, shuffle=True)\n","        splits = kf.split(X=train_data_origin)\n","    else:\n","        # default\n","        # 여기 각 label로 바꿔야됨\n","        train_labels = [sequence[-1] for sequence in train_data_origin]\n","        skf = StratifiedKFold(n_splits=args.fold_num, shuffle=True)\n","        splits = skf.split(X=train_data_origin, y=train_labels)\n","\n","    acc_avg = 0\n","    for fold_num, (train_index, valid_index) in enumerate(splits):\n","        train_data = train_data_origin[train_index]\n","        valid_data = train_data_origin[valid_index]\n","        best_acc = run(args, tokenizer, train_data, valid_data, fold_num + 1)\n","\n","        if not args.cv_strategy:\n","            break\n","\n","        acc_avg += best_acc\n","\n","    if args.cv_strategy:\n","        acc_avg /= args.fold_num\n","\n","        print(\"*\" * 50, 'auc_avg', \"*\" * 50)\n","        print(acc_avg)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1EgudubtKjJ"},"source":["## Run"]},{"cell_type":"code","metadata":{"id":"_xupDv9YtKGf"},"source":["import argparse\n","import easydict\n","\n","def parse_args():\n","    args = easydict.EasyDict({'run_name' : 'temp',\n","                             'seed':42,\n","                             'device' :'cuda',\n","                             'data_dir': cur_dir + '/data/open/',\n","                             'model_dir' : '/content/drive/MyDrive/KLUE_TC/models/vocab20',\n","                             'model_name_or_path' : 'klue/roberta-large',\n","                             'config_name' : None,\n","                             'tokenizer_name' : None,\n","                             'output_dir' : '/content/drive/MyDrive/KLUE_TC/output/vocab50_multi_layer',\n","                             'vocab_dir' : '/content/drive/MyDrive/KLUE_TC/data/vocab/vocab50/',\n","                             \n","                             'accum_iter' : 8,\n","                             'gradient_accumulation' : True,\n","\n","                             'cv_strategy' : 'stratified',\n","                             'fold_num' : 4,\n","\n","                             'num_workers' : 1,\n","\n","                             # 훈련\n","                             'n_epochs' : 10,\n","                             'batch_size' : 32,\n","                             'lr' : 5e-5,\n","                             'clip_grad' : 15,\n","                             'patience' : 3,\n","                             'max_seq_len' : 40,\n","\n","                             # Optimizer\n","                             'optimizer' : 'adamP',\n","\n","                             # Optimizer-parameters\n","                             'weight_decay' : 0.05,\n","                             'momentum' : 0.9,\n","\n","                             # Scheduler\n","                             'scheduler' : 'step_lr',\n","\n","                             # Scheduler-parameters\n","                             # plateau\n","                             'plateau_patience' : 10,\n","                             'plateau_factor' : 0.5,\n","                              \n","                             't_max' : 10,\n","                             'T_0' : 10,\n","                             'T_mult' : 2,\n","                             '--eta_min' : 0.01,\n","\n","                             # linear_warmup\n","                             'warmup_ratio' : 0.3,\n","\n","                             # Step LR\n","                             'step_size' : 50,\n","                             'gamma' : 0.1,\n","\n","                             'criterion' : 'CE',\n","\n","                             'log_steps' : 100})\n","    \n","    return args"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211,"referenced_widgets":["d795e4c3384a4bf09055b77e8d4c9cc0","30028b2da94b49539ef42ec4202288a1","2f5b62333d0f4025aad0f13f473b00c0","63ca311bf8cf40d7904c26b6aff7d9ec","d4ddb55e05a847968ae2edc1589733c1","a4e6911897fc442184bc07e50f31916c","7696320cf0ea41908b660aae9edc65d7","bb678cf2d31f46b79d4118b9eb2af751"]},"id":"YKuE65Ct3d34","outputId":"7563de02-ca7b-4fcc-9ce3-3ded67854b4f"},"source":["if __name__ == '__main__':\n","    args = parse_args()\n","    args['vocab'] = make_vocab(args)\n","    main(args)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["============ READ VOCABS ============\n","category 0 reading end, size : 1446\n","category 1 reading end, size : 1522\n","category 2 reading end, size : 2107\n","category 3 reading end, size : 1853\n","category 4 reading end, size : 2006\n","category 5 reading end, size : 1527\n","category 6 reading end, size : 1692\n","Size of train data : 45654\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d795e4c3384a4bf09055b77e8d4c9cc0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1346854671.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"YfVIY2z0kZZx"},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txpJN5vFPWCq"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"hUWUggp_PQqy"},"source":["def inference_main():\n","    args = parse_args()\n","    args['vocab'] = make_vocab(args)\n","    args.model_name = \"temp\"\n","    preprocess = Preprocess(args)\n","    preprocess.load_test_data()\n","    test_data = preprocess.test_data\n","\n","    print(f\"size of test data : {len(test_data)}\")\n","    torch.cuda.empty_cache()\n","    # del model\n","    inference(args, test_data)\n","\n","inference_main()"],"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Baseline_GPT2.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1XLwaT6ysIbn13zK_Df6OUz3pC5GxCm1D","authorship_tag":"ABX9TyPZ9/CCvml33SE26BFczVii"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e9ae994c8e584490a8c28429afc284c8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f18f8250df5945248b965c03b6291cbb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d71a0fa3bc2d47ae9818e36dbb7e4265","IPY_MODEL_9f3a1789d44141a9ad3b98619d6de10a"]}},"f18f8250df5945248b965c03b6291cbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d71a0fa3bc2d47ae9818e36dbb7e4265":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6d329c4ae7dd488b894d4b3af902b2c5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1200,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1200,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_67f1f8c6b04a42f6bfe64353af4f7808"}},"9f3a1789d44141a9ad3b98619d6de10a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_675ff7d6642a4921a6b4eefcdb381c99","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.20k/1.20k [00:00&lt;00:00, 3.18kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d148cdf7784d49a6ae41e65177921bd3"}},"6d329c4ae7dd488b894d4b3af902b2c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"67f1f8c6b04a42f6bfe64353af4f7808":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"675ff7d6642a4921a6b4eefcdb381c99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d148cdf7784d49a6ae41e65177921bd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"01d5a9d5aad0477fad487578334ec6a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_494d3c04561f4e6a8d42e3a3e22946d0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_584a0c48118b4e8d9f13ce7783edbbd4","IPY_MODEL_e49bc0ce5b9f498387871b2e08dcd61b"]}},"494d3c04561f4e6a8d42e3a3e22946d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"584a0c48118b4e8d9f13ce7783edbbd4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_776bc8df87104fcf8b7d7439d3663a00","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":791656,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":791656,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_17fc9a40fdf04f2a91b4f452b45dc57d"}},"e49bc0ce5b9f498387871b2e08dcd61b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4b6f5dfc26e446e7a2c28c46b1fee894","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 792k/792k [00:00&lt;00:00, 7.04MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c5d8033304ac4cf9917b4fb78c67148b"}},"776bc8df87104fcf8b7d7439d3663a00":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"17fc9a40fdf04f2a91b4f452b45dc57d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b6f5dfc26e446e7a2c28c46b1fee894":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c5d8033304ac4cf9917b4fb78c67148b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f7ac03b4ad2a4cca9fc4efaf53df8edc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cf9fd5b1a459434da2ce0d015878e99f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a3ca704416fe43699f7e70253956d2bf","IPY_MODEL_b14f8edab85842e8b4408d7c58d77e06"]}},"cf9fd5b1a459434da2ce0d015878e99f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a3ca704416fe43699f7e70253956d2bf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d59f6868ea1b4ae995d069c83640f9c6","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1389353,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1389353,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a6ca97613f4b41d1a08de0194d0f4533"}},"b14f8edab85842e8b4408d7c58d77e06":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ab3447e491b84a699586c4cc0ff788b4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.39M/1.39M [00:00&lt;00:00, 14.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_051ca4996b1b426e93d9ae24caae0ecf"}},"d59f6868ea1b4ae995d069c83640f9c6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a6ca97613f4b41d1a08de0194d0f4533":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ab3447e491b84a699586c4cc0ff788b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"051ca4996b1b426e93d9ae24caae0ecf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"bbWmvczoKOwQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627027941486,"user_tz":-540,"elapsed":501,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"3d621078-5362-416c-832e-d03249bb3352"},"source":["!nvidia-smi"],"execution_count":50,"outputs":[{"output_type":"stream","text":["Fri Jul 23 08:12:17 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P0    34W / 250W |  11755MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4kthgVPqr5VC"},"source":["## Directory 설정, 구글 드라이브 import"]},{"cell_type":"code","metadata":{"id":"FOkMqa8hrHl_","executionInfo":{"status":"ok","timestamp":1627027941969,"user_tz":-540,"elapsed":4,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["cur_dir = '/content/drive/MyDrive/KLUE_TC'"],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jx_d93v9rzQI"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"2KbbvLPitR1N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627027948327,"user_tz":-540,"elapsed":6361,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"a84a3120-b9a9-4135-e15a-ccac55f2c67e"},"source":["!pip install adamp\n","!pip install transformers"],"execution_count":52,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: adamp in /usr/local/lib/python3.7/dist-packages (0.3.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6oadxEi9rZ7x","executionInfo":{"status":"ok","timestamp":1627027948327,"user_tz":-540,"elapsed":4,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import os\n","import random\n","import torch\n","import numpy as np\n","from torch import nn\n","\n","from torch.optim import Adam, AdamW, SGD\n","from adamp import AdamP\n","from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR, ExponentialLR, \\\n","    CosineAnnealingWarmRestarts\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","\n","\n","def set_seeds(seed=42):\n","    # 랜덤 시드를 설정하여 매 코드를 실행할 때마다 동일한 결과를 얻게 합니다.\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","def save_checkpoint(state, model_dir, model_filename):\n","    print('saving model ...')\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","    torch.save(state, model_filename)\n","\n","\n","def get_optimizer(model, args):\n","    if args.optimizer == 'adam':\n","        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'adamW':\n","        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'adamP':\n","        optimizer = AdamP(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'SGD':\n","        optimizer = SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n","\n","    # 모든 parameter들의 grad값을 0으로 초기화\n","    optimizer.zero_grad()\n","\n","    return optimizer\n","\n","\n","def get_scheduler(optimizer, args):\n","    if args.scheduler == 'plateau':\n","        scheduler = ReduceLROnPlateau(optimizer, patience=args.plateau_patience, factor=args.plateau_factor, mode='max',\n","                                      verbose=True)\n","    elif args.scheduler == 'linear_warmup':\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n","                                                    num_training_steps=args.total_steps)\n","    elif args.scheduler == 'step_lr':\n","        scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n","    elif args.scheduler == 'exp_lr':\n","        scheduler = ExponentialLR(optimizer, gamma=args.gamma)\n","    elif args.scheduler == 'cosine_annealing':\n","        scheduler = CosineAnnealingLR(optimizer, T_max=args.t_max, eta_min=args.eta_min)\n","    elif args.scheduler == 'cosine_annealing_warmstart':\n","        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=args.T_0, T_mult=args.T_mult, eta_min=args.eta_min,\n","                                                last_epoch=-1)\n","\n","    return scheduler\n","\n","\n","def update_params(loss, model, optimizer, args):\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","\n","def load_tokenizer(args):\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.tokenizer_name\n","        if args.tokenizer_name\n","        else args.model_name_or_path,\n","        use_fast=True,\n","        bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n","        pad_token='<pad>', mask_token='<mask>'\n","    )\n","    return tokenizer\n","\n","\n","def load_model(args, model_name=None):\n","    if not model_name:\n","        model_name = args.model_name\n","    model_path = os.path.join(args.model_dir, model_name)\n","    print(\"Loading Model from:\", model_path)\n","    load_state = torch.load(model_path)\n","\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        args.config_name\n","        if args.config_name\n","        else args.model_name_or_path,\n","    )\n","\n","    config.num_labels = 7\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_path,\n","        from_tf=bool(\".ckpt\" in model_path),\n","        config=config\n","    ).to(args.device)\n","\n","    model.load_state_dict(load_state['state_dict'], strict=True)\n","\n","    print(model)\n","\n","    print(\"Loading Model from:\", model_path, \"...Finished.\")\n","\n","    return model\n","\n","\n","def get_model(args):\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        args.config_name\n","        if args.config_name\n","        else args.model_name_or_path,\n","    )\n","\n","    config.num_labels = 7\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","    ).to(args.device)\n","\n","    return model\n","\n","\n","def get_loaders(args, train, valid, is_inference=False):\n","    pin_memory = True\n","    train_loader, valid_loader = None, None\n","\n","    if is_inference:\n","        test_dataset = YNAT_dataset(args, valid, is_inference)\n","        test_loader = torch.utils.data.DataLoader(test_dataset, num_workers=args.num_workers, shuffle=False,\n","                                                  batch_size=args.batch_size, pin_memory=pin_memory)\n","        return test_loader\n","\n","    if train is not None:\n","        train_dataset = YNAT_dataset(args, train, is_inference)\n","        train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=args.num_workers, shuffle=True,\n","                                                   batch_size=args.batch_size, pin_memory=pin_memory)\n","    if valid is not None:\n","        valid_dataset = YNAT_dataset(args, valid, is_inference)\n","        valid_loader = torch.utils.data.DataLoader(valid_dataset, num_workers=args.num_workers, shuffle=False,\n","                                                   batch_size=args.batch_size, pin_memory=pin_memory)\n","\n","    return train_loader, valid_loader\n","\n","\n","# loss계산하고 parameter update!\n","def compute_loss(preds, targets, args):\n","    \"\"\"\n","    Args :\n","        preds   : (batch_size, max_seq_len)\n","        targets : (batch_size, max_seq_len)\n","    \"\"\"\n","    # print(preds, targets)\n","    loss = get_criterion(preds, targets, args)\n","    # 마지막 시퀀스에 대한 값만 loss 계산\n","    # loss = loss[:, -1]\n","    # loss = torch.mean(loss)\n","    return loss\n","\n","\n","def get_criterion(pred, target, args):\n","    if args.criterion == 'BCE':\n","        loss = nn.BCELoss(reduction=\"none\")\n","    elif args.criterion == \"BCELogit\":\n","        loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    elif args.criterion == \"MSE\":\n","        loss = nn.MSELoss(reduction=\"none\")\n","    elif args.criterion == \"L1\":\n","        loss = nn.L1Loss(reduction=\"none\")\n","    elif args.criterion == \"CE\":\n","        loss = nn.CrossEntropyLoss()\n","    # NLL, CrossEntropy not available\n","    return loss(pred, target)\n"],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mLM-aadds6H9"},"source":["## Dataloader"]},{"cell_type":"code","metadata":{"id":"2s9RxMi7rlfb","executionInfo":{"status":"ok","timestamp":1627027948328,"user_tz":-540,"elapsed":4,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import os\n","import torch\n","import pandas as pd\n","\n","\n","class Preprocess:\n","    def __init__(self, args):\n","        self.args = args\n","        self.train_data = None\n","        self.test_data = None\n","\n","    def load_data(self, file_name):\n","        csv_file_name = os.path.join(self.args.data_dir, file_name)\n","        df = pd.read_csv(csv_file_name)\n","        return df.values\n","\n","    def load_train_data(self):\n","        self.train_data = self.load_data('train_data.csv')\n","\n","    def load_test_data(self):\n","        self.test_data = self.load_data('test_data.csv')\n","\n","\n","class YNAT_dataset(torch.utils.data.Dataset):\n","    def __init__(self, args, data, is_inference):\n","        self.args = args\n","        self.data = data\n","        self.is_inference = is_inference\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        row = self.data[index]\n","        element = [row[i] for i in range(len(row))]\n","        #print(type(row))\n","        # np.array -> torch.tensor 형변환\n","        #for i, col in enumerate(row):\n","        #    if type(col) == str:\n","        #        pass\n","        #    else:\n","        #        row[i] = torch.tensor(col)\n","\n","        return element\n","\n"],"execution_count":54,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2g9iLEBtDnJ"},"source":["## Trainer"]},{"cell_type":"code","metadata":{"id":"Xq3sntmNtErb","executionInfo":{"status":"ok","timestamp":1627027949286,"user_tz":-540,"elapsed":467,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["from sklearn.metrics import accuracy_score\n","from torch.nn.functional import one_hot\n","from tqdm import tqdm\n","\n","\n","def run(args, tokenizer, train_data, valid_data, cv_count):\n","    train_loader, valid_loader = get_loaders(args, train_data, valid_data)\n","\n","    # only when using warmup scheduler\n","    # args.total_steps = int(len(train_loader.dataset) / args.batch_size) * args.n_epochs\n","    # args.warmup_steps = int(args.total_steps * args.warmup_ratio)\n","\n","    model = get_model(args)\n","    optimizer = get_optimizer(model, args)\n","    scheduler = get_scheduler(optimizer, args)\n","\n","    best_acc = -1\n","    early_stopping_counter = 0\n","    for epoch in range(args.n_epochs):\n","\n","        print(f\"Start Training: Epoch {epoch + 1}\")\n","\n","        if not args.cv_strategy:\n","            model_name = args.run_name\n","        else:\n","            model_name = f\"{args.run_name.split('.pt')[0]}_{cv_count}.pt\"\n","\n","        # TRAIN\n","        train_acc, train_loss = train(args, model, tokenizer, train_loader, optimizer)\n","\n","        # VALID\n","        acc, val_loss = validate(args, model, tokenizer, valid_loader)\n","\n","        # TODO: model save or early stopping\n","        if args.scheduler == 'plateau':\n","            last_lr = optimizer.param_groups[0]['lr']\n","        else:\n","            last_lr = scheduler.get_last_lr()[0]\n","\n","        print({\"epoch\": epoch, \"train_loss\": train_loss, \"train_acc\": train_acc,\n","                   \"valid_acc\": acc, \"val_loss\": val_loss, \"learning_rate\": last_lr})\n","\n","        if acc > best_acc:\n","            best_acc = acc\n","            # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n","            model_to_save = model.module if hasattr(model, 'module') else model\n","            save_checkpoint({\n","                'epoch': epoch + 1,\n","                'state_dict': model_to_save.state_dict(),\n","            },\n","                args.model_dir, model_name,\n","            )\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","            if early_stopping_counter >= args.patience:\n","                print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n","                break\n","\n","        # scheduler\n","        if args.scheduler == 'plateau':\n","            scheduler.step(best_acc)\n","        else:\n","            scheduler.step()\n","\n","    return best_acc\n","\n","\n","def inference(args, test_data):\n","    # ckpt_file_names = []\n","    all_fold_preds = []\n","    all_fold_argmax_preds = []\n","\n","    if not args.cv_strategy:\n","        ckpt_file_names = [args.model_name]\n","    else:\n","        ckpt_file_names = [f\"{args.model_name.split('.pt')[0]}_{i + 1}.pt\" for i in range(args.fold_num)]\n","\n","    tokenizer = load_tokenizer(args)\n","\n","    for fold_idx, ckpt in enumerate(ckpt_file_names):\n","        model = load_model(args, ckpt)\n","        model.eval()\n","        test_loader = get_loaders(args, None, test_data, True)\n","\n","        total_preds = []\n","        total_argmax_preds = []\n","        total_ids = []\n","\n","        for step, batch in tqdm(enumerate(test_loader), desc='Inferencing', total=len(test_loader)):\n","            idx, text = batch\n","            tokenized_examples = tokenizer(\n","                text,\n","                max_length=args.max_seq_len,\n","                padding=True,\n","                return_tensors=\"pt\"\n","            ).to(args.device)\n","\n","            preds = model(**tokenized_examples)\n","\n","            logits = preds['logits']\n","            argmax_logits = torch.argmax(logits, dim=1)\n","\n","            if args.device == 'cuda':\n","                argmax_preds = argmax_logits.to('cpu').detach().numpy()\n","                preds = logits.to('cpu').detach().numpy()\n","            else:  # cpu\n","                argmax_preds = argmax_logits.detach().numpy()\n","                preds = logits.detach().numpy()\n","\n","            total_preds += list(preds)\n","            total_argmax_preds += list(argmax_preds)\n","            total_ids += list(idx)\n","\n","        all_fold_preds.append(total_preds)\n","        all_fold_argmax_preds.append(total_argmax_preds)\n","\n","        output_file_name = \"output.csv\" if not args.cv_strategy else f\"output_{fold_idx + 1}.csv\"\n","        write_path = os.path.join(args.output_dir, output_file_name)\n","        if not os.path.exists(args.output_dir):\n","            os.makedirs(args.output_dir)\n","        with open(write_path, 'w', encoding='utf8') as w:\n","            print(\"writing prediction : {}\".format(write_path))\n","            w.write(\"index,topic_idx\\n\")\n","            for index, p in zip(total_ids, total_argmax_preds):\n","                w.write('{},{}\\n'.format(index, p))\n","\n","    if len(all_fold_preds) > 1:\n","        # Soft voting ensemble\n","        votes = np.sum(all_fold_preds, axis=0)\n","        votes = np.argmax(votes, axis=1)\n","\n","        write_path = os.path.join(args.output_dir, \"output_softvote.csv\")\n","        if not os.path.exists(args.output_dir):\n","            os.makedirs(args.output_dir)\n","        with open(write_path, 'w', encoding='utf8') as w:\n","            print(\"writing prediction : {}\".format(write_path))\n","            w.write(\"index,topic_idx\\n\")\n","            for id, p in zip(total_ids, votes):\n","                w.write('{},{}\\n'.format(id, p))\n","\n","\n","def train(args, model, tokenizer, train_loader, optimizer):\n","    model.train()\n","\n","    total_preds = []\n","    total_targets = []\n","    losses = []\n","    for step, batch in tqdm(enumerate(train_loader), desc='Training', total=len(train_loader)):\n","        idx, text, label = batch\n","        label = label.to(args.device)\n","        # print(idx[:10])\n","        # print(text[:10])\n","        # print(label[:10])\n","        tokenized_examples = tokenizer(\n","            text,\n","            max_length=args.max_seq_len,\n","            padding=True,\n","            return_tensors=\"pt\",\n","        ).to(args.device)\n","\n","        # tokenize\n","        # 모델의 입력으로\n","        # label은 one-hot?\n","        # loss 주고\n","        # argmax를 golden\n","\n","        preds = model(**tokenized_examples)\n","        logits = preds['logits']\n","        softmax_logits = nn.Softmax(dim=1)(logits)\n","        argmax_logits = torch.argmax(logits, dim=1)\n","\n","        # one_hot_logits = one_hot(argmax_logits, num_classes=7).float()\n","        # print(one_hot(argmax_logits, num_classes=7).type(torch.FloatTensor))\n","        loss = compute_loss(logits,\n","                            label, args)\n","\n","        # print(loss)\n","\n","        update_params(loss, model, optimizer, args)\n","\n","        if step % args.log_steps == 0:\n","            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n","\n","        if args.device == 'cuda':\n","            argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","            label = label.to('cpu').detach().numpy()\n","            loss = loss.to('cpu').detach().numpy()\n","        else:  # cpu\n","            argmax_logits = argmax_logits.detach().numpy()\n","            label = label.detach().numpy()\n","            loss = loss.detach().numpy()\n","\n","        total_preds.append(argmax_logits)\n","        total_targets.append(label)\n","        losses.append(loss)\n","\n","    total_preds = np.concatenate(total_preds)\n","    total_targets = np.concatenate(total_targets)\n","\n","    # Train AUC / ACC\n","    acc = accuracy_score(total_targets, total_preds)\n","    loss_avg = sum(losses) / len(losses)\n","    print(f'TRAIN ACC : {acc}, TRAIN LOSS : {loss_avg}')\n","    return acc, loss_avg\n","\n","\n","def validate(args, model, tokenizer, valid_loader):\n","    model.eval()\n","\n","    total_preds = []\n","    total_targets = []\n","    losses = []\n","    for step, batch in tqdm(enumerate(valid_loader), desc='Training', total=len(valid_loader)):\n","        idx, text, label = batch\n","        label = label.to(args.device)\n","        tokenized_examples = tokenizer(\n","            text,\n","            max_length=args.max_seq_len,\n","            padding=True,\n","            return_tensors=\"pt\"\n","        ).to(args.device)\n","\n","        # tokenize\n","        # 모델의 입력으로\n","        # label은 one-hot?\n","        # loss 주고\n","        # argmax를 golden\n","\n","        preds = model(**tokenized_examples)\n","        logits = preds['logits']\n","        softmax_logits = nn.Softmax(dim=1)(logits)\n","        argmax_logits = torch.argmax(logits, dim=1)\n","\n","        # one_hot_logits = one_hot(argmax_logits, num_classes=7).float()\n","        # print(one_hot(argmax_logits, num_classes=7).type(torch.FloatTensor))\n","        loss = compute_loss(logits,\n","                            label, args)\n","\n","        if step % args.log_steps == 0:\n","            print(f\"Validation steps: {step} Loss: {str(loss.item())}\")\n","\n","        if args.device == 'cuda':\n","            argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","            label = label.to('cpu').detach().numpy()\n","            loss = loss.to('cpu').detach().numpy()\n","        else:  # cpu\n","            argmax_logits = argmax_logits.detach().numpy()\n","            label = label.detach().numpy()\n","            loss = loss.detach().numpy()\n","\n","        total_preds.append(argmax_logits)\n","        total_targets.append(label)\n","        losses.append(loss)\n","\n","    total_preds = np.concatenate(total_preds)\n","    total_targets = np.concatenate(total_targets)\n","\n","    # Train AUC / ACC\n","    acc = accuracy_score(total_targets, total_preds)\n","    loss_avg = sum(losses) / len(losses)\n","    print(f'VALID ACC : {acc}, VALID LOSS : {loss_avg}')\n","    return acc, loss_avg\n"],"execution_count":55,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5CS14MP9tFCQ"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"K9DdutwStF4B","executionInfo":{"status":"ok","timestamp":1627027949287,"user_tz":-540,"elapsed":4,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import torch\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","from datetime import datetime\n","from pytz import timezone\n","\n","\n","def main(args):\n","    if not args.run_name:\n","        args.run_name = datetime.now(timezone(\"Asia/Seoul\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n","\n","    set_seeds(args.seed)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    args.device = device\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.tokenizer_name\n","        if args.tokenizer_name\n","        else args.model_name_or_path,\n","        use_fast=True,\n","        bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n","        pad_token='<pad>', mask_token='<mask>'\n","    )\n","\n","    preprocess = Preprocess(args)\n","    preprocess.load_train_data()\n","    train_data_origin = preprocess.train_data\n","\n","    print(f\"Size of train data : {len(train_data_origin)}\")\n","    # print(f\"size of test data : {len(test_data)}\")\n","\n","    if args.cv_strategy == 'random':\n","        kf = KFold(n_splits=args.fold_num, shuffle=True)\n","        splits = kf.split(X=train_data_origin)\n","    else:\n","        # default\n","        # 여기 각 label로 바꿔야됨\n","        train_labels = [sequence[-1] for sequence in train_data_origin]\n","        skf = StratifiedKFold(n_splits=args.fold_num, shuffle=True)\n","        splits = skf.split(X=train_data_origin, y=train_labels)\n","\n","    acc_avg = 0\n","    for fold_num, (train_index, valid_index) in enumerate(splits):\n","        train_data = train_data_origin[train_index]\n","        valid_data = train_data_origin[valid_index]\n","        best_acc = run(args, tokenizer, train_data, valid_data, fold_num + 1)\n","\n","        if not args.cv_strategy:\n","            break\n","\n","        acc_avg += best_acc\n","\n","    if args.cv_strategy:\n","        acc_avg /= args.fold_num\n","\n","        print(\"*\" * 50, 'auc_avg', \"*\" * 50)\n","        print(acc_avg)\n"],"execution_count":56,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1EgudubtKjJ"},"source":["## Run"]},{"cell_type":"code","metadata":{"id":"_xupDv9YtKGf","executionInfo":{"status":"ok","timestamp":1627027949287,"user_tz":-540,"elapsed":3,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import argparse\n","import easydict\n","\n","def parse_args():\n","    args = easydict.EasyDict({'run_name' : 'temp',\n","                             'seed':42,\n","                             'device' :'cuda',\n","                             'data_dir': cur_dir + '/data/open/',\n","                             'model_dir' : '/content/drive/MyDrive/KLUE_TC/models/',\n","                             'model_name_or_path' : 'skt/kogpt2-base-v2',\n","                             'config_name' : None,\n","                             'tokenizer_name' : None,\n","                             'output_dir' : '/content/drive/MyDrive/KLUE_TC/output/',\n","\n","                             'cv_strategy' : 'stratified',\n","                             'fold_num' : 4,\n","\n","                             'num_workers' : 1,\n","\n","                             # 훈련\n","                             'n_epochs' : 3,\n","                             'batch_size' : 64,\n","                             'lr' : 1e-5,\n","                             'clip_grad' : 10,\n","                             'patience' : 5,\n","                             'max_seq_len' : 40,\n","\n","                             # Optimizer\n","                             'optimizer' : 'adamW',\n","\n","                             # Optimizer-parameters\n","                             'weight_decay' : 0.01,\n","                             'momentum' : 0.9,\n","\n","                             # Scheduler\n","                             'scheduler' : 'step_lr',\n","\n","                             # Scheduler-parameters\n","                             # plateau\n","                             'plateau_patience' : 10,\n","                             'plateau_factor' : 0.5,\n","                              \n","                             't_max' : 10,\n","                             'T_0' : 10,\n","                             'T_mult' : 2,\n","                             '--eta_min' : 0.01,\n","\n","                             # linear_warmup\n","                             'warmup_ratio' : 0.3,\n","\n","                             # Step LR\n","                             'step_size' : 50,\n","                             'gamma' : 0.1,\n","\n","                             'criterion' : 'CE',\n","\n","                             'log_steps' : 100})\n","    \n","    return args"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":598,"referenced_widgets":["e9ae994c8e584490a8c28429afc284c8","f18f8250df5945248b965c03b6291cbb","d71a0fa3bc2d47ae9818e36dbb7e4265","9f3a1789d44141a9ad3b98619d6de10a","6d329c4ae7dd488b894d4b3af902b2c5","67f1f8c6b04a42f6bfe64353af4f7808","675ff7d6642a4921a6b4eefcdb381c99","d148cdf7784d49a6ae41e65177921bd3","01d5a9d5aad0477fad487578334ec6a0","494d3c04561f4e6a8d42e3a3e22946d0","584a0c48118b4e8d9f13ce7783edbbd4","e49bc0ce5b9f498387871b2e08dcd61b","776bc8df87104fcf8b7d7439d3663a00","17fc9a40fdf04f2a91b4f452b45dc57d","4b6f5dfc26e446e7a2c28c46b1fee894","c5d8033304ac4cf9917b4fb78c67148b","f7ac03b4ad2a4cca9fc4efaf53df8edc","cf9fd5b1a459434da2ce0d015878e99f","a3ca704416fe43699f7e70253956d2bf","b14f8edab85842e8b4408d7c58d77e06","d59f6868ea1b4ae995d069c83640f9c6","a6ca97613f4b41d1a08de0194d0f4533","ab3447e491b84a699586c4cc0ff788b4","051ca4996b1b426e93d9ae24caae0ecf"]},"id":"YKuE65Ct3d34","executionInfo":{"status":"error","timestamp":1627027951359,"user_tz":-540,"elapsed":2075,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"ed9f8507-6c0f-4e9d-df1d-358277297a3f"},"source":["if __name__ == '__main__':\n","    args = parse_args()\n","    main(args)"],"execution_count":58,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9ae994c8e584490a8c28429afc284c8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1200.0, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01d5a9d5aad0477fad487578334ec6a0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7ac03b4ad2a4cca9fc4efaf53df8edc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1389353.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"],"name":"stderr"},{"output_type":"stream","text":["Size of train data : 45654\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-58-48b1cbe580da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-56-88ded3529ccb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_origin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_origin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-55-8c12201d0586>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args, tokenizer, train_data, valid_data, cv_count)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# args.warmup_steps = int(args.total_steps * args.warmup_ratio)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-53-394bbf1a4311>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".ckpt\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     ).to(args.device)\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         raise ValueError(\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;34mf\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         )\n","\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForSequenceClassification.\nModel type should be one of CanineConfig, RoFormerConfig, BigBirdPegasusConfig, BigBirdConfig, ConvBertConfig, LEDConfig, DistilBertConfig, AlbertConfig, CamembertConfig, XLMRobertaConfig, MBartConfig, BartConfig, LongformerConfig, RobertaConfig, SqueezeBertConfig, LayoutLMConfig, BertConfig, XLNetConfig, MegatronBertConfig, MobileBertConfig, FlaubertConfig, XLMConfig, ElectraConfig, FunnelConfig, DebertaConfig, DebertaV2Config, GPT2Config, GPTNeoConfig, OpenAIGPTConfig, ReformerConfig, CTRLConfig, TransfoXLConfig, MPNetConfig, TapasConfig, IBertConfig."]}]},{"cell_type":"code","metadata":{"id":"YfVIY2z0kZZx","executionInfo":{"status":"aborted","timestamp":1627027951357,"user_tz":-540,"elapsed":318,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txpJN5vFPWCq"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"hUWUggp_PQqy","executionInfo":{"status":"aborted","timestamp":1627027951358,"user_tz":-540,"elapsed":319,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["def inference_main():\n","    args = parse_args()\n","    args.model_name = \"temp\"\n","    preprocess = Preprocess(args)\n","    preprocess.load_test_data()\n","    test_data = preprocess.test_data\n","\n","    print(f\"size of test data : {len(test_data)}\")\n","    torch.cuda.empty_cache()\n","    # del model\n","    inference(args, test_data)\n","\n","inference_main()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LaRxISbnmOXG","executionInfo":{"status":"aborted","timestamp":1627027951358,"user_tz":-540,"elapsed":318,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":[""],"execution_count":null,"outputs":[]}]}
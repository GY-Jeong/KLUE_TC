{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Baseline_translation_klue_kr.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1rjMM5kzU5DDhf6OzE-HSb9XEcW1_wecj","authorship_tag":"ABX9TyNJo0jsmVYlhp+EKFcMzcur"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbWmvczoKOwQ","executionInfo":{"status":"ok","timestamp":1627527825854,"user_tz":-540,"elapsed":358,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"ab7afa54-baf1-4fe5-e685-d5e6bf3966a4"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Thu Jul 29 03:03:42 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   47C    P0    54W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4kthgVPqr5VC"},"source":["## Directory 설정, 구글 드라이브 import"]},{"cell_type":"code","metadata":{"id":"FOkMqa8hrHl_","executionInfo":{"status":"ok","timestamp":1627527826455,"user_tz":-540,"elapsed":2,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["cur_dir = '/content/drive/MyDrive/KLUE_TC'"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jx_d93v9rzQI"},"source":["## Utils"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2KbbvLPitR1N","executionInfo":{"status":"ok","timestamp":1627527832116,"user_tz":-540,"elapsed":5663,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"dc51cb83-3579-43ef-b5a3-772f349b4dd7"},"source":["!pip install adamp\n","!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: adamp in /usr/local/lib/python3.7/dist-packages (0.3.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6oadxEi9rZ7x","executionInfo":{"status":"ok","timestamp":1627527834930,"user_tz":-540,"elapsed":2816,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import os\n","import random\n","import torch\n","import numpy as np\n","from torch import nn\n","\n","from torch.optim import Adam, AdamW, SGD\n","from adamp import AdamP\n","from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR, ExponentialLR, \\\n","    CosineAnnealingWarmRestarts\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","\n","\n","def set_seeds(seed=42):\n","    # 랜덤 시드를 설정하여 매 코드를 실행할 때마다 동일한 결과를 얻게 합니다.\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","def save_checkpoint(state, model_dir, model_filename):\n","    print('saving model ...')\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","    # torch.save(state, os.path.join(model_dir, model_filename))\n","    torch.save(state, model_filename)\n","\n","\n","def get_optimizer(model, args):\n","    if args.optimizer == 'adam':\n","        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'adamW':\n","        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'adamP':\n","        optimizer = AdamP(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'SGD':\n","        optimizer = SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n","\n","    # 모든 parameter들의 grad값을 0으로 초기화\n","    optimizer.zero_grad()\n","\n","    return optimizer\n","\n","\n","def get_scheduler(optimizer, args):\n","    if args.scheduler == 'plateau':\n","        scheduler = ReduceLROnPlateau(optimizer, patience=args.plateau_patience, factor=args.plateau_factor, mode='max',\n","                                      verbose=True)\n","    elif args.scheduler == 'linear_warmup':\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n","                                                    num_training_steps=args.total_steps)\n","    elif args.scheduler == 'step_lr':\n","        scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n","    elif args.scheduler == 'exp_lr':\n","        scheduler = ExponentialLR(optimizer, gamma=args.gamma)\n","    elif args.scheduler == 'cosine_annealing':\n","        scheduler = CosineAnnealingLR(optimizer, T_max=args.t_max, eta_min=args.eta_min)\n","    elif args.scheduler == 'cosine_annealing_warmstart':\n","        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=args.T_0, T_mult=args.T_mult, eta_min=args.eta_min,\n","                                                last_epoch=-1)\n","\n","    return scheduler\n","\n","\n","def update_params(loss, model, optimizer, batch_idx, max_len, args):\n","    if args.gradient_accumulation:\n","        # normalize loss to account for batch accumulation\n","        loss = loss / args.accum_iter \n","\n","        # backward pass\n","        loss.backward()\n","\n","        # weights update\n","        if ((batch_idx + 1) % args.accum_iter == 0) or (batch_idx + 1 == max_len):\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","    else:\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","\n","def load_tokenizer(args):\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.tokenizer_name\n","        if args.tokenizer_name\n","        else args.model_name_or_path,\n","        use_fast=True,\n","    )\n","\n","    return tokenizer\n","\n","\n","def load_model(args, model_name=None):\n","    if not model_name:\n","        model_name = args.model_name\n","    model_path = os.path.join(args.model_dir, model_name)\n","    print(\"Loading Model from:\", model_path)\n","    load_state = torch.load(model_path)\n","\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        args.config_name\n","        if args.config_name\n","        else args.model_name_or_path,\n","    )\n","\n","    config.num_labels = 7\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_path,\n","        from_tf=bool(\".ckpt\" in model_path),\n","        config=config\n","    )\n","\n","    model.classifier = nn.Sequential(\n","        nn.Linear(1024, 1024),\n","        nn.Dropout(p=0.3, inplace=False),\n","        nn.Linear(1024, 512),\n","        nn.Dropout(p=0.3, inplace=False),\n","        nn.Linear(512, 7),\n","    )\n","\n","    model.load_state_dict(load_state['state_dict'], strict=True)\n","\n","    model = model.to(args.device)\n","\n","    print(\"Loading Model from:\", model_path, \"...Finished.\")\n","\n","    return model\n","\n","\n","def get_model(args):\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        args.config_name\n","        if args.config_name\n","        else args.model_name_or_path,\n","    )\n","\n","    config.num_labels = 7\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","    )\n","\n","    model.classifier = nn.Sequential(\n","        nn.Linear(1024, 1024),\n","        nn.Dropout(p=0.3, inplace=False),\n","        nn.Linear(1024, 512),\n","        nn.Dropout(p=0.3, inplace=False),\n","        nn.Linear(512, 7),\n","    )\n","\n","    model = model.to(args.device)\n","\n","    return model\n","\n","\n","def get_loaders(args, train, valid, is_inference=False):\n","    pin_memory = True\n","    train_loader, valid_loader = None, None\n","\n","    if is_inference:\n","        test_dataset = YNAT_dataset(args, valid, is_inference)\n","        test_loader = torch.utils.data.DataLoader(test_dataset, num_workers=args.num_workers, shuffle=False,\n","                                                  batch_size=args.batch_size, pin_memory=pin_memory)\n","        return test_loader\n","\n","    if train is not None:\n","        train_dataset = YNAT_dataset(args, train, is_inference)\n","        train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=args.num_workers, shuffle=True,\n","                                                   batch_size=args.batch_size, pin_memory=pin_memory)\n","    if valid is not None:\n","        valid_dataset = YNAT_dataset(args, valid, is_inference)\n","        valid_loader = torch.utils.data.DataLoader(valid_dataset, num_workers=args.num_workers, shuffle=False,\n","                                                   batch_size=args.batch_size, pin_memory=pin_memory)\n","\n","    return train_loader, valid_loader\n","\n","\n","# loss계산하고 parameter update!\n","def compute_loss(preds, targets, args):\n","    \"\"\"\n","    Args :\n","        preds   : (batch_size, max_seq_len)\n","        targets : (batch_size, max_seq_len)\n","    \"\"\"\n","    # print(preds, targets)\n","    loss = get_criterion(preds, targets, args)\n","    # 마지막 시퀀스에 대한 값만 loss 계산\n","    # loss = loss[:, -1]\n","    # loss = torch.mean(loss)\n","    return loss\n","\n","\n","def get_criterion(pred, target, args):\n","    if args.criterion == 'BCE':\n","        loss = nn.BCELoss(reduction=\"none\")\n","    elif args.criterion == \"BCELogit\":\n","        loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    elif args.criterion == \"MSE\":\n","        loss = nn.MSELoss(reduction=\"none\")\n","    elif args.criterion == \"L1\":\n","        loss = nn.L1Loss(reduction=\"none\")\n","    elif args.criterion == \"CE\":\n","        #weights = [1,1,2,1,1,1,1] #as class distribution\n","        #class_weights = torch.FloatTensor(weights).cuda()\n","        #loss = nn.CrossEntropyLoss(weight=class_weights)\n","        loss = nn.CrossEntropyLoss()\n","    # NLL, CrossEntropy not available\n","    return loss(pred, target)\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mLM-aadds6H9"},"source":["## Dataloader"]},{"cell_type":"code","metadata":{"id":"2s9RxMi7rlfb","executionInfo":{"status":"ok","timestamp":1627527834931,"user_tz":-540,"elapsed":10,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import os\n","import torch\n","import pandas as pd\n","\n","\n","class Preprocess:\n","    def __init__(self, args):\n","        self.args = args\n","        self.train_data = None\n","        self.test_data = None\n","\n","    def load_data(self, file_name):\n","        csv_file_name = os.path.join(self.args.data_dir, file_name)\n","        df = pd.read_csv(csv_file_name)\n","        del df['Unnamed: 0']\n","        return df.values\n","\n","    def load_train_data(self):\n","        self.train_data = self.load_data('train_data_kr.csv')\n","\n","    def load_test_data(self):\n","        self.test_data = self.load_data('test_data_translated.csv')\n","\n","\n","class YNAT_dataset(torch.utils.data.Dataset):\n","    def __init__(self, args, data, is_inference):\n","        self.args = args\n","        self.data = data\n","        self.is_inference = is_inference\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        row = self.data[index]\n","        element = [row[i] for i in range(len(row))]\n","        #print(type(row))\n","        # np.array -> torch.tensor 형변환\n","        #for i, col in enumerate(row):\n","        #    if type(col) == str:\n","        #        pass\n","        #    else:\n","        #        row[i] = torch.tensor(col)\n","\n","        return element\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2g9iLEBtDnJ"},"source":["## Trainer"]},{"cell_type":"code","metadata":{"id":"Xq3sntmNtErb","executionInfo":{"status":"ok","timestamp":1627527836010,"user_tz":-540,"elapsed":1089,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["from sklearn.metrics import accuracy_score\n","from torch.nn.functional import one_hot\n","from tqdm import tqdm\n","from sklearn import metrics\n","\n","\n","def run(args, tokenizer, train_data, valid_data, cv_count):\n","    train_loader, valid_loader = get_loaders(args, train_data, valid_data)\n","\n","    # only when using warmup scheduler\n","    # args.total_steps = int(len(train_loader.dataset) / args.batch_size) * args.n_epochs\n","    # args.warmup_steps = int(args.total_steps * args.warmup_ratio)\n","\n","    model = get_model(args)\n","    optimizer = get_optimizer(model, args)\n","    scheduler = get_scheduler(optimizer, args)\n","\n","    best_acc = -1\n","    early_stopping_counter = 0\n","    for epoch in range(args.n_epochs):\n","\n","        print(f\"Start Training: Epoch {epoch + 1}\")\n","\n","        if not args.cv_strategy:\n","            model_name = args.run_name\n","        else:\n","            model_name = f\"{args.run_name.split('.pt')[0]}_{cv_count}.pt\"\n","\n","        # TRAIN\n","        train_acc, train_loss = train(args, model, tokenizer, train_loader, optimizer)\n","\n","        # VALID\n","        acc, val_loss = validate(args, model, tokenizer, valid_loader)\n","\n","        # TODO: model save or early stopping\n","        if args.scheduler == 'plateau':\n","            last_lr = optimizer.param_groups[0]['lr']\n","        else:\n","            last_lr = scheduler.get_last_lr()[0]\n","\n","        print({\"epoch\": epoch, \"train_loss\": train_loss, \"train_acc\": train_acc,\n","                   \"valid_acc\": acc, \"val_loss\": val_loss, \"learning_rate\": last_lr})\n","\n","        if acc > best_acc:\n","            best_acc = acc\n","            # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n","            model_to_save = model.module if hasattr(model, 'module') else model\n","            save_checkpoint({\n","                'epoch': epoch + 1,\n","                'state_dict': model_to_save.state_dict(),\n","            },\n","                args.model_dir, model_name,\n","            )\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","            if early_stopping_counter >= args.patience:\n","                print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n","                break\n","\n","        # scheduler\n","        if args.scheduler == 'plateau':\n","            scheduler.step(best_acc)\n","        else:\n","            scheduler.step()\n","\n","    return best_acc\n","\n","\n","def inference(args, test_data):\n","    # ckpt_file_names = []\n","    all_fold_preds = []\n","    all_fold_argmax_preds = []\n","\n","    if not args.cv_strategy:\n","        ckpt_file_names = [args.model_name]\n","    else:\n","        ckpt_file_names = [f\"{args.model_name.split('.pt')[0]}_{i + 1}.pt\" for i in range(args.fold_num)]\n","\n","    tokenizer = load_tokenizer(args)\n","\n","    for fold_idx, ckpt in enumerate(ckpt_file_names):\n","        model = load_model(args, ckpt)\n","        model.eval()\n","        test_loader = get_loaders(args, None, test_data, True)\n","\n","        total_preds = []\n","        total_argmax_preds = []\n","        total_ids = []\n","\n","        for step, batch in tqdm(enumerate(test_loader), desc='Inferencing', total=len(test_loader)):\n","            idx, text, text_kr = batch\n","            tokenized_examples = tokenizer(\n","                #text,\n","                text_kr,\n","                max_length=args.max_seq_len,\n","                padding=\"max_length\",\n","                return_tensors=\"pt\"\n","            ).to(args.device)\n","\n","            preds = model(input_ids = tokenized_examples['input_ids'],\n","                          attention_mask = tokenized_examples['attention_mask'])\n","\n","            logits = preds['logits']\n","            logits = logits[:,0,:]\n","            argmax_logits = torch.argmax(logits, dim=1)\n","\n","            if args.device == 'cuda':\n","                argmax_preds = argmax_logits.to('cpu').detach().numpy()\n","                preds = logits.to('cpu').detach().numpy()\n","            else:  # cpu\n","                argmax_preds = argmax_logits.detach().numpy()\n","                preds = logits.detach().numpy()\n","\n","            total_preds += list(preds)\n","            total_argmax_preds += list(argmax_preds)\n","            total_ids += list(idx)\n","\n","        all_fold_preds.append(total_preds)\n","        all_fold_argmax_preds.append(total_argmax_preds)\n","\n","        output_file_name = \"output.csv\" if not args.cv_strategy else f\"output_{fold_idx + 1}.csv\"\n","        write_path = os.path.join(args.output_dir, output_file_name)\n","        if not os.path.exists(args.output_dir):\n","            os.makedirs(args.output_dir)\n","        with open(write_path, 'w', encoding='utf8') as w:\n","            print(\"writing prediction : {}\".format(write_path))\n","            w.write(\"index,topic_idx\\n\")\n","            for index, p in zip(total_ids, total_argmax_preds):\n","                w.write('{},{}\\n'.format(index, p))\n","\n","    if len(all_fold_preds) > 1:\n","        # Soft voting ensemble\n","        votes = np.sum(all_fold_preds, axis=0)\n","        votes = np.argmax(votes, axis=1)\n","\n","        write_path = os.path.join(args.output_dir, \"output_softvote.csv\")\n","        if not os.path.exists(args.output_dir):\n","            os.makedirs(args.output_dir)\n","        with open(write_path, 'w', encoding='utf8') as w:\n","            print(\"writing prediction : {}\".format(write_path))\n","            w.write(\"index,topic_idx\\n\")\n","            for id, p in zip(total_ids, votes):\n","                w.write('{},{}\\n'.format(id, p))\n","\n","\n","def train(args, model, tokenizer, train_loader, optimizer):\n","    model.train()\n","\n","    total_preds = []\n","    total_targets = []\n","    losses = []\n","    for step, batch in tqdm(enumerate(train_loader), desc='Training', total=len(train_loader)):\n","        idx, text, text_kr, label = batch\n","        label = label.to(args.device)\n","        tokenized_examples = tokenizer(\n","            #text,\n","            text_kr,\n","            max_length=args.max_seq_len,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        ).to(args.device)\n","\n","        # tokenize\n","        # 모델의 입력으로\n","        # label은 one-hot?\n","        # loss 주고\n","        # argmax를 golden\n","\n","        #print(tokenized_examples)\n","        #print(tokenized_examples['input_ids'][:5])\n","        #print(tokenized_examples['attention_mask'][:5])\n","        #print(tokenized_examples['token_type_ids'][:5])\n","\n","        #preds = model(**tokenized_examples)\n","\n","        \n","        preds = model(input_ids = tokenized_examples['input_ids'],\n","                        attention_mask = tokenized_examples['attention_mask'])\n","            \n","\n","        logits = preds['logits']\n","        logits = logits[:,0,:]\n","        softmax_logits = nn.Softmax(dim=1)(logits)\n","        argmax_logits = torch.argmax(logits, dim=1)\n","\n","        # one_hot_logits = one_hot(argmax_logits, num_classes=7).float()\n","        # print(one_hot(argmax_logits, num_classes=7).type(torch.FloatTensor))\n","        loss = compute_loss(logits,\n","                            label, args)\n","\n","        # print(loss)\n","\n","        update_params(loss, model, optimizer, step, len(train_loader), args)\n","\n","        if step % args.log_steps == 0:\n","            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n","\n","        if args.device == 'cuda':\n","            argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","            label = label.to('cpu').detach().numpy()\n","            loss = loss.to('cpu').detach().numpy()\n","            tokenized_examples = tokenized_examples.to('cpu')\n","            logits = logits.to('cpu').detach().numpy()\n","        else:  # cpu\n","            argmax_logits = argmax_logits.detach().numpy()\n","            label = label.detach().numpy()\n","            loss = loss.detach().numpy()\n","\n","        total_preds.append(argmax_logits)\n","        total_targets.append(label)\n","        losses.append(loss)\n","\n","    total_preds = np.concatenate(total_preds)\n","    total_targets = np.concatenate(total_targets)\n","\n","    # Train AUC / ACC\n","    acc = accuracy_score(total_targets, total_preds)\n","    loss_avg = sum(losses) / len(losses)\n","    print(f'TRAIN ACC : {acc}, TRAIN LOSS : {loss_avg}')\n","    return acc, loss_avg\n","\n","\n","def validate(args, model, tokenizer, valid_loader):\n","    model.eval()\n","\n","    total_preds = []\n","    total_targets = []\n","    losses = []\n","    for step, batch in tqdm(enumerate(valid_loader), desc='Training', total=len(valid_loader)):\n","        idx, text, text_kr, label = batch\n","        label = label.to(args.device)\n","        tokenized_examples = tokenizer(\n","            #text,\n","            text_kr,\n","            max_length=args.max_seq_len,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        ).to(args.device)\n","\n","        # tokenize\n","        # 모델의 입력으로\n","        # label은 one-hot?\n","        # loss 주고\n","        # argmax를 golden\n","        #print(tokenized_examples)\n","        #print(tokenized_examples['input_ids'][:5])\n","        #print(tokenized_examples['attention_mask'][:5])\n","        #print(tokenized_examples['token_type_ids'][:5])\n","\n","        #preds = model(**tokenized_examples)\n","\n","        \n","        preds = model(input_ids = tokenized_examples['input_ids'],\n","                        attention_mask = tokenized_examples['attention_mask'])\n","        \n","        logits = preds['logits']\n","        logits = logits[:,0,:]\n","        softmax_logits = nn.Softmax(dim=1)(logits)\n","        argmax_logits = torch.argmax(logits, dim=1)\n","\n","        # one_hot_logits = one_hot(argmax_logits, num_classes=7).float()\n","        # print(one_hot(argmax_logits, num_classes=7).type(torch.FloatTensor))\n","        loss = compute_loss(logits,\n","                            label, args)\n","\n","        if step % args.log_steps == 0:\n","            print(f\"Validation steps: {step} Loss: {str(loss.item())}\")\n","\n","        if args.device == 'cuda':\n","            argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","            label = label.to('cpu').detach().numpy()\n","            loss = loss.to('cpu').detach().numpy()\n","            tokenized_examples = tokenized_examples.to('cpu')\n","            logits = logits.to('cpu').detach().numpy()\n","        else:  # cpu\n","            argmax_logits = argmax_logits.detach().numpy()\n","            label = label.detach().numpy()\n","            loss = loss.detach().numpy()\n","\n","        total_preds.append(argmax_logits)\n","        total_targets.append(label)\n","        losses.append(loss)\n","\n","    total_preds = np.concatenate(total_preds)\n","    total_targets = np.concatenate(total_targets)\n","\n","    # Train AUC / ACC\n","    target_names = ['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치']\n","    print(metrics.classification_report(total_targets, total_preds, target_names=target_names))\n","    matrix = metrics.confusion_matrix(total_targets, total_preds)\n","    print(matrix.diagonal()/matrix.sum(axis=1))\n","\n","    # Train AUC / ACC\n","    acc = accuracy_score(total_targets, total_preds)\n","    loss_avg = sum(losses) / len(losses)\n","    print(f'VALID ACC : {acc}, VALID LOSS : {loss_avg}')\n","    return acc, loss_avg\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5CS14MP9tFCQ"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"K9DdutwStF4B","executionInfo":{"status":"ok","timestamp":1627527836010,"user_tz":-540,"elapsed":3,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import torch\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","from datetime import datetime\n","from pytz import timezone\n","\n","\n","def main(args):\n","    if not args.run_name:\n","        args.run_name = datetime.now(timezone(\"Asia/Seoul\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n","\n","    set_seeds(args.seed)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    args.device = device\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.tokenizer_name\n","        if args.tokenizer_name\n","        else args.model_name_or_path,\n","        #use_fast=True,\n","    )\n","\n","    preprocess = Preprocess(args)\n","    preprocess.load_train_data()\n","    train_data_origin = preprocess.train_data\n","\n","    print(f\"Size of train data : {len(train_data_origin)}\")\n","    # print(f\"size of test data : {len(test_data)}\")\n","\n","    if args.cv_strategy == 'random':\n","        kf = KFold(n_splits=args.fold_num, shuffle=True)\n","        splits = kf.split(X=train_data_origin)\n","    else:\n","        # default\n","        # 여기 각 label로 바꿔야됨\n","        train_labels = [sequence[-1] for sequence in train_data_origin]\n","        skf = StratifiedKFold(n_splits=args.fold_num, shuffle=True)\n","        splits = skf.split(X=train_data_origin, y=train_labels)\n","\n","    acc_avg = 0\n","    for fold_num, (train_index, valid_index) in enumerate(splits):\n","        train_data = train_data_origin[train_index]\n","        valid_data = train_data_origin[valid_index]\n","        best_acc = run(args, tokenizer, train_data, valid_data, fold_num + 1)\n","\n","        if not args.cv_strategy:\n","            break\n","\n","        acc_avg += best_acc\n","\n","    if args.cv_strategy:\n","        acc_avg /= args.fold_num\n","\n","        print(\"*\" * 50, 'auc_avg', \"*\" * 50)\n","        print(acc_avg)\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1EgudubtKjJ"},"source":["## Run"]},{"cell_type":"code","metadata":{"id":"_xupDv9YtKGf","executionInfo":{"status":"ok","timestamp":1627527836011,"user_tz":-540,"elapsed":3,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import argparse\n","import easydict\n","\n","def parse_args():\n","    args = easydict.EasyDict({'run_name' : 'temp',\n","                             'seed':42,\n","                             'device' :'cuda',\n","                             'data_dir': cur_dir + '/data/open/',\n","                             'model_dir' : '/content/drive/MyDrive/KLUE_TC/models/',\n","                             'model_name_or_path' : 'klue/roberta-large',\n","                             #'model_name_or_path' : 'xlm-roberta-large',\n","                             'config_name' : None,\n","                             'tokenizer_name' : None,\n","                             'output_dir' : '/content/drive/MyDrive/KLUE_TC/output/translation/',\n","                             \n","                             'accum_iter' : 8,\n","                             'gradient_accumulation' : True,\n","\n","                             'cv_strategy' : 'stratified',\n","                             'fold_num' : 4,\n","\n","                             'num_workers' : 1,\n","\n","                             # 훈련\n","                             'n_epochs' : 3,\n","                             'batch_size' : 16,\n","                             'lr' : 5e-6,\n","                             'clip_grad' : 10,\n","                             'patience' : 5,\n","                             'max_seq_len' : 110,\n","\n","                             # Optimizer\n","                             'optimizer' : 'adamW',\n","\n","                             # Optimizer-parameters\n","                             'weight_decay' : 0.05,\n","                             'momentum' : 0.9,\n","\n","                             # Scheduler\n","                             'scheduler' : 'step_lr',\n","\n","                             # Scheduler-parameters\n","                             # plateau\n","                             'plateau_patience' : 10,\n","                             'plateau_factor' : 0.5,\n","                              \n","                             't_max' : 10,\n","                             'T_0' : 10,\n","                             'T_mult' : 2,\n","                             '--eta_min' : 0.01,\n","\n","                             # linear_warmup\n","                             'warmup_ratio' : 0.3,\n","\n","                             # Step LR\n","                             'step_size' : 50,\n","                             'gamma' : 0.1,\n","\n","                             'criterion' : 'CE',\n","\n","                             'log_steps' : 100})\n","    \n","    return args"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"YKuE65Ct3d34","executionInfo":{"status":"error","timestamp":1627530370010,"user_tz":-540,"elapsed":2534002,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"6bdc2e49-aa2c-4f5b-931c-ab6b9f6b8877"},"source":["if __name__ == '__main__':\n","    args = parse_args()\n","    main(args)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Size of train data : 45654\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Start Training: Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2140 [00:00<10:41,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 1.9433690309524536\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2140 [00:29<10:00,  3.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 1.9136300086975098\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2140 [00:59<09:49,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 1.806891679763794\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2140 [01:29<09:02,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 1.4294134378433228\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2140 [01:59<08:48,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 1.0813603401184082\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2140 [02:29<08:04,  3.38it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 1.0171688795089722\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2140 [02:59<07:45,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.7323077321052551\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2140 [03:29<07:03,  3.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.7875252962112427\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2140 [03:58<06:46,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.592852771282196\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2140 [04:28<06:04,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.49834778904914856\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2140 [04:58<05:45,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.7653273940086365\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2140 [05:28<05:06,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.2410917580127716\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2140 [05:58<04:46,  3.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 1.0456475019454956\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2140 [06:28<04:07,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.08563227951526642\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2140 [06:57<03:44,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.24742986261844635\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2140 [07:27<03:08,  3.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.3870387375354767\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2140 [07:57<02:43,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.30874815583229065\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2140 [08:27<02:09,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.3583820164203644\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2140 [08:57<01:42,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.49719908833503723\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2140 [09:27<01:10,  3.38it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.5124335289001465\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 2001/2140 [09:56<00:42,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.6722394227981567\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2140 [10:26<00:11,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.6797186136245728\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2140/2140 [10:38<00:00,  3.35it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.7533002336448598, TRAIN LOSS : 0.7996995668475316\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/714 [00:00<01:17,  9.18it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.4153074324131012\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 102/714 [00:10<01:01,  9.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.41167908906936646\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 201/714 [00:20<00:50, 10.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.5745862722396851\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 303/714 [00:30<00:40, 10.10it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.33740565180778503\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 403/714 [00:40<00:30, 10.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.5405115485191345\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 503/714 [00:50<00:20, 10.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.1197073683142662\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 603/714 [01:00<00:10, 10.10it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 0.4015890061855316\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 703/714 [01:09<00:01, 10.05it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.2175680249929428\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [01:11<00:00, 10.04it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.76      0.91      0.82      1206\n","          경제       0.83      0.80      0.82      1555\n","          사회       0.77      0.72      0.74      1841\n","        생활문화       0.88      0.84      0.86      1483\n","          세계       0.88      0.89      0.89      1908\n","         스포츠       0.94      0.97      0.95      1734\n","          정치       0.89      0.85      0.87      1687\n","\n","    accuracy                           0.85     11414\n","   macro avg       0.85      0.85      0.85     11414\n","weighted avg       0.85      0.85      0.85     11414\n","\n","[0.90713101 0.80385852 0.71700163 0.84086312 0.89465409 0.96828143\n"," 0.85299348]\n","VALID ACC : 0.8529875591379008, VALID LOSS : 0.46265474338011414\n","{'epoch': 0, 'train_loss': 0.7996995668475316, 'train_acc': 0.7533002336448598, 'valid_acc': 0.8529875591379008, 'val_loss': 0.46265474338011414, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2140 [00:00<15:05,  2.36it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.5124078989028931\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2140 [00:30<10:00,  3.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.20073184370994568\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2140 [01:00<09:48,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.7373939752578735\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2140 [01:29<09:03,  3.38it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.10243690758943558\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2140 [01:59<08:48,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.6937940716743469\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2140 [02:29<08:02,  3.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.5122249722480774\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2140 [02:59<07:45,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.21854956448078156\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2140 [03:29<07:04,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.09707139432430267\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2140 [03:58<06:46,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.5887240767478943\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2140 [04:28<06:04,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.3460904657840729\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2140 [04:58<05:45,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.27482932806015015\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2140 [05:28<05:06,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.9223419427871704\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2140 [05:58<04:44,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 1.0811665058135986\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2140 [06:28<04:07,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.08836578577756882\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2140 [06:57<03:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.06638966500759125\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2140 [07:27<03:08,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.13151131570339203\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2140 [07:57<02:43,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.35465675592422485\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2140 [08:27<02:09,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.30872491002082825\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2140 [08:57<01:42,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.1310228556394577\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2140 [09:27<01:10,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.16465263068675995\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 2001/2140 [09:56<00:42,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.4079093039035797\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2140 [10:26<00:11,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.6498906016349792\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2140/2140 [10:38<00:00,  3.35it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.8610689252336449, TRAIN LOSS : 0.4313633055643779\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/714 [00:00<01:16,  9.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.4538847804069519\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 103/714 [00:10<01:00, 10.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.2598813474178314\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 203/714 [00:20<00:50, 10.05it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.5158703327178955\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 303/714 [00:30<00:40, 10.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.2659655511379242\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 403/714 [00:39<00:30, 10.12it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.5282600522041321\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 503/714 [00:49<00:20, 10.12it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.08703335374593735\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 603/714 [00:59<00:10, 10.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 0.39144811034202576\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 703/714 [01:09<00:01, 10.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.17801326513290405\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [01:10<00:00, 10.10it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.81      0.84      0.83      1206\n","          경제       0.83      0.81      0.82      1555\n","          사회       0.77      0.73      0.75      1841\n","        생활문화       0.86      0.87      0.86      1483\n","          세계       0.87      0.91      0.89      1908\n","         스포츠       0.94      0.97      0.95      1734\n","          정치       0.90      0.85      0.87      1687\n","\n","    accuracy                           0.86     11414\n","   macro avg       0.85      0.85      0.85     11414\n","weighted avg       0.85      0.86      0.85     11414\n","\n","[0.83996683 0.81414791 0.72569256 0.86581254 0.90828092 0.97289504\n"," 0.85477178]\n","VALID ACC : 0.8551778517609953, VALID LOSS : 0.4374374602725651\n","{'epoch': 1, 'train_loss': 0.4313633055643779, 'train_acc': 0.8610689252336449, 'valid_acc': 0.8551778517609953, 'val_loss': 0.4374374602725651, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2140 [00:00<11:29,  3.10it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.2774089276790619\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2140 [00:30<10:00,  3.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.5430668592453003\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2140 [00:59<09:48,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.41121089458465576\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2140 [01:29<09:02,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.5244501829147339\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2140 [01:59<08:47,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.11308394372463226\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2140 [02:29<08:03,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.056051794439554214\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2140 [02:59<07:46,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.19462290406227112\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2140 [03:28<07:03,  3.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 1.1651599407196045\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2140 [03:58<06:46,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.606045126914978\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2140 [04:28<06:04,  3.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.5682233572006226\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2140 [04:58<05:45,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.42930343747138977\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2140 [05:28<05:05,  3.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.1112217977643013\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2140 [05:58<04:44,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.09100695699453354\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2140 [06:27<04:07,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.597690761089325\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2140 [06:57<03:44,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.3262377679347992\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2140 [07:27<03:08,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.28694239258766174\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2140 [07:57<02:43,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.4860115051269531\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2140 [08:27<02:09,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.5095551609992981\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2140 [08:57<01:42,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.19981536269187927\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2140 [09:26<01:10,  3.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.6343971490859985\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 2001/2140 [09:56<00:42,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.5093687176704407\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2140 [10:26<00:11,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.3751487731933594\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2140/2140 [10:38<00:00,  3.35it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.8788843457943926, TRAIN LOSS : 0.3686391635649951\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/714 [00:00<01:17,  9.23it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.4746152460575104\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 103/714 [00:10<01:00, 10.13it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.2608073353767395\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 203/714 [00:20<00:50, 10.08it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.5237995386123657\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 303/714 [00:30<00:41, 10.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.21108528971672058\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 403/714 [00:39<00:30, 10.08it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.5969356894493103\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 503/714 [00:49<00:20, 10.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.06738651543855667\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 603/714 [00:59<00:11, 10.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 0.3271309733390808\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 703/714 [01:09<00:01, 10.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.08999928832054138\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [01:10<00:00, 10.09it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.78      0.89      0.83      1206\n","          경제       0.82      0.82      0.82      1555\n","          사회       0.74      0.74      0.74      1841\n","        생활문화       0.89      0.83      0.86      1483\n","          세계       0.89      0.90      0.89      1908\n","         스포츠       0.94      0.97      0.95      1734\n","          정치       0.92      0.83      0.87      1687\n","\n","    accuracy                           0.85     11414\n","   macro avg       0.85      0.85      0.85     11414\n","weighted avg       0.86      0.85      0.85     11414\n","\n","[0.8880597  0.81672026 0.74416078 0.83412003 0.9009434  0.97174164\n"," 0.82691168]\n","VALID ACC : 0.8539512878920624, VALID LOSS : 0.4393319756924814\n","{'epoch': 2, 'train_loss': 0.3686391635649951, 'train_acc': 0.8788843457943926, 'valid_acc': 0.8539512878920624, 'val_loss': 0.4393319756924814, 'learning_rate': 5e-06}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Start Training: Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2140 [00:00<11:37,  3.07it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 1.938393473625183\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2140 [00:30<10:00,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 1.838400959968567\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2140 [00:59<09:47,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 1.8236443996429443\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2140 [01:29<09:02,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 1.7307792901992798\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2140 [01:59<08:49,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 1.3967061042785645\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2140 [02:29<08:02,  3.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.9632500410079956\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2140 [02:59<07:45,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.7551053166389465\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2140 [03:29<07:03,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.27627038955688477\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2140 [03:59<06:46,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.8268328309059143\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2140 [04:28<06:05,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.4033464193344116\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2140 [04:58<05:45,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.7108793258666992\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2140 [05:28<05:06,  3.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.5290884971618652\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2140 [05:58<04:44,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.9123919606208801\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  60%|█████▉    | 1274/2140 [06:20<04:19,  3.34it/s]"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-48b1cbe580da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-02047b9dc672>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_origin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_origin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-9a446cfea06a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args, tokenizer, train_data, valid_data, cv_count)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# TRAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# VALID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-9a446cfea06a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, tokenizer, train_loader, optimizer)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-939a88e66424>\u001b[0m in \u001b[0;36mupdate_params\u001b[0;34m(loss, model, optimizer, batch_idx, max_len, args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# weights update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"YfVIY2z0kZZx","executionInfo":{"status":"aborted","timestamp":1627530370008,"user_tz":-540,"elapsed":12,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["# torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4nwVSlNpLG-","executionInfo":{"status":"aborted","timestamp":1627530370009,"user_tz":-540,"elapsed":13,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txpJN5vFPWCq"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"hUWUggp_PQqy","executionInfo":{"status":"aborted","timestamp":1627530370010,"user_tz":-540,"elapsed":14,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["def inference_main():\n","    args = parse_args()\n","    args.model_name = \"temp\"\n","    preprocess = Preprocess(args)\n","    preprocess.load_test_data()\n","    test_data = preprocess.test_data\n","\n","    print(f\"size of test data : {len(test_data)}\")\n","    torch.cuda.empty_cache()\n","    # del model\n","    inference(args, test_data)\n","\n","inference_main()"],"execution_count":null,"outputs":[]}]}
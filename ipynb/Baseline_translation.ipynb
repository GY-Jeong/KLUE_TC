{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Baseline_translation.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1fMeAL1hjoeTDBlBclJuc7xkdRBCTzI_3","authorship_tag":"ABX9TyOmN2ENnY8vxHYuPLGtf13R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbWmvczoKOwQ","executionInfo":{"status":"ok","timestamp":1627057194889,"user_tz":-540,"elapsed":387,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"cdbd1a88-fa16-42c1-ae4c-763275bb1bfa"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Fri Jul 23 16:19:53 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    49W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4kthgVPqr5VC"},"source":["## Directory 설정, 구글 드라이브 import"]},{"cell_type":"code","metadata":{"id":"FOkMqa8hrHl_","executionInfo":{"status":"ok","timestamp":1627057195432,"user_tz":-540,"elapsed":3,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["cur_dir = '/content/drive/MyDrive/KLUE_TC'"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jx_d93v9rzQI"},"source":["## Utils"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2KbbvLPitR1N","executionInfo":{"status":"ok","timestamp":1627057201344,"user_tz":-540,"elapsed":5914,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"446ecf65-0539-4e42-fa1e-6fb5b8197a60"},"source":["!pip install adamp\n","!pip install transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: adamp in /usr/local/lib/python3.7/dist-packages (0.3.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6oadxEi9rZ7x","executionInfo":{"status":"ok","timestamp":1627057204388,"user_tz":-540,"elapsed":3052,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import os\n","import random\n","import torch\n","import numpy as np\n","from torch import nn\n","\n","from torch.optim import Adam, AdamW, SGD\n","from adamp import AdamP\n","from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR, ExponentialLR, \\\n","    CosineAnnealingWarmRestarts\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","\n","\n","def set_seeds(seed=42):\n","    # 랜덤 시드를 설정하여 매 코드를 실행할 때마다 동일한 결과를 얻게 합니다.\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","def save_checkpoint(state, model_dir, model_filename):\n","    print('saving model ...')\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","    torch.save(state, os.path.join(model_dir, model_filename))\n","\n","\n","def get_optimizer(model, args):\n","    if args.optimizer == 'adam':\n","        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'adamW':\n","        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'adamP':\n","        optimizer = AdamP(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'SGD':\n","        optimizer = SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n","\n","    # 모든 parameter들의 grad값을 0으로 초기화\n","    optimizer.zero_grad()\n","\n","    return optimizer\n","\n","\n","def get_scheduler(optimizer, args):\n","    if args.scheduler == 'plateau':\n","        scheduler = ReduceLROnPlateau(optimizer, patience=args.plateau_patience, factor=args.plateau_factor, mode='max',\n","                                      verbose=True)\n","    elif args.scheduler == 'linear_warmup':\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n","                                                    num_training_steps=args.total_steps)\n","    elif args.scheduler == 'step_lr':\n","        scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n","    elif args.scheduler == 'exp_lr':\n","        scheduler = ExponentialLR(optimizer, gamma=args.gamma)\n","    elif args.scheduler == 'cosine_annealing':\n","        scheduler = CosineAnnealingLR(optimizer, T_max=args.t_max, eta_min=args.eta_min)\n","    elif args.scheduler == 'cosine_annealing_warmstart':\n","        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=args.T_0, T_mult=args.T_mult, eta_min=args.eta_min,\n","                                                last_epoch=-1)\n","\n","    return scheduler\n","\n","\n","def update_params(loss, model, optimizer, args):\n","    loss.backward()\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","\n","def load_tokenizer(args):\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.tokenizer_name\n","        if args.tokenizer_name\n","        else args.model_name_or_path,\n","        use_fast=True,\n","    )\n","\n","    return tokenizer\n","\n","\n","def load_model(args, model_name=None):\n","    if not model_name:\n","        model_name = args.model_name\n","    model_path = os.path.join(args.model_dir, model_name)\n","    print(\"Loading Model from:\", model_path)\n","    load_state = torch.load(model_path)\n","\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        args.config_name\n","        if args.config_name\n","        else args.model_name_or_path,\n","    )\n","\n","    config.num_labels = 7\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_path,\n","        from_tf=bool(\".ckpt\" in model_path),\n","        config=config\n","    ).to(args.device)\n","\n","    model.load_state_dict(load_state['state_dict'], strict=True)\n","\n","    print(model)\n","\n","    print(\"Loading Model from:\", model_path, \"...Finished.\")\n","\n","    return model\n","\n","\n","def get_model(args):\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        args.config_name\n","        if args.config_name\n","        else args.model_name_or_path,\n","    )\n","\n","    config.num_labels = 7\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","    ).to(args.device)\n","\n","    model.classifier.dropout = nn.Dropout(p=0.4, inplace = False)\n","    # print(model)\n","    return model\n","\n","\n","def get_loaders(args, train, valid, is_inference=False):\n","    pin_memory = True\n","    train_loader, valid_loader = None, None\n","\n","    if is_inference:\n","        test_dataset = YNAT_dataset(args, valid, is_inference)\n","        test_loader = torch.utils.data.DataLoader(test_dataset, num_workers=args.num_workers, shuffle=False,\n","                                                  batch_size=args.batch_size, pin_memory=pin_memory)\n","        return test_loader\n","\n","    if train is not None:\n","        train_dataset = YNAT_dataset(args, train, is_inference)\n","        train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=args.num_workers, shuffle=True,\n","                                                   batch_size=args.batch_size, pin_memory=pin_memory)\n","    if valid is not None:\n","        valid_dataset = YNAT_dataset(args, valid, is_inference)\n","        valid_loader = torch.utils.data.DataLoader(valid_dataset, num_workers=args.num_workers, shuffle=False,\n","                                                   batch_size=args.batch_size, pin_memory=pin_memory)\n","\n","    return train_loader, valid_loader\n","\n","\n","# loss계산하고 parameter update!\n","def compute_loss(preds, targets, args):\n","    \"\"\"\n","    Args :\n","        preds   : (batch_size, max_seq_len)\n","        targets : (batch_size, max_seq_len)\n","    \"\"\"\n","    # print(preds, targets)\n","    loss = get_criterion(preds, targets, args)\n","    # 마지막 시퀀스에 대한 값만 loss 계산\n","    # loss = loss[:, -1]\n","    # loss = torch.mean(loss)\n","    return loss\n","\n","\n","def get_criterion(pred, target, args):\n","    if args.criterion == 'BCE':\n","        loss = nn.BCELoss(reduction=\"none\")\n","    elif args.criterion == \"BCELogit\":\n","        loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    elif args.criterion == \"MSE\":\n","        loss = nn.MSELoss(reduction=\"none\")\n","    elif args.criterion == \"L1\":\n","        loss = nn.L1Loss(reduction=\"none\")\n","    elif args.criterion == \"CE\":\n","        loss = nn.CrossEntropyLoss()\n","    # NLL, CrossEntropy not available\n","    return loss(pred, target)\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mLM-aadds6H9"},"source":["## Dataloader"]},{"cell_type":"code","metadata":{"id":"2s9RxMi7rlfb","executionInfo":{"status":"ok","timestamp":1627057204389,"user_tz":-540,"elapsed":10,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import os\n","import torch\n","import pandas as pd\n","\n","\n","class Preprocess:\n","    def __init__(self, args):\n","        self.args = args\n","        self.train_data = None\n","        self.test_data = None\n","\n","    def load_data(self, file_name):\n","        csv_file_name = os.path.join(self.args.data_dir, file_name)\n","        df = pd.read_csv(csv_file_name)\n","        del df['Unnamed: 0']\n","        return df.values\n","\n","    def load_train_data(self):\n","        self.train_data = self.load_data('train_data_translated.csv')\n","\n","    def load_test_data(self):\n","        self.test_data = self.load_data('test_data_translated.csv')\n","\n","\n","class YNAT_dataset(torch.utils.data.Dataset):\n","    def __init__(self, args, data, is_inference):\n","        self.args = args\n","        self.data = data\n","        self.is_inference = is_inference\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        row = self.data[index]\n","        element = [row[i] for i in range(len(row))]\n","        #print(type(row))\n","        # np.array -> torch.tensor 형변환\n","        #for i, col in enumerate(row):\n","        #    if type(col) == str:\n","        #        pass\n","        #    else:\n","        #        row[i] = torch.tensor(col)\n","\n","        return element\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2g9iLEBtDnJ"},"source":["## Trainer"]},{"cell_type":"code","metadata":{"id":"Xq3sntmNtErb","executionInfo":{"status":"ok","timestamp":1627057204937,"user_tz":-540,"elapsed":557,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["from sklearn.metrics import accuracy_score\n","from torch.nn.functional import one_hot\n","from tqdm import tqdm\n","\n","\n","def run(args, tokenizer, train_data, valid_data, cv_count):\n","    train_loader, valid_loader = get_loaders(args, train_data, valid_data)\n","\n","    # only when using warmup scheduler\n","    # args.total_steps = int(len(train_loader.dataset) / args.batch_size) * args.n_epochs\n","    # args.warmup_steps = int(args.total_steps * args.warmup_ratio)\n","\n","    model = get_model(args)\n","    optimizer = get_optimizer(model, args)\n","    scheduler = get_scheduler(optimizer, args)\n","\n","    best_acc = -1\n","    early_stopping_counter = 0\n","    for epoch in range(args.n_epochs):\n","\n","        print(f\"Start Training: Epoch {epoch + 1}\")\n","\n","        if not args.cv_strategy:\n","            model_name = args.run_name\n","        else:\n","            model_name = f\"{args.run_name.split('.pt')[0]}_{cv_count}.pt\"\n","\n","        # TRAIN\n","        train_acc, train_loss = train(args, model, tokenizer, train_loader, optimizer)\n","\n","        # VALID\n","        acc, val_loss = validate(args, model, tokenizer, valid_loader)\n","\n","        # TODO: model save or early stopping\n","        if args.scheduler == 'plateau':\n","            last_lr = optimizer.param_groups[0]['lr']\n","        else:\n","            last_lr = scheduler.get_last_lr()[0]\n","\n","        print({\"epoch\": epoch, \"train_loss\": train_loss, \"train_acc\": train_acc,\n","                   \"valid_acc\": acc, \"val_loss\": val_loss, \"learning_rate\": last_lr})\n","\n","        if acc > best_acc:\n","            best_acc = acc\n","            # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n","            model_to_save = model.module if hasattr(model, 'module') else model\n","            save_checkpoint({\n","                'epoch': epoch + 1,\n","                'state_dict': model_to_save.state_dict(),\n","            },\n","                args.model_dir, model_name,\n","            )\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","            if early_stopping_counter >= args.patience:\n","                print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n","                break\n","\n","        # scheduler\n","        if args.scheduler == 'plateau':\n","            scheduler.step(best_acc)\n","        else:\n","            scheduler.step()\n","\n","    return best_acc\n","\n","\n","def inference(args, test_data):\n","    # ckpt_file_names = []\n","    all_fold_preds = []\n","    all_fold_argmax_preds = []\n","\n","    if not args.cv_strategy:\n","        ckpt_file_names = [args.model_name]\n","    else:\n","        ckpt_file_names = [f\"{args.model_name.split('.pt')[0]}_{i + 1}.pt\" for i in range(args.fold_num)]\n","\n","    tokenizer = load_tokenizer(args)\n","\n","    for fold_idx, ckpt in enumerate(ckpt_file_names):\n","        model = load_model(args, ckpt)\n","        model.eval()\n","        test_loader = get_loaders(args, None, test_data, True)\n","\n","        total_preds = []\n","        total_argmax_preds = []\n","        total_ids = []\n","\n","        for step, batch in tqdm(enumerate(test_loader), desc='Inferencing', total=len(test_loader)):\n","            idx, text, text_en = batch\n","            tokenized_examples = tokenizer(\n","                text,\n","                text_en,\n","                max_length=args.max_seq_len,\n","                padding=\"max_length\",\n","                return_tensors=\"pt\"\n","            ).to(args.device)\n","\n","            preds = model(**tokenized_examples)\n","\n","            logits = preds['logits']\n","            argmax_logits = torch.argmax(logits, dim=1)\n","\n","            if args.device == 'cuda':\n","                argmax_preds = argmax_logits.to('cpu').detach().numpy()\n","                preds = logits.to('cpu').detach().numpy()\n","            else:  # cpu\n","                argmax_preds = argmax_logits.detach().numpy()\n","                preds = logits.detach().numpy()\n","\n","            total_preds += list(preds)\n","            total_argmax_preds += list(argmax_preds)\n","            total_ids += list(idx)\n","\n","        all_fold_preds.append(total_preds)\n","        all_fold_argmax_preds.append(total_argmax_preds)\n","\n","        output_file_name = \"output.csv\" if not args.cv_strategy else f\"output_{fold_idx + 1}.csv\"\n","        write_path = os.path.join(args.output_dir, output_file_name)\n","        if not os.path.exists(args.output_dir):\n","            os.makedirs(args.output_dir)\n","        with open(write_path, 'w', encoding='utf8') as w:\n","            print(\"writing prediction : {}\".format(write_path))\n","            w.write(\"index,topic_idx\\n\")\n","            for index, p in zip(total_ids, total_argmax_preds):\n","                w.write('{},{}\\n'.format(index, p))\n","\n","    if len(all_fold_preds) > 1:\n","        # Soft voting ensemble\n","        votes = np.sum(all_fold_preds, axis=0)\n","        votes = np.argmax(votes, axis=1)\n","\n","        write_path = os.path.join(args.output_dir, \"output_softvote.csv\")\n","        if not os.path.exists(args.output_dir):\n","            os.makedirs(args.output_dir)\n","        with open(write_path, 'w', encoding='utf8') as w:\n","            print(\"writing prediction : {}\".format(write_path))\n","            w.write(\"index,topic_idx\\n\")\n","            for id, p in zip(total_ids, votes):\n","                w.write('{},{}\\n'.format(id, p))\n","\n","\n","def train(args, model, tokenizer, train_loader, optimizer):\n","    model.train()\n","\n","    total_preds = []\n","    total_targets = []\n","    losses = []\n","    for step, batch in tqdm(enumerate(train_loader), desc='Training', total=len(train_loader)):\n","        idx, text, text_en, label = batch\n","        label = label.to(args.device)\n","        # print(idx[:10])\n","        # print(text[:10])\n","        # print(label[:10])\n","        tokenized_examples = tokenizer(\n","            text,\n","            text_en,\n","            max_length=args.max_seq_len,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        ).to(args.device)\n","\n","        # tokenize\n","        # 모델의 입력으로\n","        # label은 one-hot?\n","        # loss 주고\n","        # argmax를 golden\n","\n","        preds = model(**tokenized_examples)\n","        logits = preds['logits']\n","        softmax_logits = nn.Softmax(dim=1)(logits)\n","        argmax_logits = torch.argmax(logits, dim=1)\n","\n","        # one_hot_logits = one_hot(argmax_logits, num_classes=7).float()\n","        # print(one_hot(argmax_logits, num_classes=7).type(torch.FloatTensor))\n","        loss = compute_loss(logits,\n","                            label, args)\n","\n","        # print(loss)\n","\n","        update_params(loss, model, optimizer, args)\n","\n","        if step % args.log_steps == 0:\n","            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n","\n","        if args.device == 'cuda':\n","            argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","            label = label.to('cpu').detach().numpy()\n","            loss = loss.to('cpu').detach().numpy()\n","            tokenized_examples = tokenized_examples.to('cpu')\n","            logits = logits.to('cpu').detach().numpy()\n","        else:  # cpu\n","            argmax_logits = argmax_logits.detach().numpy()\n","            label = label.detach().numpy()\n","            loss = loss.detach().numpy()\n","\n","        total_preds.append(argmax_logits)\n","        total_targets.append(label)\n","        losses.append(loss)\n","\n","    total_preds = np.concatenate(total_preds)\n","    total_targets = np.concatenate(total_targets)\n","\n","    # Train AUC / ACC\n","    acc = accuracy_score(total_targets, total_preds)\n","    loss_avg = sum(losses) / len(losses)\n","    print(f'TRAIN ACC : {acc}, TRAIN LOSS : {loss_avg}')\n","    return acc, loss_avg\n","\n","\n","def validate(args, model, tokenizer, valid_loader):\n","    model.eval()\n","\n","    total_preds = []\n","    total_targets = []\n","    losses = []\n","    for step, batch in tqdm(enumerate(valid_loader), desc='Training', total=len(valid_loader)):\n","        idx, text, text_en, label = batch\n","        label = label.to(args.device)\n","        tokenized_examples = tokenizer(\n","            text,\n","            text_en,\n","            max_length=args.max_seq_len,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        ).to(args.device)\n","\n","        # tokenize\n","        # 모델의 입력으로\n","        # label은 one-hot?\n","        # loss 주고\n","        # argmax를 golden\n","\n","        preds = model(**tokenized_examples)\n","        logits = preds['logits']\n","        softmax_logits = nn.Softmax(dim=1)(logits)\n","        argmax_logits = torch.argmax(logits, dim=1)\n","\n","        # one_hot_logits = one_hot(argmax_logits, num_classes=7).float()\n","        # print(one_hot(argmax_logits, num_classes=7).type(torch.FloatTensor))\n","        loss = compute_loss(logits,\n","                            label, args)\n","\n","        if step % args.log_steps == 0:\n","            print(f\"Validation steps: {step} Loss: {str(loss.item())}\")\n","\n","        if args.device == 'cuda':\n","            argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","            label = label.to('cpu').detach().numpy()\n","            loss = loss.to('cpu').detach().numpy()\n","            tokenized_examples = tokenized_examples.to('cpu')\n","            logits = logits.to('cpu').detach().numpy()\n","        else:  # cpu\n","            argmax_logits = argmax_logits.detach().numpy()\n","            label = label.detach().numpy()\n","            loss = loss.detach().numpy()\n","\n","        total_preds.append(argmax_logits)\n","        total_targets.append(label)\n","        losses.append(loss)\n","\n","    total_preds = np.concatenate(total_preds)\n","    total_targets = np.concatenate(total_targets)\n","\n","    # Train AUC / ACC\n","    acc = accuracy_score(total_targets, total_preds)\n","    loss_avg = sum(losses) / len(losses)\n","    print(f'VALID ACC : {acc}, VALID LOSS : {loss_avg}')\n","    return acc, loss_avg\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5CS14MP9tFCQ"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"K9DdutwStF4B","executionInfo":{"status":"ok","timestamp":1627057204938,"user_tz":-540,"elapsed":3,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import torch\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","from datetime import datetime\n","from pytz import timezone\n","\n","\n","def main(args):\n","    if not args.run_name:\n","        args.run_name = datetime.now(timezone(\"Asia/Seoul\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n","\n","    set_seeds(args.seed)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    args.device = device\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.tokenizer_name\n","        if args.tokenizer_name\n","        else args.model_name_or_path,\n","        use_fast=True,\n","    )\n","\n","    preprocess = Preprocess(args)\n","    preprocess.load_train_data()\n","    train_data_origin = preprocess.train_data\n","\n","    print(f\"Size of train data : {len(train_data_origin)}\")\n","    # print(f\"size of test data : {len(test_data)}\")\n","\n","    if args.cv_strategy == 'random':\n","        kf = KFold(n_splits=args.fold_num, shuffle=True)\n","        splits = kf.split(X=train_data_origin)\n","    else:\n","        # default\n","        # 여기 각 label로 바꿔야됨\n","        train_labels = [sequence[-1] for sequence in train_data_origin]\n","        skf = StratifiedKFold(n_splits=args.fold_num, shuffle=True)\n","        splits = skf.split(X=train_data_origin, y=train_labels)\n","\n","    acc_avg = 0\n","    for fold_num, (train_index, valid_index) in enumerate(splits):\n","        train_data = train_data_origin[train_index]\n","        valid_data = train_data_origin[valid_index]\n","        best_acc = run(args, tokenizer, train_data, valid_data, fold_num + 1)\n","\n","        if not args.cv_strategy:\n","            break\n","\n","        acc_avg += best_acc\n","\n","    if args.cv_strategy:\n","        acc_avg /= args.fold_num\n","\n","        print(\"*\" * 50, 'auc_avg', \"*\" * 50)\n","        print(acc_avg)\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1EgudubtKjJ"},"source":["## Run"]},{"cell_type":"code","metadata":{"id":"_xupDv9YtKGf","executionInfo":{"status":"ok","timestamp":1627057204938,"user_tz":-540,"elapsed":2,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import argparse\n","import easydict\n","\n","def parse_args():\n","    args = easydict.EasyDict({'run_name' : 'temp',\n","                             'seed':42,\n","                             'device' :'cuda',\n","                             'data_dir': cur_dir + '/data/open/',\n","                             'model_dir' : '/content/drive/MyDrive/KLUE_TC/models/',\n","                             'model_name_or_path' : 'xlm-roberta-large',\n","                             'config_name' : None,\n","                             'tokenizer_name' : None,\n","                             'output_dir' : '/content/drive/MyDrive/KLUE_TC/output/translation/',\n","\n","                             'cv_strategy' : 'stratified',\n","                             'fold_num' : 4,\n","\n","                             'num_workers' : 1,\n","\n","                             # 훈련\n","                             'n_epochs' : 3,\n","                             'batch_size' : 16,\n","                             'lr' : 5e-6,\n","                             'clip_grad' : 10,\n","                             'patience' : 5,\n","                             'max_seq_len' : 80,\n","\n","                             # Optimizer\n","                             'optimizer' : 'adamW',\n","\n","                             # Optimizer-parameters\n","                             'weight_decay' : 0.01,\n","                             'momentum' : 0.9,\n","\n","                             # Scheduler\n","                             'scheduler' : 'step_lr',\n","\n","                             # Scheduler-parameters\n","                             # plateau\n","                             'plateau_patience' : 10,\n","                             'plateau_factor' : 0.5,\n","                              \n","                             't_max' : 10,\n","                             'T_0' : 10,\n","                             'T_mult' : 2,\n","                             '--eta_min' : 0.01,\n","\n","                             # linear_warmup\n","                             'warmup_ratio' : 0.3,\n","\n","                             # Step LR\n","                             'step_size' : 50,\n","                             'gamma' : 0.1,\n","\n","                             'criterion' : 'CE',\n","\n","                             'log_steps' : 100})\n","    \n","    return args"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YKuE65Ct3d34","executionInfo":{"status":"ok","timestamp":1627065740393,"user_tz":-540,"elapsed":8535457,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"7735bbfd-8f0e-4874-8358-52890edaf75c"},"source":["if __name__ == '__main__':\n","    args = parse_args()\n","    main(args)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Size of train data : 45654\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Start Training: Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2140 [00:00<11:42,  3.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 1.9897428750991821\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2140 [00:30<10:25,  3.26it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 1.7341123819351196\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2140 [01:00<09:45,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.9641258120536804\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2140 [01:30<09:13,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.5547016859054565\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2140 [02:00<08:44,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.4257950484752655\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2140 [02:31<08:14,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.1729910522699356\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2140 [03:01<07:45,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.28079795837402344\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2140 [03:31<07:14,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.8450873494148254\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2140 [04:01<06:46,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.186244398355484\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2140 [04:31<06:14,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.2889895737171173\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2140 [05:01<05:44,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.4411683678627014\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2140 [05:32<05:12,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.2728343605995178\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2140 [06:02<04:42,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.3200002908706665\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2140 [06:32<04:13,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.42877987027168274\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2140 [07:02<03:42,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.5136380195617676\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2140 [07:32<03:13,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.4456239342689514\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2140 [08:02<02:42,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.23705464601516724\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2140 [08:33<02:12,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.4352942109107971\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2140 [09:03<01:42,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.779090166091919\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2140 [09:33<01:11,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.2852676212787628\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 2001/2140 [10:03<00:42,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.6911035180091858\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2140 [10:33<00:11,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.46843504905700684\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2140/2140 [10:45<00:00,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.8168224299065421, TRAIN LOSS : 0.5582443712914589\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 0/714 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.3306698203086853\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 102/714 [00:07<00:44, 13.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.6280769109725952\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 202/714 [00:14<00:36, 13.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.560386598110199\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 302/714 [00:21<00:29, 13.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.1931847482919693\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 402/714 [00:28<00:22, 13.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.5967592597007751\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 502/714 [00:35<00:15, 13.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.2451133131980896\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 602/714 [00:43<00:08, 13.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 0.08712708950042725\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 702/714 [00:50<00:00, 13.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.08300890028476715\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [00:51<00:00, 13.95it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["VALID ACC : 0.8769055545820922, VALID LOSS : 0.4290382828288229\n","{'epoch': 0, 'train_loss': 0.5582443712914589, 'train_acc': 0.8168224299065421, 'valid_acc': 0.8769055545820922, 'val_loss': 0.4290382828288229, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2140 [00:00<11:58,  2.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.4141247868537903\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2140 [00:30<10:18,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.24946463108062744\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2140 [01:00<09:43,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.10348251461982727\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2140 [01:30<09:13,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.05872631445527077\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2140 [02:01<08:45,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.057598553597927094\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2140 [02:31<08:13,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.7009661197662354\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2140 [03:01<07:44,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.2570623457431793\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2140 [03:31<07:16,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.4390595555305481\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2140 [04:01<06:44,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.1896304041147232\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2140 [04:31<06:14,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.2654356062412262\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2140 [05:02<05:43,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.23753830790519714\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2140 [05:32<05:13,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.7125740051269531\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2140 [06:02<04:44,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.12812848389148712\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2140 [06:32<04:12,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.1644313633441925\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2140 [07:02<03:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.7269488573074341\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2140 [07:33<03:13,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.1565258800983429\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2140 [08:03<02:42,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.16285711526870728\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2140 [08:33<02:12,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.26216214895248413\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2140 [09:03<01:42,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.3501555025577545\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2140 [09:33<01:12,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.6464861035346985\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 2001/2140 [10:04<00:41,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.19527855515480042\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2140 [10:34<00:11,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.3786007761955261\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2140/2140 [10:46<00:00,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.890303738317757, TRAIN LOSS : 0.34002564149934833\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 0/714 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.5913920402526855\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 102/714 [00:07<00:43, 13.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.3875342309474945\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 202/714 [00:14<00:36, 13.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.5109320282936096\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 302/714 [00:21<00:29, 13.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.363810658454895\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 402/714 [00:28<00:22, 13.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.5289619565010071\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 502/714 [00:36<00:15, 13.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.25204575061798096\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 602/714 [00:43<00:08, 13.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 0.13212142884731293\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 702/714 [00:50<00:00, 13.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.28712883591651917\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [00:51<00:00, 13.92it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["VALID ACC : 0.8850534431400036, VALID LOSS : 0.37585159776281013\n","{'epoch': 1, 'train_loss': 0.34002564149934833, 'train_acc': 0.890303738317757, 'valid_acc': 0.8850534431400036, 'val_loss': 0.37585159776281013, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2140 [00:00<11:39,  3.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.25894033908843994\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2140 [00:30<10:15,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.5096768736839294\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2140 [01:00<09:44,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.41200461983680725\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2140 [01:30<09:12,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.10452470183372498\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2140 [02:01<08:46,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.11743313074111938\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2140 [02:31<08:14,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.5552412867546082\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2140 [03:01<07:44,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.3423633575439453\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2140 [03:31<07:16,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.5977783799171448\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2140 [04:01<06:43,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.4266216456890106\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2140 [04:31<06:13,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.029345616698265076\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2140 [05:02<05:43,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.03743874654173851\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2140 [05:32<05:14,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.26818642020225525\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2140 [06:02<04:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.11583337187767029\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2140 [06:32<04:14,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.17409613728523254\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2140 [07:02<03:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.5255331993103027\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2140 [07:32<03:14,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.4368504285812378\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2140 [08:03<02:43,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.44165897369384766\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2140 [08:33<02:12,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.40463897585868835\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2140 [09:03<01:42,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.5188642144203186\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2140 [09:33<01:12,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.3658648133277893\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 2001/2140 [10:03<00:42,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.34665748476982117\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2140 [10:33<00:11,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.2601730525493622\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2140/2140 [10:45<00:00,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9076810747663552, TRAIN LOSS : 0.2870838575961643\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 0/714 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 1.2986257076263428\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 102/714 [00:07<00:44, 13.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.2099766731262207\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 202/714 [00:14<00:36, 13.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.4684719443321228\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 302/714 [00:21<00:29, 13.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.33055561780929565\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 402/714 [00:28<00:22, 13.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.4659135341644287\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 502/714 [00:36<00:15, 13.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.4280887842178345\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 602/714 [00:43<00:08, 13.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 0.2422022521495819\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 702/714 [00:50<00:00, 13.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.14184635877609253\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [00:51<00:00, 13.91it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["VALID ACC : 0.8824250919922901, VALID LOSS : 0.40241467805594955\n","{'epoch': 2, 'train_loss': 0.2870838575961643, 'train_acc': 0.9076810747663552, 'valid_acc': 0.8824250919922901, 'val_loss': 0.40241467805594955, 'learning_rate': 5e-06}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Start Training: Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2140 [00:00<11:26,  3.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 1.9261094331741333\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2140 [00:30<10:16,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 1.9245063066482544\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2140 [01:00<09:45,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.9124456644058228\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2140 [01:31<09:16,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.44590920209884644\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2140 [02:01<08:47,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.7293850183486938\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2140 [02:31<08:18,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.3051021099090576\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2140 [03:01<07:44,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.40683916211128235\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2140 [03:32<07:16,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.3692683279514313\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2140 [04:02<06:46,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.39839649200439453\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2140 [04:32<06:15,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.2729201316833496\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2140 [05:02<05:44,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.18370383977890015\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2140 [05:33<05:14,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.29469746351242065\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2140 [06:03<04:44,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.27185487747192383\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2140 [06:33<04:13,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.27431440353393555\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2140 [07:03<03:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.297188937664032\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2140 [07:33<03:13,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.3202686607837677\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2140 [08:04<02:42,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.3076959550380707\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2140 [08:34<02:13,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.48654526472091675\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2140 [09:04<01:42,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.34483978152275085\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2140 [09:34<01:12,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.5504066944122314\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 2001/2140 [10:04<00:42,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.9984256625175476\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2140 [10:35<00:11,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.5052536129951477\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2140/2140 [10:47<00:00,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.8114778037383178, TRAIN LOSS : 0.5614246631093393\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 0/714 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.36025938391685486\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 102/714 [00:07<00:43, 14.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.38324132561683655\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 202/714 [00:14<00:36, 14.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 1.0261690616607666\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 302/714 [00:21<00:29, 13.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.5923615097999573\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 402/714 [00:28<00:22, 14.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.07529950886964798\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 502/714 [00:35<00:15, 14.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.15947791934013367\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 602/714 [00:42<00:07, 14.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 0.9353556632995605\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 702/714 [00:50<00:00, 13.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.1138557717204094\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [00:51<00:00, 13.99it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["VALID ACC : 0.8735763097949886, VALID LOSS : 0.41570401104644505\n","{'epoch': 0, 'train_loss': 0.5614246631093393, 'train_acc': 0.8114778037383178, 'valid_acc': 0.8735763097949886, 'val_loss': 0.41570401104644505, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2140 [00:00<20:56,  1.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.07444313168525696\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2140 [00:30<10:20,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.25323402881622314\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2140 [01:01<09:46,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.805570662021637\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2140 [01:31<09:17,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.5163930654525757\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2140 [02:01<08:45,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.5426090359687805\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2140 [02:31<08:16,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.30474767088890076\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2140 [03:01<07:43,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.07140367478132248\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2140 [03:32<07:16,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.44133061170578003\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2140 [04:02<06:45,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.23886796832084656\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2140 [04:32<06:13,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.4925195872783661\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2140 [05:02<05:44,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.38675248622894287\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2140 [05:32<05:12,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.02265198528766632\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2140 [06:03<04:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.5998172163963318\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2140 [06:33<04:13,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.21370428800582886\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2140 [07:03<03:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.12714040279388428\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2140 [07:33<03:14,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.18414686620235443\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2140 [08:03<02:41,  3.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.028110966086387634\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2140 [08:34<02:12,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.5289005041122437\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2140 [09:04<01:42,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.06774347275495529\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2140 [09:34<01:12,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.19492468237876892\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 2001/2140 [10:04<00:41,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.04771619662642479\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2140 [10:34<00:11,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.7364579439163208\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2140/2140 [10:46<00:00,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.8908878504672897, TRAIN LOSS : 0.33287469067776176\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 0/714 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.2383602261543274\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 102/714 [00:07<00:44, 13.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.5219048857688904\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 202/714 [00:14<00:36, 13.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.5701873898506165\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 302/714 [00:21<00:29, 13.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.37409508228302\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 402/714 [00:28<00:22, 13.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.17208991944789886\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 502/714 [00:36<00:15, 13.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.03822615370154381\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 602/714 [00:43<00:08, 13.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 0.846095085144043\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 702/714 [00:50<00:00, 14.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.20597510039806366\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [00:51<00:00, 13.91it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["VALID ACC : 0.8876817942877169, VALID LOSS : 0.3865162525704282\n","{'epoch': 1, 'train_loss': 0.33287469067776176, 'train_acc': 0.8908878504672897, 'valid_acc': 0.8876817942877169, 'val_loss': 0.3865162525704282, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2140 [00:00<11:38,  3.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.2834719717502594\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2140 [00:30<10:20,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.4028545618057251\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2140 [01:00<09:47,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.21911029517650604\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2140 [01:31<09:15,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.0884355828166008\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2140 [02:01<08:47,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.10509134083986282\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2140 [02:31<08:15,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.11999189108610153\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2140 [03:01<07:42,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.022268878296017647\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2140 [03:31<07:14,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.4562908113002777\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2140 [04:02<06:42,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.5570234060287476\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2140 [04:32<06:13,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.1478678286075592\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2140 [05:02<05:45,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.3036172389984131\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2140 [05:32<05:15,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.42070087790489197\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2140 [06:02<04:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.5967422127723694\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2140 [06:33<04:12,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.31630465388298035\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2140 [07:03<03:43,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.7564688324928284\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2140 [07:33<03:12,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.3778105676174164\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2140 [08:03<02:42,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.05675987899303436\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2140 [08:33<02:12,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.40071621537208557\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2140 [09:03<01:43,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.3073666989803314\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2140 [09:34<01:11,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.05771259590983391\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 2001/2140 [10:04<00:42,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 1.140985131263733\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2140 [10:34<00:11,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.2697735130786896\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2140/2140 [10:46<00:00,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9052570093457943, TRAIN LOSS : 0.2859406569339422\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 0/714 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.31054940819740295\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 102/714 [00:07<00:43, 13.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.4565780758857727\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 202/714 [00:14<00:36, 13.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.34713464975357056\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 302/714 [00:21<00:29, 13.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.3649244010448456\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 402/714 [00:28<00:22, 13.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.13976003229618073\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 502/714 [00:36<00:15, 13.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.0283679086714983\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 602/714 [00:43<00:08, 13.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 0.9161330461502075\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 702/714 [00:50<00:00, 13.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.2534182667732239\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [00:51<00:00, 13.90it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["VALID ACC : 0.8861923953040126, VALID LOSS : 0.40078553763943453\n","{'epoch': 2, 'train_loss': 0.2859406569339422, 'train_acc': 0.9052570093457943, 'valid_acc': 0.8861923953040126, 'val_loss': 0.40078553763943453, 'learning_rate': 5e-06}\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Start Training: Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2141 [00:00<11:56,  2.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 1.8811229467391968\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2141 [00:30<10:16,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 1.8895790576934814\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2141 [01:00<09:48,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.9796773791313171\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2141 [01:31<09:15,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.5594099164009094\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2141 [02:01<08:46,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.6963773369789124\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2141 [02:32<08:14,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.8195551633834839\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2141 [03:02<07:45,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.3299283981323242\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2141 [03:32<07:16,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.8234213590621948\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2141 [04:02<06:46,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.4926972985267639\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2141 [04:33<06:15,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.1894388645887375\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2141 [05:03<05:45,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.8835515379905701\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2141 [05:33<05:15,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.25986772775650024\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2141 [06:04<04:45,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.9311360120773315\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2141 [06:34<04:12,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.29713568091392517\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2141 [07:04<03:44,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.36657020449638367\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2141 [07:34<03:13,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.4779397249221802\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2141 [08:04<02:44,  3.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.6354064345359802\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2141 [08:35<02:13,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.4084497094154358\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2141 [09:05<01:42,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.4312728941440582\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2141 [09:35<01:12,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.10772377997636795\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 2001/2141 [10:05<00:42,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.5456793904304504\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2141 [10:36<00:12,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.5402238965034485\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2141/2141 [10:48<00:00,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.810169095528752, TRAIN LOSS : 0.5711602736299267\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 0/714 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.5877512693405151\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 102/714 [00:07<00:44, 13.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.9711741209030151\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 202/714 [00:14<00:36, 13.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.4672229290008545\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 302/714 [00:21<00:29, 13.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.4242030680179596\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 402/714 [00:28<00:22, 13.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.2157527059316635\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 502/714 [00:36<00:15, 13.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.009954674169421196\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 602/714 [00:43<00:07, 14.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 1.1460134983062744\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 702/714 [00:50<00:00, 13.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.12319979816675186\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [00:51<00:00, 13.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["VALID ACC : 0.8811881188118812, VALID LOSS : 0.38382761795347664\n","{'epoch': 0, 'train_loss': 0.5711602736299267, 'train_acc': 0.810169095528752, 'valid_acc': 0.8811881188118812, 'val_loss': 0.38382761795347664, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2141 [00:00<11:35,  3.08it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.24837109446525574\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2141 [00:30<10:21,  3.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.19071967899799347\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2141 [01:00<09:46,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.2863507568836212\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2141 [01:31<09:15,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.15223845839500427\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2141 [02:01<08:46,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.40556421875953674\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2141 [02:31<08:16,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.35795751214027405\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2141 [03:01<07:44,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.0782284215092659\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2141 [03:32<07:12,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.07291832566261292\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2141 [04:02<06:46,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.11296039819717407\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2141 [04:32<06:15,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.09791231155395508\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2141 [05:02<05:44,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.16578394174575806\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2141 [05:32<05:13,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.17241746187210083\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2141 [06:03<04:44,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.08708691596984863\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2141 [06:33<04:13,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.13239124417304993\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2141 [07:03<03:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.6963334083557129\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2141 [07:33<03:14,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.15026408433914185\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2141 [08:03<02:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.6306202411651611\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2141 [08:34<02:12,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.19636766612529755\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2141 [09:04<01:43,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.7991535663604736\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2141 [09:34<01:12,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.27557820081710815\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 2001/2141 [10:04<00:42,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.12922316789627075\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2141 [10:35<00:12,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.12244214862585068\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2141/2141 [10:46<00:00,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.8879705616074297, TRAIN LOSS : 0.35089807107388654\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 0/714 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.5842822790145874\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 102/714 [00:07<00:44, 13.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 1.0419609546661377\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 202/714 [00:14<00:36, 13.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.44112861156463623\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 302/714 [00:21<00:29, 13.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.3603275418281555\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 402/714 [00:28<00:22, 13.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.16815711557865143\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 502/714 [00:36<00:15, 13.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.008044867776334286\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 602/714 [00:43<00:08, 13.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 1.0366328954696655\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 702/714 [00:50<00:00, 13.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.15446403622627258\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [00:51<00:00, 13.91it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["VALID ACC : 0.8880224305616402, VALID LOSS : 0.3689098885936365\n","{'epoch': 1, 'train_loss': 0.35089807107388654, 'train_acc': 0.8879705616074297, 'valid_acc': 0.8880224305616402, 'val_loss': 0.3689098885936365, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2141 [00:05<3:18:12,  5.56s/it]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.3822324573993683\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2141 [00:35<10:21,  3.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.23589156568050385\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2141 [01:06<09:46,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.13740698993206024\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2141 [01:36<09:16,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.06265519559383392\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2141 [02:06<08:43,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.017479265108704567\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2141 [02:36<08:13,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.34987398982048035\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2141 [03:06<07:45,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.16932426393032074\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2141 [03:37<07:14,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.09487131983041763\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2141 [04:07<06:44,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.6017641425132751\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2141 [04:37<06:14,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.3037278652191162\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2141 [05:07<05:44,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.6651172041893005\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2141 [05:37<05:15,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.5318798422813416\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2141 [06:08<04:44,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.17648963630199432\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2141 [06:38<04:12,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.3290310502052307\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2141 [07:08<03:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.31623825430870056\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2141 [07:38<03:14,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.1901938021183014\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2141 [08:08<02:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.0859891027212143\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2141 [08:39<02:12,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.1989666223526001\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2141 [09:09<01:42,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.05832965672016144\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2141 [09:39<01:12,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.4737023711204529\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 2001/2141 [10:09<00:42,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.38601037859916687\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2141 [10:40<00:12,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.19669252634048462\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2141/2141 [10:52<00:00,  3.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9038871528284804, TRAIN LOSS : 0.29183466899387506\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 0/714 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.8039504289627075\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 102/714 [00:07<00:44, 13.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 1.044792890548706\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 202/714 [00:14<00:36, 14.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.21242831647396088\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 302/714 [00:21<00:29, 13.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.42950335144996643\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 402/714 [00:28<00:22, 13.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.19241847097873688\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 502/714 [00:35<00:15, 13.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.008978331461548805\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 602/714 [00:43<00:08, 13.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 1.670079231262207\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 702/714 [00:50<00:00, 13.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.10365107655525208\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [00:51<00:00, 13.92it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["VALID ACC : 0.8884605274686761, VALID LOSS : 0.37798752112794115\n","{'epoch': 2, 'train_loss': 0.29183466899387506, 'train_acc': 0.9038871528284804, 'valid_acc': 0.8884605274686761, 'val_loss': 0.37798752112794115, 'learning_rate': 5e-06}\n","saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Start Training: Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2141 [00:00<31:20,  1.14it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 2.0064868927001953\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2141 [00:31<10:22,  3.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 1.8158384561538696\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2141 [01:01<09:45,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.7586265206336975\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2141 [01:31<09:16,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.40253207087516785\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2141 [02:01<08:47,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.35808265209198\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2141 [02:32<08:15,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.2793596386909485\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2141 [03:02<07:44,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.30841943621635437\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2141 [03:32<07:14,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.8591518402099609\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2141 [04:02<06:45,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.35357528924942017\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2141 [04:33<06:15,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.4491084814071655\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2141 [05:03<05:44,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.09831059724092484\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2141 [05:33<05:15,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.6010152101516724\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2141 [06:03<04:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.736746609210968\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2141 [06:34<04:15,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.35815876722335815\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2141 [07:04<03:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.4035674035549164\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2141 [07:34<03:14,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.4062543511390686\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2141 [08:04<02:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.515329897403717\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2141 [08:34<02:12,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.7883517146110535\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2141 [09:05<01:42,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.6653663516044617\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2141 [09:35<01:12,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.05338694155216217\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 2001/2141 [10:05<00:42,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.5256528854370117\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2141 [10:35<00:12,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.6044194102287292\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2141/2141 [10:47<00:00,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.8194854122251103, TRAIN LOSS : 0.5477244263059873\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 0/714 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.021326981484889984\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 102/714 [00:07<00:44, 13.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.16184721887111664\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 202/714 [00:14<00:36, 13.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.3742023706436157\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 302/714 [00:21<00:29, 13.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.6117756962776184\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 402/714 [00:28<00:22, 14.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.5419984459877014\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 502/714 [00:36<00:15, 13.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.009459269233047962\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 602/714 [00:43<00:08, 13.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 0.8273007273674011\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 702/714 [00:50<00:00, 13.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.12869815528392792\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [00:51<00:00, 13.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["VALID ACC : 0.8741785682993078, VALID LOSS : 0.3841330075573896\n","{'epoch': 0, 'train_loss': 0.5477244263059873, 'train_acc': 0.8194854122251103, 'valid_acc': 0.8741785682993078, 'val_loss': 0.3841330075573896, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2141 [00:00<11:34,  3.08it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.4772661030292511\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2141 [00:30<10:19,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.38322582840919495\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2141 [01:00<09:49,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.34168216586112976\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2141 [01:31<09:18,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.23446588218212128\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2141 [02:01<08:45,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.6078871488571167\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2141 [02:31<08:17,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.31473496556282043\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2141 [03:01<07:47,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.07040185481309891\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2141 [03:32<07:15,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.533821165561676\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2141 [04:02<06:45,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.42027485370635986\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2141 [04:32<06:15,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.22990164160728455\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2141 [05:02<05:43,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.3614605963230133\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2141 [05:32<05:13,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.2526368200778961\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2141 [06:03<04:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.6615341901779175\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2141 [06:33<04:15,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.3380335867404938\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2141 [07:03<03:42,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.7381388545036316\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2141 [07:33<03:14,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.34515029191970825\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2141 [08:03<02:43,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.17564928531646729\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2141 [08:34<02:12,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.4551556706428528\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2141 [09:04<01:42,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.6995732188224792\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2141 [09:34<01:13,  3.28it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.6448945999145508\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 2001/2141 [10:04<00:42,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.4502802789211273\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2141 [10:34<00:12,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.1745445877313614\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2141/2141 [10:46<00:00,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.890452965742823, TRAIN LOSS : 0.33614104268276374\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 0/714 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.02586800791323185\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 102/714 [00:07<00:44, 13.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.1690395474433899\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 202/714 [00:14<00:37, 13.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.4177192449569702\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 302/714 [00:21<00:29, 13.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.6718763709068298\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 402/714 [00:28<00:22, 13.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.5719658732414246\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 502/714 [00:36<00:15, 13.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.005379393696784973\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 602/714 [00:43<00:08, 13.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 0.754355788230896\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 702/714 [00:50<00:00, 13.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.145980566740036\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [00:51<00:00, 13.92it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["VALID ACC : 0.8849557522123894, VALID LOSS : 0.3650467043034263\n","{'epoch': 1, 'train_loss': 0.33614104268276374, 'train_acc': 0.890452965742823, 'valid_acc': 0.8849557522123894, 'val_loss': 0.3650467043034263, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/2141 [00:00<11:56,  2.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.23724734783172607\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   5%|▍         | 101/2141 [00:30<10:19,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.20180627703666687\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 201/2141 [01:00<09:44,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.37199917435646057\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 301/2141 [01:31<09:15,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.1678474396467209\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▊        | 401/2141 [02:01<08:44,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.25203976035118103\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  23%|██▎       | 501/2141 [02:31<08:16,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.3342229127883911\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 601/2141 [03:01<07:46,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.060539357364177704\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  33%|███▎      | 701/2141 [03:31<07:17,  3.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.39473098516464233\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 801/2141 [04:01<06:45,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.4048183262348175\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 901/2141 [04:32<06:13,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.08846389502286911\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 1001/2141 [05:02<05:44,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.1695086508989334\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  51%|█████▏    | 1101/2141 [05:32<05:15,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1100 Loss: 0.3836090564727783\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 1201/2141 [06:02<04:40,  3.35it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1200 Loss: 0.03833664208650589\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  61%|██████    | 1301/2141 [06:32<04:12,  3.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1300 Loss: 0.04005080834031105\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 1401/2141 [07:03<03:42,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1400 Loss: 0.05914650112390518\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 1501/2141 [07:33<03:13,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1500 Loss: 0.37903258204460144\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 1601/2141 [08:03<02:43,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1600 Loss: 0.3592953383922577\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  79%|███████▉  | 1701/2141 [08:33<02:13,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1700 Loss: 0.1956956833600998\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 1801/2141 [09:04<01:42,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1800 Loss: 0.06098828464746475\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  89%|████████▉ | 1901/2141 [09:34<01:12,  3.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1900 Loss: 0.3512175381183624\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 2001/2141 [10:04<00:42,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2000 Loss: 0.2855571508407593\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 2101/2141 [10:34<00:11,  3.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 2100 Loss: 0.399824857711792\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 2141/2141 [10:46<00:00,  3.31it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.908121842235916, TRAIN LOSS : 0.2831622151260354\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 0/714 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.023209353908896446\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  14%|█▍        | 102/714 [00:07<00:44, 13.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.1618785709142685\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 202/714 [00:14<00:36, 13.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.3742953836917877\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  42%|████▏     | 302/714 [00:21<00:29, 13.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.7475258708000183\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▋    | 402/714 [00:28<00:22, 13.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 400 Loss: 0.5344774723052979\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  70%|███████   | 502/714 [00:36<00:15, 13.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 500 Loss: 0.006976488512009382\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 602/714 [00:43<00:08, 13.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 600 Loss: 0.8003606796264648\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  98%|█████████▊| 702/714 [00:50<00:00, 13.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 700 Loss: 0.044075172394514084\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 714/714 [00:51<00:00, 13.94it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["VALID ACC : 0.8860071847892754, VALID LOSS : 0.3759598066601172\n","{'epoch': 2, 'train_loss': 0.2831622151260354, 'train_acc': 0.908121842235916, 'valid_acc': 0.8860071847892754, 'val_loss': 0.3759598066601172, 'learning_rate': 5e-06}\n","saving model ...\n","************************************************** auc_avg **************************************************\n","0.886800737421418\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YfVIY2z0kZZx","executionInfo":{"status":"ok","timestamp":1627065740393,"user_tz":-540,"elapsed":13,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["# torch.cuda.empty_cache()"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w4nwVSlNpLG-","executionInfo":{"status":"ok","timestamp":1627065740394,"user_tz":-540,"elapsed":7,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"5e438517-1d67-4689-a5b5-5aed847404e8"},"source":["!nvidia-smi"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Fri Jul 23 18:42:18 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0    39W / 300W |  14817MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"txpJN5vFPWCq"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"hUWUggp_PQqy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627066095083,"user_tz":-540,"elapsed":354693,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"eb805ed6-2d9a-40d2-8ce4-27c8580508fc"},"source":["def inference_main():\n","    args = parse_args()\n","    args.model_name = \"temp\"\n","    preprocess = Preprocess(args)\n","    preprocess.load_test_data()\n","    test_data = preprocess.test_data\n","\n","    print(f\"size of test data : {len(test_data)}\")\n","    torch.cuda.empty_cache()\n","    # del model\n","    inference(args, test_data)\n","\n","inference_main()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["size of test data : 9131\n","Loading Model from: /content/drive/MyDrive/KLUE_TC/models/temp_1.pt\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/drive/MyDrive/KLUE_TC/models/temp_1.pt were not used when initializing XLMRobertaForSequenceClassification: ['state_dict', 'epoch']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/KLUE_TC/models/temp_1.pt and are newly initialized: ['encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'classifier.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.12.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.12.output.dense.bias', 'classifier.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.20.intermediate.dense.weight', 'classifier.out_proj.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'classifier.out_proj.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.16.attention.output.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["XLMRobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n","      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=1024, out_features=7, bias=True)\n","  )\n",")\n","Loading Model from: /content/drive/MyDrive/KLUE_TC/models/temp_1.pt ...Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["Inferencing: 100%|██████████| 571/571 [00:40<00:00, 14.06it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["writing prediction : /content/drive/MyDrive/KLUE_TC/output/translation/output_1.csv\n","Loading Model from: /content/drive/MyDrive/KLUE_TC/models/temp_2.pt\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/drive/MyDrive/KLUE_TC/models/temp_2.pt were not used when initializing XLMRobertaForSequenceClassification: ['state_dict', 'epoch']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/KLUE_TC/models/temp_2.pt and are newly initialized: ['encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'classifier.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.12.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.12.output.dense.bias', 'classifier.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.20.intermediate.dense.weight', 'classifier.out_proj.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'classifier.out_proj.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.16.attention.output.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["XLMRobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n","      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=1024, out_features=7, bias=True)\n","  )\n",")\n","Loading Model from: /content/drive/MyDrive/KLUE_TC/models/temp_2.pt ...Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["Inferencing: 100%|██████████| 571/571 [00:44<00:00, 12.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["writing prediction : /content/drive/MyDrive/KLUE_TC/output/translation/output_2.csv\n","Loading Model from: /content/drive/MyDrive/KLUE_TC/models/temp_3.pt\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/drive/MyDrive/KLUE_TC/models/temp_3.pt were not used when initializing XLMRobertaForSequenceClassification: ['state_dict', 'epoch']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/KLUE_TC/models/temp_3.pt and are newly initialized: ['encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'classifier.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.12.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.12.output.dense.bias', 'classifier.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.20.intermediate.dense.weight', 'classifier.out_proj.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'classifier.out_proj.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.16.attention.output.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["XLMRobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n","      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=1024, out_features=7, bias=True)\n","  )\n",")\n","Loading Model from: /content/drive/MyDrive/KLUE_TC/models/temp_3.pt ...Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["Inferencing: 100%|██████████| 571/571 [00:42<00:00, 13.36it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["writing prediction : /content/drive/MyDrive/KLUE_TC/output/translation/output_3.csv\n","Loading Model from: /content/drive/MyDrive/KLUE_TC/models/temp_4.pt\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at /content/drive/MyDrive/KLUE_TC/models/temp_4.pt were not used when initializing XLMRobertaForSequenceClassification: ['state_dict', 'epoch']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/KLUE_TC/models/temp_4.pt and are newly initialized: ['encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'classifier.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.12.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.12.output.dense.bias', 'classifier.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.20.intermediate.dense.weight', 'classifier.out_proj.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'classifier.out_proj.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.16.attention.output.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["XLMRobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n","      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=1024, out_features=7, bias=True)\n","  )\n",")\n","Loading Model from: /content/drive/MyDrive/KLUE_TC/models/temp_4.pt ...Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["Inferencing: 100%|██████████| 571/571 [00:42<00:00, 13.32it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["writing prediction : /content/drive/MyDrive/KLUE_TC/output/translation/output_4.csv\n","writing prediction : /content/drive/MyDrive/KLUE_TC/output/translation/output_softvote.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LaRxISbnmOXG","executionInfo":{"status":"ok","timestamp":1627066095083,"user_tz":-540,"elapsed":17,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":[""],"execution_count":12,"outputs":[]}]}
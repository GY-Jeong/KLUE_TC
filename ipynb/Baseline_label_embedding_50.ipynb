{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Baseline_label_embedding_50.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1ptkq8DnHrsoMn0eKSxWhZq8E1RvsRXy5","authorship_tag":"ABX9TyNOaikR7IFAemtf1/DPdH6g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"569686078e124ada8a6d684aa5b5aacf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ffaba92c096b4cbda55618d290036288","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d3082b15ec0347c9960bff459c3a28c4","IPY_MODEL_51aa38974e4a48ada9ef9b59b3d45785"]}},"ffaba92c096b4cbda55618d290036288":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d3082b15ec0347c9960bff459c3a28c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6dca2cdf0ecf4ae4a44d94360645a8e4","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":337,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":337,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c95fe081cb0d41a7916854887da3969e"}},"51aa38974e4a48ada9ef9b59b3d45785":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bb078edb727f44f8a435d5da0fdac3de","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 337/337 [00:01&lt;00:00, 304B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f487a71788d4a1f83f283cf2b251e44"}},"6dca2cdf0ecf4ae4a44d94360645a8e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c95fe081cb0d41a7916854887da3969e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bb078edb727f44f8a435d5da0fdac3de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4f487a71788d4a1f83f283cf2b251e44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"96c2995be9db4529bafd3990f08c50a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0f630e9dbd094a828dd2d7a98b2cce60","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2e07def207e944929d26fdd785128f99","IPY_MODEL_78d9ae50327548eeb0c192e95c5c8cb7"]}},"0f630e9dbd094a828dd2d7a98b2cce60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2e07def207e944929d26fdd785128f99":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_21176568341c47839c0895848064598f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":547,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":547,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bee626ef0c5440759d89ef385828b08d"}},"78d9ae50327548eeb0c192e95c5c8cb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7aeec550e1ed4ee9bb82c919849e79af","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 547/547 [00:00&lt;00:00, 1.45kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7511ddfbfb644fb59b9e87f611a2ff0e"}},"21176568341c47839c0895848064598f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bee626ef0c5440759d89ef385828b08d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7aeec550e1ed4ee9bb82c919849e79af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7511ddfbfb644fb59b9e87f611a2ff0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cf9eea1dec944f929b9ac9e571956016":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_693600af07614c4d8802b85ee7e76b50","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_39676a680b6443d182b52a76487e3fda","IPY_MODEL_42e4220994d14f3396dac9d62ccbbeaf"]}},"693600af07614c4d8802b85ee7e76b50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"39676a680b6443d182b52a76487e3fda":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_830ba2ae53ab46df86ea2c105a3aa162","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":248477,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":248477,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3d6db41c939d4b98aeba6baafb1fe963"}},"42e4220994d14f3396dac9d62ccbbeaf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8dc058ece5214f5d881ec1f931e89ef4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 248k/248k [00:02&lt;00:00, 107kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_89394e2a86ba42939cd5861024637a68"}},"830ba2ae53ab46df86ea2c105a3aa162":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3d6db41c939d4b98aeba6baafb1fe963":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8dc058ece5214f5d881ec1f931e89ef4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"89394e2a86ba42939cd5861024637a68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1d35b492cc1e44849dd2dbe43123715d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_439e0060f08740a887447a8cd4286636","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a23d6b8269ca40c992b72a36759acd9b","IPY_MODEL_9bb9745f330042d98f0749ebd52216e8"]}},"439e0060f08740a887447a8cd4286636":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a23d6b8269ca40c992b72a36759acd9b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f0527887a9474519b397bcb6d20df029","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":173,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":173,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0a8f238b409c4ddca5d06153fae963a5"}},"9bb9745f330042d98f0749ebd52216e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8751ea784ed740cf85bf8b2f080462c1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 173/173 [00:00&lt;00:00, 231B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_17080c8db2f34c55857ff44c205df0d7"}},"f0527887a9474519b397bcb6d20df029":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0a8f238b409c4ddca5d06153fae963a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8751ea784ed740cf85bf8b2f080462c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"17080c8db2f34c55857ff44c205df0d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"adbe65bebfe34e04adb79f5cf21bc1ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e175bce5152a40be8650f8878ab1d53d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4030c74d89c343ccb4933aea256de58d","IPY_MODEL_9c9af8d0c94c4f03ad4200bdb3893cae"]}},"e175bce5152a40be8650f8878ab1d53d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4030c74d89c343ccb4933aea256de58d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4762c6b626c043edab28fc0b226ea25b","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1346854671,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1346854671,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_906056f24a74486b8306d569419d05da"}},"9c9af8d0c94c4f03ad4200bdb3893cae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_005b49b3876344739bc295e119423f5b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.35G/1.35G [00:51&lt;00:00, 25.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1caaf94e6cad4e7d988bf321c864b9bd"}},"4762c6b626c043edab28fc0b226ea25b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"906056f24a74486b8306d569419d05da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"005b49b3876344739bc295e119423f5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1caaf94e6cad4e7d988bf321c864b9bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbWmvczoKOwQ","executionInfo":{"status":"ok","timestamp":1627582795122,"user_tz":-540,"elapsed":503,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"2cd7ff99-e59a-4bdd-dd2d-dede6563cb70"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Thu Jul 29 18:19:52 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4kthgVPqr5VC"},"source":["## Directory 설정, 구글 드라이브 import"]},{"cell_type":"code","metadata":{"id":"FOkMqa8hrHl_","executionInfo":{"status":"ok","timestamp":1627582795525,"user_tz":-540,"elapsed":5,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["cur_dir = '/content/drive/MyDrive/KLUE_TC'"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jx_d93v9rzQI"},"source":["## Utils"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2KbbvLPitR1N","executionInfo":{"status":"ok","timestamp":1627582820057,"user_tz":-540,"elapsed":24536,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"acfc3c7e-4ff1-434a-87f8-f1dfc0650fd8"},"source":["!pip install adamp\n","!pip install git+https://github.com/GY-Jeong/transformers"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting adamp\n","  Downloading adamp-0.3.0.tar.gz (5.1 kB)\n","Building wheels for collected packages: adamp\n","  Building wheel for adamp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for adamp: filename=adamp-0.3.0-py3-none-any.whl size=5998 sha256=20eeea891c2dae64229f14d1b0cda458fa665d85ab04dd6a1c71af21b25443b9\n","  Stored in directory: /root/.cache/pip/wheels/bb/95/21/ced2d2cb9944e3a72e58fece7958973eed3fd8d0aeb6e2e450\n","Successfully built adamp\n","Installing collected packages: adamp\n","Successfully installed adamp-0.3.0\n","Collecting git+https://github.com/GY-Jeong/transformers\n","  Cloning https://github.com/GY-Jeong/transformers to /tmp/pip-req-build-x1ri9wiz\n","  Running command git clone -q https://github.com/GY-Jeong/transformers /tmp/pip-req-build-x1ri9wiz\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (3.0.12)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 13.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.41.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.6.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2019.12.20)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 35.2 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 58.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (21.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers==4.10.0.dev0) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.10.0.dev0) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0.dev0) (3.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.15.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.10.0.dev0-py3-none-any.whl size=2624020 sha256=8c8e548ea0fc412b3528367af1f2880a8db0de554dab10d37e26145181de13c8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-_4rt_p36/wheels/e3/55/b9/7bc34594c5d9f4b3d1851c8a8a630cf1eaaf13795ff8388c33\n","Successfully built transformers\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0.dev0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6oadxEi9rZ7x","executionInfo":{"status":"ok","timestamp":1627582824921,"user_tz":-540,"elapsed":4874,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import os\n","import random\n","import torch\n","import numpy as np\n","from torch import nn\n","\n","from torch.optim import Adam, AdamW, SGD\n","from adamp import AdamP\n","from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR, ExponentialLR, CosineAnnealingWarmRestarts\n","\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup\n","\n","\n","def set_seeds(seed=42):\n","    # 랜덤 시드를 설정하여 매 코드를 실행할 때마다 동일한 결과를 얻게 합니다.\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","def save_checkpoint(state, model_dir, model_filename):\n","    print('saving model ...')\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","    # torch.save(state, os.path.join(model_dir, model_filename))\n","    torch.save(state, os.path.join(model_filename))\n","\n","\n","def get_optimizer(model, args):\n","    if args.optimizer == 'adam':\n","        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'adamW':\n","        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'adamP':\n","        optimizer = AdamP(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'SGD':\n","        optimizer = SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n","\n","    # 모든 parameter들의 grad값을 0으로 초기화\n","    optimizer.zero_grad()\n","\n","    return optimizer\n","\n","\n","def get_scheduler(optimizer, args):\n","    if args.scheduler == 'plateau':\n","        scheduler = ReduceLROnPlateau(optimizer, patience=args.plateau_patience, factor=args.plateau_factor, mode='max',\n","                                      verbose=True)\n","    elif args.scheduler == 'linear_warmup':\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n","                                                    num_training_steps=args.total_steps)\n","    elif args.scheduler == 'step_lr':\n","        scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n","    elif args.scheduler == 'exp_lr':\n","        scheduler = ExponentialLR(optimizer, gamma=args.gamma)\n","    elif args.scheduler == 'cosine_annealing':\n","        scheduler = CosineAnnealingLR(optimizer, T_max=args.t_max, eta_min=args.eta_min)\n","    elif args.scheduler == 'cosine_annealing_warmstart':\n","        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=args.T_0, T_mult=args.T_mult, eta_min=args.eta_min,\n","                                                last_epoch=-1)\n","\n","    return scheduler\n","\n","\n","def update_params(loss, model, optimizer, batch_idx, max_len, args):\n","    if args.gradient_accumulation:\n","        # normalize loss to account for batch accumulation\n","        loss = loss / args.accum_iter \n","\n","        # backward pass\n","        loss.backward()\n","\n","        # weights update\n","        if ((batch_idx + 1) % args.accum_iter == 0) or (batch_idx + 1 == max_len):\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","    else:\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","\n","def load_tokenizer(args):\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.tokenizer_name\n","        if args.tokenizer_name\n","        else args.model_name_or_path,\n","        use_fast=True,\n","    )\n","\n","    return tokenizer\n","\n","\n","def load_model(args, model_name=None):\n","    if not model_name:\n","        model_name = args.model_name\n","    model_path = os.path.join(args.model_dir, model_name)\n","    print(\"Loading Model from:\", model_path)\n","    # load_state = torch.load(model_path)\n","    load_state = torch.load(model_name)\n","\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        args.config_name\n","        if args.config_name\n","        else args.model_name_or_path,\n","    )\n","\n","    config.num_labels = 7\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","    ).to(args.device)\n","\n","    # model.classifier = nn.Sequential(\n","    #     nn.Dropout(p=0.3, inplace=False),\n","    #     nn.Linear(1024, 1024),\n","    #     nn.Tanh(),\n","    #     nn.Dropout(p=0.3, inplace=False),\n","    #     nn.Linear(1024, 512),\n","    #     nn.Tanh(),\n","    #     nn.Dropout(p=0.3, inplace=False),\n","    #     nn.Linear(512, 7),\n","    # )\n","\n","    model.load_state_dict(load_state['state_dict'], strict=True)\n","\n","    # model = model.to(args.device)\n","\n","    print(\"Loading Model from:\", model_path, \"...Finished.\")\n","\n","    return model\n","\n","\n","def get_model(args):\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        args.config_name\n","        if args.config_name\n","        else args.model_name_or_path,\n","    )\n","\n","    config.num_labels = 7\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","    ).to(args.device)\n","\n","    # model.classifier = nn.Sequential(\n","    #     nn.Dropout(p=0.3, inplace=False),\n","    #     nn.Linear(1024, 1024),\n","    #     nn.Tanh(),\n","    #     nn.Dropout(p=0.3, inplace=False),\n","    #     nn.Linear(1024, 512),\n","    #     nn.Tanh(),\n","    #     nn.Dropout(p=0.3, inplace=False),\n","    #     nn.Linear(512, 7),\n","    # )\n","\n","    # print(model)\n","    #model.classifier.dropout = nn.Dropout(p=0.3, inplace = False)\n","\n","    model = model.to(args.device)\n","\n","    return model\n","\n","\n","def get_loaders(args, train, valid, is_inference=False):\n","    pin_memory = True\n","    train_loader, valid_loader = None, None\n","\n","    if is_inference:\n","        test_dataset = YNAT_dataset(args, valid, is_inference)\n","        test_loader = torch.utils.data.DataLoader(test_dataset, num_workers=args.num_workers, shuffle=False,\n","                                                  batch_size=args.batch_size, pin_memory=pin_memory)\n","        return test_loader\n","\n","    if train is not None:\n","        train_dataset = YNAT_dataset(args, train, is_inference)\n","        train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=args.num_workers, shuffle=True,\n","                                                   batch_size=args.batch_size, pin_memory=pin_memory)\n","    if valid is not None:\n","        valid_dataset = YNAT_dataset(args, valid, is_inference)\n","        valid_loader = torch.utils.data.DataLoader(valid_dataset, num_workers=args.num_workers, shuffle=False,\n","                                                   batch_size=args.batch_size, pin_memory=pin_memory)\n","\n","    return train_loader, valid_loader\n","\n","\n","# loss계산하고 parameter update!\n","def compute_loss(preds, targets, args):\n","    \"\"\"\n","    Args :\n","        preds   : (batch_size, max_seq_len)\n","        targets : (batch_size, max_seq_len)\n","    \"\"\"\n","    # print(preds, targets)\n","    loss = get_criterion(preds, targets, args)\n","    # 마지막 시퀀스에 대한 값만 loss 계산\n","    # loss = loss[:, -1]\n","    # loss = torch.mean(loss)\n","    return loss\n","\n","\n","def get_criterion(pred, target, args):\n","    if args.criterion == 'BCE':\n","        loss = nn.BCELoss(reduction=\"none\")\n","    elif args.criterion == \"BCELogit\":\n","        loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    elif args.criterion == \"MSE\":\n","        loss = nn.MSELoss(reduction=\"none\")\n","    elif args.criterion == \"L1\":\n","        loss = nn.L1Loss(reduction=\"none\")\n","    elif args.criterion == \"CE\":\n","        #weights = [1,1,2,1,1,1,1] #as class distribution\n","        #class_weights = torch.FloatTensor(weights).cuda()\n","        #loss = nn.CrossEntropyLoss(weight=class_weights)\n","        loss = nn.CrossEntropyLoss()\n","    # NLL, CrossEntropy not available\n","    return loss(pred, target)\n","\n","\n","def make_vocab(args):\n","    print(\"============ READ VOCABS ============\")\n","    vocabs = []\n","    for i in range(7):\n","        vocab = set()\n","        f = open(args.vocab_dir + str(i) + '.txt', 'r')\n","        while True:\n","            line = f.readline()\n","            if not line: break\n","            vocab.add(line[:-1])\n","        f.close()\n","        vocabs.append(vocab)\n","        print(f\"category {i} reading end, size : {len(vocab)}\")\n","    return vocabs"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mLM-aadds6H9"},"source":["## Dataloader"]},{"cell_type":"code","metadata":{"id":"2s9RxMi7rlfb","executionInfo":{"status":"ok","timestamp":1627582824922,"user_tz":-540,"elapsed":12,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import os\n","import torch\n","import pandas as pd\n","\n","\n","class Preprocess:\n","    def __init__(self, args):\n","        self.args = args\n","        self.train_data = None\n","        self.test_data = None\n","\n","    def load_data(self, file_name):\n","        csv_file_name = os.path.join(self.args.data_dir, file_name)\n","        df = pd.read_csv(csv_file_name)\n","        #del df['Unnamed: 0']\n","        return df.values\n","\n","    def load_train_data(self):\n","        self.train_data = self.load_data('train_data.csv')\n","\n","    def load_test_data(self):\n","        self.test_data = self.load_data('test_data.csv')\n","\n","\n","class YNAT_dataset(torch.utils.data.Dataset):\n","    def __init__(self, args, data, is_inference):\n","        self.args = args\n","        self.data = data\n","        self.is_inference = is_inference\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        row = self.data[index]\n","        element = [row[i] for i in range(len(row))]\n","        #print(type(row))\n","        # np.array -> torch.tensor 형변환\n","        #for i, col in enumerate(row):\n","        #    if type(col) == str:\n","        #        pass\n","        #    else:\n","        #        row[i] = torch.tensor(col)\n","\n","        return element\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2g9iLEBtDnJ"},"source":["## Trainer"]},{"cell_type":"code","metadata":{"id":"Xq3sntmNtErb","executionInfo":{"status":"ok","timestamp":1627582825906,"user_tz":-540,"elapsed":995,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["from sklearn.metrics import accuracy_score\n","from torch.nn.functional import one_hot\n","from tqdm import tqdm\n","from sklearn import metrics\n","\n","\n","def run(args, tokenizer, train_data, valid_data, cv_count):\n","    train_loader, valid_loader = get_loaders(args, train_data, valid_data)\n","\n","    # only when using warmup scheduler\n","    # args.total_steps = int(len(train_loader.dataset) / args.batch_size) * args.n_epochs\n","    # args.warmup_steps = int(args.total_steps * args.warmup_ratio)\n","\n","    model = get_model(args)\n","    optimizer = get_optimizer(model, args)\n","    scheduler = get_scheduler(optimizer, args)\n","\n","    best_acc = -1\n","    early_stopping_counter = 0\n","    for epoch in range(args.n_epochs):\n","\n","        print(f\"Start Training: Epoch {epoch + 1}\")\n","\n","        if not args.cv_strategy:\n","            model_name = args.run_name\n","        else:\n","            model_name = f\"{args.run_name.split('.pt')[0]}_{cv_count}.pt\"\n","\n","        # TRAIN\n","        train_acc, train_loss = train(args, model, tokenizer, train_loader, optimizer)\n","\n","        # VALID\n","        acc, val_loss = validate(args, model, tokenizer, valid_loader)\n","\n","        # TODO: model save or early stopping\n","        if args.scheduler == 'plateau':\n","            last_lr = optimizer.param_groups[0]['lr']\n","        else:\n","            last_lr = scheduler.get_last_lr()[0]\n","\n","        print({\"epoch\": epoch, \"train_loss\": train_loss, \"train_acc\": train_acc,\n","                   \"valid_acc\": acc, \"val_loss\": val_loss, \"learning_rate\": last_lr})\n","\n","        if acc > best_acc:\n","            best_acc = acc\n","            # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n","            model_to_save = model.module if hasattr(model, 'module') else model\n","            save_checkpoint({\n","                'epoch': epoch + 1,\n","                'state_dict': model_to_save.state_dict(),\n","            },\n","                args.model_dir, model_name,\n","            )\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","            if early_stopping_counter >= args.patience:\n","                print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n","                break\n","\n","        # scheduler\n","        if args.scheduler == 'plateau':\n","            scheduler.step(best_acc)\n","        else:\n","            scheduler.step()\n","\n","    return best_acc\n","\n","\n","def inference(args, test_data):\n","    # ckpt_file_names = []\n","    all_fold_preds = []\n","    all_fold_argmax_preds = []\n","\n","    if not args.cv_strategy:\n","        ckpt_file_names = [args.model_name]\n","    else:\n","        ckpt_file_names = [f\"{args.model_name.split('.pt')[0]}_{i + 1}.pt\" for i in range(args.fold_num)]\n","\n","    tokenizer = load_tokenizer(args)\n","\n","    for fold_idx, ckpt in enumerate(ckpt_file_names):\n","        model = load_model(args, ckpt)\n","        model.eval()\n","        test_loader = get_loaders(args, None, test_data, True)\n","\n","        total_preds = []\n","        total_argmax_preds = []\n","        total_ids = []\n","\n","        for step, batch in tqdm(enumerate(test_loader), desc='Inferencing', total=len(test_loader)):\n","            idx, text = batch\n","            tokenized_examples = tokenizer(\n","                text,\n","                max_length=args.max_seq_len,\n","                padding=\"max_length\",\n","                return_tensors=\"pt\"\n","            ).to(args.device)\n","\n","            token_label_type_ids = []\n","            for row in tokenized_examples['input_ids']:\n","                temp = []\n","                row = tokenizer.convert_ids_to_tokens(row)\n","                for token in row:\n","                    for i, element in enumerate(args.vocab):\n","                        if token in element:\n","                            temp.append(i+1)\n","                            break\n","                    else:\n","                        temp.append(0)\n","                #print(temp)\n","                token_label_type_ids.append(temp)\n","\n","            token_label_type_ids = torch.tensor(token_label_type_ids).to(args.device)\n","            tokenized_examples['token_label_type_ids'] = token_label_type_ids\n","\n","            preds = model(**tokenized_examples)\n","            logits = preds['logits']\n","            #logits = logits[:,0,:]\n","            argmax_logits = torch.argmax(logits, dim=1)\n","\n","            if args.device == 'cuda':\n","                argmax_preds = argmax_logits.to('cpu').detach().numpy()\n","                preds = logits.to('cpu').detach().numpy()\n","                token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","            else:  # cpu\n","                argmax_preds = argmax_logits.detach().numpy()\n","                preds = logits.detach().numpy()\n","                token_label_type_ids = token_label_type_ids.detach().numpy()\n","\n","            total_preds += list(preds)\n","            total_argmax_preds += list(argmax_preds)\n","            total_ids += list(idx)\n","\n","        all_fold_preds.append(total_preds)\n","        all_fold_argmax_preds.append(total_argmax_preds)\n","\n","        output_file_name = \"output.csv\" if not args.cv_strategy else f\"output_{fold_idx + 1}.csv\"\n","        write_path = os.path.join(args.output_dir, output_file_name)\n","        if not os.path.exists(args.output_dir):\n","            os.makedirs(args.output_dir)\n","        with open(write_path, 'w', encoding='utf8') as w:\n","            print(\"writing prediction : {}\".format(write_path))\n","            w.write(\"index,topic_idx\\n\")\n","            for index, p in zip(total_ids, total_argmax_preds):\n","                w.write('{},{}\\n'.format(index, p))\n","\n","    if len(all_fold_preds) > 1:\n","        # Soft voting ensemble\n","        votes = np.sum(all_fold_preds, axis=0)\n","        votes = np.argmax(votes, axis=1)\n","\n","        write_path = os.path.join(args.output_dir, \"output_softvote.csv\")\n","        #write_path = \"output_softvote.csv\"\n","        if not os.path.exists(args.output_dir):\n","            os.makedirs(args.output_dir)\n","        with open(write_path, 'w', encoding='utf8') as w:\n","            print(\"writing prediction : {}\".format(write_path))\n","            w.write(\"index,topic_idx\\n\")\n","            for id, p in zip(total_ids, votes):\n","                w.write('{},{}\\n'.format(id, p))\n","\n","\n","def train(args, model, tokenizer, train_loader, optimizer):\n","    model.train()\n","\n","    total_preds = []\n","    total_targets = []\n","    losses = []\n","    for step, batch in tqdm(enumerate(train_loader), desc='Training', total=len(train_loader)):\n","        idx, text, label = batch\n","        label = label.to(args.device)\n","        # print(idx[:10])\n","        # print(text[:10])\n","        # print(label[:10])\n","        tokenized_examples = tokenizer(\n","            text,\n","            max_length=args.max_seq_len,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        ).to(args.device)\n","        \n","        # tokenize\n","        # 모델의 입력으로\n","        # label은 one-hot?\n","        # loss 주고\n","        # argmax를 golden\n","\n","        token_label_type_ids = []\n","        for row in tokenized_examples['input_ids']:\n","            temp = []\n","            row = tokenizer.convert_ids_to_tokens(row)\n","            for token in row:\n","                for i, element in enumerate(args.vocab):\n","                    if token in element:\n","                        temp.append(i+1)\n","                        break\n","                else:\n","                    temp.append(0)\n","            #print(temp)\n","            token_label_type_ids.append(temp)\n","\n","        token_label_type_ids = torch.tensor(token_label_type_ids).to(args.device)\n","        tokenized_examples['token_label_type_ids'] = token_label_type_ids\n","\n","        preds = model(**tokenized_examples, labels = label)\n","        # print(preds)\n","        logits = preds['logits']\n","        # logits = logits[:,0,:]\n","        softmax_logits = nn.Softmax(dim=1)(logits)\n","        argmax_logits = torch.argmax(logits, dim=1)\n","\n","        # one_hot_logits = one_hot(argmax_logits, num_classes=7).float()\n","        # print(one_hot(argmax_logits, num_classes=7).type(torch.FloatTensor))\n","        # loss = compute_loss(logits,\n","        #                     label, args)\n","        loss = preds['loss']\n","        # print(loss)\n","\n","        update_params(loss, model, optimizer, step, len(train_loader), args)\n","\n","        if step % args.log_steps == 0:\n","            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n","\n","        if args.device == 'cuda':\n","            argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","            label = label.to('cpu').detach().numpy()\n","            loss = loss.to('cpu').detach().numpy()\n","            token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","        else:  # cpu\n","            argmax_logits = argmax_logits.detach().numpy()\n","            label = label.detach().numpy()\n","            loss = loss.detach().numpy()\n","            token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","\n","        total_preds.append(argmax_logits)\n","        total_targets.append(label)\n","        losses.append(loss)\n","\n","    total_preds = np.concatenate(total_preds)\n","    total_targets = np.concatenate(total_targets)\n","\n","    # Train AUC / ACC\n","    acc = accuracy_score(total_targets, total_preds)\n","    loss_avg = sum(losses) / len(losses)\n","    print(f'TRAIN ACC : {acc}, TRAIN LOSS : {loss_avg}')\n","    return acc, loss_avg\n","\n","\n","def validate(args, model, tokenizer, valid_loader):\n","    model.eval()\n","\n","    total_preds = []\n","    total_targets = []\n","    losses = []\n","    for step, batch in tqdm(enumerate(valid_loader), desc='Training', total=len(valid_loader)):\n","        idx, text, label = batch\n","        label = label.to(args.device)\n","        tokenized_examples = tokenizer(\n","            text,\n","            max_length=args.max_seq_len,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        ).to(args.device)\n","\n","        # tokenize\n","        # 모델의 입력으로\n","        # label은 one-hot?\n","        # loss 주고\n","        # argmax를 golden\n","        token_label_type_ids = []\n","        for row in tokenized_examples['input_ids']:\n","            temp = []\n","            row = tokenizer.convert_ids_to_tokens(row)\n","            for token in row:\n","                for i, element in enumerate(args.vocab):\n","                    if token in element:\n","                        temp.append(i+1)\n","                        break\n","                else:\n","                    temp.append(0)\n","            #print(temp)\n","            token_label_type_ids.append(temp)\n","\n","        token_label_type_ids = torch.tensor(token_label_type_ids).to(args.device)\n","        tokenized_examples['token_label_type_ids'] = token_label_type_ids\n","\n","        preds = model(**tokenized_examples, labels = label)\n","        logits = preds['logits']\n","        # logits = logits[:,0,:]\n","        softmax_logits = nn.Softmax(dim=1)(logits)\n","        argmax_logits = torch.argmax(logits, dim=1)\n","\n","        # one_hot_logits = one_hot(argmax_logits, num_classes=7).float()\n","        # print(one_hot(argmax_logits, num_classes=7).type(torch.FloatTensor))\n","        # loss = compute_loss(logits,\n","        #                     label, args)\n","        loss = preds['loss']\n","\n","        if step % args.log_steps == 0:\n","            print(f\"Validation steps: {step} Loss: {str(loss.item())}\")\n","\n","        if args.device == 'cuda':\n","            argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","            label = label.to('cpu').detach().numpy()\n","            loss = loss.to('cpu').detach().numpy()\n","            token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","        else:  # cpu\n","            argmax_logits = argmax_logits.detach().numpy()\n","            label = label.detach().numpy()\n","            loss = loss.detach().numpy()\n","            token_label_type_ids = token_label_type_ids.detach().numpy()\n","\n","        total_preds.append(argmax_logits)\n","        total_targets.append(label)\n","        losses.append(loss)\n","\n","    total_preds = np.concatenate(total_preds)\n","    total_targets = np.concatenate(total_targets)\n","\n","    # Train AUC / ACC\n","    target_names = ['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치']\n","    print(metrics.classification_report(total_targets, total_preds, target_names=target_names))\n","    matrix = metrics.confusion_matrix(total_targets, total_preds)\n","    print(matrix.diagonal()/matrix.sum(axis=1))\n","\n","    acc = accuracy_score(total_targets, total_preds)\n","    loss_avg = sum(losses) / len(losses)\n","    print(f'VALID ACC : {acc}, VALID LOSS : {loss_avg}')\n","    return acc, loss_avg\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5CS14MP9tFCQ"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"K9DdutwStF4B","executionInfo":{"status":"ok","timestamp":1627582826464,"user_tz":-540,"elapsed":560,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import torch\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","from datetime import datetime\n","from pytz import timezone\n","\n","\n","def main(args):\n","    if not args.run_name:\n","        args.run_name = datetime.now(timezone(\"Asia/Seoul\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n","\n","    set_seeds(args.seed)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    args.device = device\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.tokenizer_name\n","        if args.tokenizer_name\n","        else args.model_name_or_path,\n","        use_fast=True,\n","    )\n","\n","    preprocess = Preprocess(args)\n","    preprocess.load_train_data()\n","    train_data_origin = preprocess.train_data\n","\n","    print(f\"Size of train data : {len(train_data_origin)}\")\n","    # print(f\"size of test data : {len(test_data)}\")\n","\n","    if args.cv_strategy == 'random':\n","        kf = KFold(n_splits=args.fold_num, shuffle=True)\n","        splits = kf.split(X=train_data_origin)\n","    else:\n","        # default\n","        # 여기 각 label로 바꿔야됨\n","        train_labels = [sequence[-1] for sequence in train_data_origin]\n","        skf = StratifiedKFold(n_splits=args.fold_num, shuffle=True)\n","        splits = skf.split(X=train_data_origin, y=train_labels)\n","\n","    acc_avg = 0\n","    for fold_num, (train_index, valid_index) in enumerate(splits):\n","        train_data = train_data_origin[train_index]\n","        valid_data = train_data_origin[valid_index]\n","        best_acc = run(args, tokenizer, train_data, valid_data, fold_num + 1)\n","\n","        if not args.cv_strategy:\n","            break\n","\n","        acc_avg += best_acc\n","\n","    if args.cv_strategy:\n","        acc_avg /= args.fold_num\n","\n","        print(\"*\" * 50, 'auc_avg', \"*\" * 50)\n","        print(acc_avg)\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1EgudubtKjJ"},"source":["## Run"]},{"cell_type":"code","metadata":{"id":"_xupDv9YtKGf","executionInfo":{"status":"ok","timestamp":1627582826465,"user_tz":-540,"elapsed":5,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import argparse\n","import easydict\n","\n","def parse_args():\n","    args = easydict.EasyDict({'run_name' : 'temp',\n","                             'seed':42,\n","                             'device' :'cuda',\n","                             'data_dir': cur_dir + '/data/open/',\n","                             'model_dir' : '/content/drive/MyDrive/KLUE_TC/models/vocab20',\n","                             'transformers_dir' : '/content/drive/MyDrive/KLUE_TC/transformers/',\n","                             'model_name_or_path' : 'klue/roberta-large',\n","                             'config_name' : None,\n","                             'tokenizer_name' : None,\n","                             'output_dir' : '/content/drive/MyDrive/KLUE_TC/output/vocab50',\n","                             'vocab_dir' : '/content/drive/MyDrive/KLUE_TC/data/vocab/new50/',\n","                             \n","                             'accum_iter' : 8,\n","                             'gradient_accumulation' : True,\n","\n","                             'cv_strategy' : 'stratified',\n","                             'fold_num' : 4,\n","\n","                             'num_workers' : 1,\n","\n","                             # 훈련\n","                             'n_epochs' : 10,\n","                             'batch_size' : 32,\n","                             'lr' : 5e-6,\n","                             'clip_grad' : 15,\n","                             'patience' : 3,\n","                             'max_seq_len' : 40,\n","\n","                             # Optimizer\n","                             'optimizer' : 'adamP',\n","\n","                             # Optimizer-parameters\n","                             'weight_decay' : 0.05,\n","                             'momentum' : 0.9,\n","\n","                             # Scheduler\n","                             'scheduler' : 'step_lr',\n","\n","                             # Scheduler-parameters\n","                             # plateau\n","                             'plateau_patience' : 10,\n","                             'plateau_factor' : 0.5,\n","                              \n","                             't_max' : 10,\n","                             'T_0' : 10,\n","                             'T_mult' : 2,\n","                             '--eta_min' : 0.01,\n","\n","                             # linear_warmup\n","                             'warmup_ratio' : 0.3,\n","\n","                             # Step LR\n","                             'step_size' : 50,\n","                             'gamma' : 0.1,\n","\n","                             'criterion' : 'CE',\n","\n","                             'log_steps' : 100})\n","    \n","    return args"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["569686078e124ada8a6d684aa5b5aacf","ffaba92c096b4cbda55618d290036288","d3082b15ec0347c9960bff459c3a28c4","51aa38974e4a48ada9ef9b59b3d45785","6dca2cdf0ecf4ae4a44d94360645a8e4","c95fe081cb0d41a7916854887da3969e","bb078edb727f44f8a435d5da0fdac3de","4f487a71788d4a1f83f283cf2b251e44","96c2995be9db4529bafd3990f08c50a0","0f630e9dbd094a828dd2d7a98b2cce60","2e07def207e944929d26fdd785128f99","78d9ae50327548eeb0c192e95c5c8cb7","21176568341c47839c0895848064598f","bee626ef0c5440759d89ef385828b08d","7aeec550e1ed4ee9bb82c919849e79af","7511ddfbfb644fb59b9e87f611a2ff0e","cf9eea1dec944f929b9ac9e571956016","693600af07614c4d8802b85ee7e76b50","39676a680b6443d182b52a76487e3fda","42e4220994d14f3396dac9d62ccbbeaf","830ba2ae53ab46df86ea2c105a3aa162","3d6db41c939d4b98aeba6baafb1fe963","8dc058ece5214f5d881ec1f931e89ef4","89394e2a86ba42939cd5861024637a68","1d35b492cc1e44849dd2dbe43123715d","439e0060f08740a887447a8cd4286636","a23d6b8269ca40c992b72a36759acd9b","9bb9745f330042d98f0749ebd52216e8","f0527887a9474519b397bcb6d20df029","0a8f238b409c4ddca5d06153fae963a5","8751ea784ed740cf85bf8b2f080462c1","17080c8db2f34c55857ff44c205df0d7","adbe65bebfe34e04adb79f5cf21bc1ce","e175bce5152a40be8650f8878ab1d53d","4030c74d89c343ccb4933aea256de58d","9c9af8d0c94c4f03ad4200bdb3893cae","4762c6b626c043edab28fc0b226ea25b","906056f24a74486b8306d569419d05da","005b49b3876344739bc295e119423f5b","1caaf94e6cad4e7d988bf321c864b9bd"]},"id":"YKuE65Ct3d34","executionInfo":{"status":"ok","timestamp":1627591679953,"user_tz":-540,"elapsed":8853492,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"48cdc662-a8c0-4b61-f160-288572e0986a"},"source":["if __name__ == '__main__':\n","    args = parse_args()\n","    args['vocab'] = make_vocab(args)\n","    main(args)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["============ READ VOCABS ============\n","category 0 reading end, size : 972\n","category 1 reading end, size : 972\n","category 2 reading end, size : 1388\n","category 3 reading end, size : 1875\n","category 4 reading end, size : 1530\n","category 5 reading end, size : 1604\n","category 6 reading end, size : 1897\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"569686078e124ada8a6d684aa5b5aacf","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=337.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96c2995be9db4529bafd3990f08c50a0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=547.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf9eea1dec944f929b9ac9e571956016","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=248477.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d35b492cc1e44849dd2dbe43123715d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=173.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Size of train data : 45654\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"adbe65bebfe34e04adb79f5cf21bc1ce","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1346854671.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'roberta.embeddings.token_label_type_embeddings.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Start Training: Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1070 [00:00<06:41,  2.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 1.9473210573196411\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1070 [00:26<04:03,  3.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 1.900681972503662\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1070 [00:52<03:59,  3.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 1.410595417022705\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1070 [01:19<03:17,  3.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.9650253653526306\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1070 [01:45<03:04,  3.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.5025293827056885\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1070 [02:11<02:23,  3.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.7139464616775513\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1070 [02:38<02:08,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.4915387034416199\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  66%|██████▌   | 701/1070 [03:03<01:32,  4.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.36773645877838135\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1070 [03:29<01:13,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.21659249067306519\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1070 [03:55<00:42,  4.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.2931899428367615\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 1001/1070 [04:21<00:19,  3.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.3515019118785858\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1070/1070 [04:40<00:00,  3.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.7434871495327103, TRAIN LOSS : 0.7909292776381301\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:46,  7.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.20183677971363068\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:11<00:28,  9.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.49893003702163696\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:22<00:17,  8.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.5722138285636902\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:33<00:05,  9.38it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.27608388662338257\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:39<00:00,  9.10it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.79      0.91      0.85      1206\n","          경제       0.77      0.88      0.82      1555\n","          사회       0.88      0.65      0.75      1841\n","        생활문화       0.91      0.90      0.90      1483\n","          세계       0.92      0.94      0.93      1908\n","         스포츠       0.96      0.99      0.97      1734\n","          정치       0.92      0.91      0.91      1687\n","\n","    accuracy                           0.88     11414\n","   macro avg       0.88      0.88      0.88     11414\n","weighted avg       0.88      0.88      0.88     11414\n","\n","[0.91044776 0.87717042 0.65127648 0.90222522 0.93763103 0.98961938\n"," 0.90812092]\n","VALID ACC : 0.8792710706150342, VALID LOSS : 0.37385286256170075\n","{'epoch': 0, 'train_loss': 0.7909292776381301, 'train_acc': 0.7434871495327103, 'valid_acc': 0.8792710706150342, 'val_loss': 0.37385286256170075, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1070 [00:00<05:05,  3.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.47448301315307617\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1070 [00:27<04:14,  3.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.3958607614040375\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1070 [00:54<04:10,  3.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.31826069951057434\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1070 [01:22<03:22,  3.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.5399267077445984\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1070 [01:49<03:12,  3.47it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.13214421272277832\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1070 [02:16<02:28,  3.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.09717386960983276\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1070 [02:43<02:12,  3.55it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.44362303614616394\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  66%|██████▌   | 701/1070 [03:10<01:37,  3.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.42930299043655396\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1070 [03:38<01:16,  3.53it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.29946863651275635\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1070 [04:04<00:44,  3.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.3190148174762726\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 1001/1070 [04:32<00:19,  3.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.16290882229804993\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1070/1070 [04:50<00:00,  3.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.8893983644859813, TRAIN LOSS : 0.341247796134971\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:44,  7.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.18078528344631195\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:11<00:29,  8.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.5515313148498535\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:23<00:17,  8.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.449971467256546\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:34<00:06,  8.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.1717708259820938\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:41<00:00,  8.66it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.77      0.94      0.85      1206\n","          경제       0.88      0.81      0.84      1555\n","          사회       0.87      0.71      0.78      1841\n","        생활문화       0.90      0.91      0.91      1483\n","          세계       0.93      0.94      0.94      1908\n","         스포츠       0.96      0.99      0.97      1734\n","          정치       0.88      0.95      0.91      1687\n","\n","    accuracy                           0.89     11414\n","   macro avg       0.89      0.89      0.89     11414\n","weighted avg       0.89      0.89      0.89     11414\n","\n","[0.94361526 0.81028939 0.70885388 0.908294   0.93710692 0.98788927\n"," 0.94605809]\n","VALID ACC : 0.8889959698615735, VALID LOSS : 0.33274983385113444\n","{'epoch': 1, 'train_loss': 0.341247796134971, 'train_acc': 0.8893983644859813, 'valid_acc': 0.8889959698615735, 'val_loss': 0.33274983385113444, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1070 [00:00<05:00,  3.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.27763694524765015\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1070 [00:26<04:01,  4.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.23610764741897583\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1070 [00:52<03:59,  3.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.11722071468830109\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1070 [01:18<03:14,  3.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.3376014530658722\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1070 [01:44<03:00,  3.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.22715406119823456\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1070 [02:10<02:21,  4.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.36941590905189514\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1070 [02:36<02:08,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.11814755946397781\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  66%|██████▌   | 701/1070 [03:01<01:32,  3.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.24099597334861755\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1070 [03:27<01:13,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.25925344228744507\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1070 [03:53<00:41,  4.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.2812104821205139\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 1001/1070 [04:19<00:19,  3.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.3482958674430847\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1070/1070 [04:38<00:00,  3.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9025408878504673, TRAIN LOSS : 0.293943685745803\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:44,  8.05it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.19199734926223755\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:11<00:28,  8.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.5299175381660461\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:22<00:17,  8.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.446382999420166\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:34<00:06,  8.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.12769190967082977\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:40<00:00,  8.79it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.80      0.92      0.86      1206\n","          경제       0.85      0.84      0.85      1555\n","          사회       0.86      0.73      0.79      1841\n","        생활문화       0.88      0.94      0.91      1483\n","          세계       0.93      0.94      0.94      1908\n","         스포츠       0.97      0.98      0.98      1734\n","          정치       0.92      0.92      0.92      1687\n","\n","    accuracy                           0.89     11414\n","   macro avg       0.89      0.90      0.89     11414\n","weighted avg       0.89      0.89      0.89     11414\n","\n","[0.91625207 0.84244373 0.73221076 0.93661497 0.94444444 0.98269896\n"," 0.91760522]\n","VALID ACC : 0.8941650604520764, VALID LOSS : 0.3189002214161371\n","{'epoch': 2, 'train_loss': 0.293943685745803, 'train_acc': 0.9025408878504673, 'valid_acc': 0.8941650604520764, 'val_loss': 0.3189002214161371, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 4\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1070 [00:00<05:06,  3.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.25717368721961975\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1070 [00:26<04:06,  3.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.32110631465911865\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1070 [00:53<04:04,  3.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.4689047932624817\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1070 [01:19<03:15,  3.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.15749406814575195\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1070 [01:46<03:04,  3.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.1487942934036255\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1070 [02:12<02:27,  3.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.22376547753810883\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1070 [02:39<02:09,  3.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.16154687106609344\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  66%|██████▌   | 701/1070 [03:06<01:34,  3.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.14175660908222198\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1070 [03:32<01:14,  3.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.14063964784145355\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1070 [03:58<00:42,  3.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.43351033329963684\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 1001/1070 [04:25<00:19,  3.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.11232393234968185\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1070/1070 [04:43<00:00,  3.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9110981308411215, TRAIN LOSS : 0.2605412893571726\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:41,  8.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.18404337763786316\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:11<00:27,  9.25it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.5945241451263428\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:21<00:16,  9.23it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.46872010827064514\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:33<00:06,  8.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.10132414847612381\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:39<00:00,  9.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.78      0.94      0.86      1206\n","          경제       0.87      0.81      0.84      1555\n","          사회       0.85      0.74      0.80      1841\n","        생활문화       0.90      0.93      0.92      1483\n","          세계       0.92      0.95      0.93      1908\n","         스포츠       0.97      0.98      0.98      1734\n","          정치       0.93      0.91      0.92      1687\n","\n","    accuracy                           0.89     11414\n","   macro avg       0.89      0.90      0.89     11414\n","weighted avg       0.90      0.89      0.89     11414\n","\n","[0.94444444 0.81350482 0.74307442 0.92919757 0.95073375 0.97981546\n"," 0.91286307]\n","VALID ACC : 0.893902225337305, VALID LOSS : 0.3259373632590978\n","{'epoch': 3, 'train_loss': 0.2605412893571726, 'train_acc': 0.9110981308411215, 'valid_acc': 0.893902225337305, 'val_loss': 0.3259373632590978, 'learning_rate': 5e-06}\n","Start Training: Epoch 5\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/1070 [00:00<04:33,  3.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.23320411145687103\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1070 [00:26<04:07,  3.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.16269534826278687\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1070 [00:53<03:59,  3.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.2017330825328827\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1070 [01:19<03:13,  3.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.13169462978839874\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1070 [01:45<03:06,  3.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.06698216497898102\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1070 [02:11<02:23,  3.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.14069312810897827\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1070 [02:38<02:10,  3.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.29138654470443726\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  66%|██████▌   | 701/1070 [03:04<01:33,  3.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.19666212797164917\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1070 [03:30<01:13,  3.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.27744507789611816\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1070 [03:57<00:42,  3.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.3838242292404175\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 1001/1070 [04:23<00:18,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.119209423661232\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1070/1070 [04:41<00:00,  3.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9200934579439253, TRAIN LOSS : 0.23578381179614324\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:42,  8.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.17912523448467255\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:11<00:28,  8.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.5009822845458984\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:22<00:17,  9.05it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.40664082765579224\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:33<00:06,  8.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.1569666862487793\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:39<00:00,  9.07it/s]"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.83      0.88      0.86      1206\n","          경제       0.84      0.86      0.85      1555\n","          사회       0.83      0.76      0.80      1841\n","        생활문화       0.92      0.90      0.91      1483\n","          세계       0.93      0.94      0.94      1908\n","         스포츠       0.98      0.98      0.98      1734\n","          정치       0.91      0.93      0.92      1687\n","\n","    accuracy                           0.89     11414\n","   macro avg       0.89      0.89      0.89     11414\n","weighted avg       0.89      0.89      0.89     11414\n","\n","[0.87976783 0.85723473 0.76480174 0.9008766  0.94496855 0.97520185\n"," 0.93123889]\n","VALID ACC : 0.893902225337305, VALID LOSS : 0.31984020114800676\n","{'epoch': 4, 'train_loss': 0.23578381179614324, 'train_acc': 0.9200934579439253, 'valid_acc': 0.893902225337305, 'val_loss': 0.31984020114800676, 'learning_rate': 5e-06}\n","Start Training: Epoch 6\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/1070 [00:00<04:43,  3.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.1475912630558014\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1070 [00:26<04:04,  3.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.14878278970718384\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1070 [00:53<04:00,  3.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.3050922155380249\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1070 [01:19<03:17,  3.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.33854812383651733\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1070 [01:45<03:05,  3.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.13630764186382294\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1070 [02:12<02:25,  3.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.15271969139575958\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1070 [02:38<02:09,  3.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.23745077848434448\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  66%|██████▌   | 701/1070 [03:04<01:33,  3.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.09168701618909836\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1070 [03:31<01:14,  3.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.08709916472434998\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1070 [03:57<00:42,  3.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.1720392107963562\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 1001/1070 [04:23<00:19,  3.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.14287222921848297\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1070/1070 [04:42<00:00,  3.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.92821261682243, TRAIN LOSS : 0.21290134108825542\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:42,  8.44it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.360939621925354\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:10<00:27,  9.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.6150349378585815\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:21<00:16,  9.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.5044514536857605\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:32<00:05,  9.47it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.1918431967496872\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:38<00:00,  9.33it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.79      0.92      0.85      1206\n","          경제       0.86      0.83      0.84      1555\n","          사회       0.88      0.69      0.78      1841\n","        생활문화       0.86      0.94      0.90      1483\n","          세계       0.94      0.92      0.93      1908\n","         스포츠       0.96      0.99      0.97      1734\n","          정치       0.88      0.95      0.92      1687\n","\n","    accuracy                           0.89     11414\n","   macro avg       0.88      0.89      0.89     11414\n","weighted avg       0.89      0.89      0.89     11414\n","\n","[0.92288557 0.82636656 0.69201521 0.94403237 0.92400419 0.98500577\n"," 0.95139301]\n","VALID ACC : 0.8890835815664972, VALID LOSS : 0.3477158665589216\n","{'epoch': 5, 'train_loss': 0.21290134108825542, 'train_acc': 0.92821261682243, 'valid_acc': 0.8890835815664972, 'val_loss': 0.3477158665589216, 'learning_rate': 5e-06}\n","EarlyStopping counter: 3 out of 3\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'roberta.embeddings.token_label_type_embeddings.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Start Training: Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1070 [00:00<04:52,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 1.995666742324829\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1070 [00:26<04:01,  4.02it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 1.8609216213226318\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1070 [00:52<03:59,  3.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 1.516831636428833\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1070 [01:18<03:12,  3.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.9320467710494995\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1070 [01:44<03:04,  3.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.7765706777572632\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1070 [02:10<02:21,  4.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.5670201778411865\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1070 [02:36<02:06,  3.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.737296462059021\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  66%|██████▌   | 701/1070 [03:02<01:32,  3.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.4075613021850586\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1070 [03:28<01:13,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.42286837100982666\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1070 [03:53<00:42,  3.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.3389451801776886\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 1001/1070 [04:20<00:18,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.44442763924598694\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1070/1070 [04:38<00:00,  3.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.741588785046729, TRAIN LOSS : 0.8029669985801817\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:39,  8.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.34997236728668213\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:10<00:26,  9.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.7197659015655518\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:21<00:16,  9.44it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.578222393989563\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:31<00:05,  9.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.4496941864490509\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:37<00:00,  9.50it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.75      0.92      0.83      1206\n","          경제       0.84      0.82      0.83      1556\n","          사회       0.82      0.69      0.75      1841\n","        생활문화       0.86      0.93      0.89      1483\n","          세계       0.90      0.91      0.91      1907\n","         스포츠       0.95      0.99      0.97      1733\n","          정치       0.93      0.84      0.89      1688\n","\n","    accuracy                           0.87     11414\n","   macro avg       0.87      0.87      0.87     11414\n","weighted avg       0.87      0.87      0.87     11414\n","\n","[0.92039801 0.82390746 0.69418794 0.92582603 0.9124279  0.98903635\n"," 0.84478673]\n","VALID ACC : 0.8693709479586472, VALID LOSS : 0.39716787697846484\n","{'epoch': 0, 'train_loss': 0.8029669985801817, 'train_acc': 0.741588785046729, 'valid_acc': 0.8693709479586472, 'val_loss': 0.39716787697846484, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1070 [00:00<05:07,  3.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.39523881673812866\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1070 [00:27<04:18,  3.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.29350098967552185\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1070 [00:55<04:08,  3.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.2580457031726837\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1070 [01:22<03:25,  3.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.4490847587585449\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1070 [01:50<03:10,  3.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.30478227138519287\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1070 [02:17<02:30,  3.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.31036385893821716\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1070 [02:45<02:15,  3.45it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.41448351740837097\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  66%|██████▌   | 701/1070 [03:12<01:36,  3.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.08463513106107712\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1070 [03:39<01:17,  3.47it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.2531636357307434\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1070 [04:07<00:44,  3.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.1736891120672226\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 1001/1070 [04:34<00:20,  3.45it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.24354353547096252\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1070/1070 [04:53<00:00,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.8882009345794393, TRAIN LOSS : 0.3426237142775382\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:39,  8.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.2923508286476135\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:10<00:26,  9.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.5808919668197632\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:21<00:16,  9.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.3051626682281494\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:31<00:05,  9.42it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.5186997652053833\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:37<00:00,  9.52it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.81      0.90      0.85      1206\n","          경제       0.81      0.86      0.84      1556\n","          사회       0.86      0.71      0.78      1841\n","        생활문화       0.90      0.92      0.91      1483\n","          세계       0.93      0.91      0.92      1907\n","         스포츠       0.95      0.99      0.97      1733\n","          정치       0.90      0.92      0.91      1688\n","\n","    accuracy                           0.88     11414\n","   macro avg       0.88      0.89      0.88     11414\n","weighted avg       0.89      0.88      0.88     11414\n","\n","[0.90049751 0.86118252 0.71211298 0.92312879 0.90613529 0.98730525\n"," 0.92120853]\n","VALID ACC : 0.8848782197301559, VALID LOSS : 0.3450079997192745\n","{'epoch': 1, 'train_loss': 0.3426237142775382, 'train_acc': 0.8882009345794393, 'valid_acc': 0.8848782197301559, 'val_loss': 0.3450079997192745, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1070 [00:00<05:05,  3.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.22300897538661957\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1070 [00:27<04:15,  3.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.25472646951675415\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1070 [00:55<04:11,  3.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.3094918429851532\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1070 [01:22<03:23,  3.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.32715362310409546\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1070 [01:49<03:12,  3.47it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.21557269990444183\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1070 [02:16<02:29,  3.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.42559337615966797\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1070 [02:44<02:13,  3.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.3629470467567444\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  66%|██████▌   | 701/1070 [03:11<01:38,  3.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.45345598459243774\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1070 [03:38<01:16,  3.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.04381674528121948\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1070 [04:06<00:44,  3.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.17774470150470734\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 1001/1070 [04:33<00:19,  3.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.35451167821884155\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1070/1070 [04:52<00:00,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.8995619158878505, TRAIN LOSS : 0.2977887396047884\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:45,  7.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.2544447183609009\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:12<00:30,  8.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.5748254656791687\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:24<00:18,  8.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.3050384819507599\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:35<00:06,  8.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.45886653661727905\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:42<00:00,  8.41it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.82      0.90      0.86      1206\n","          경제       0.86      0.84      0.85      1556\n","          사회       0.86      0.72      0.78      1841\n","        생활문화       0.88      0.93      0.91      1483\n","          세계       0.90      0.94      0.92      1907\n","         스포츠       0.95      0.99      0.97      1733\n","          정치       0.91      0.91      0.91      1688\n","\n","    accuracy                           0.89     11414\n","   macro avg       0.88      0.89      0.89     11414\n","weighted avg       0.89      0.89      0.89     11414\n","\n","[0.90049751 0.84383033 0.71863118 0.93324343 0.93707394 0.98845932\n"," 0.90580569]\n","VALID ACC : 0.8879446294024882, VALID LOSS : 0.34125945492222365\n","{'epoch': 2, 'train_loss': 0.2977887396047884, 'train_acc': 0.8995619158878505, 'valid_acc': 0.8879446294024882, 'val_loss': 0.34125945492222365, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 4\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1070 [00:00<05:07,  3.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.3839973211288452\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1070 [00:27<04:13,  3.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.26509976387023926\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1070 [00:54<04:06,  3.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.39019227027893066\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1070 [01:22<03:23,  3.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.06252516061067581\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1070 [01:49<03:10,  3.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.2197473645210266\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1070 [02:16<02:31,  3.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.09258393943309784\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1070 [02:44<02:13,  3.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.18138998746871948\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  66%|██████▌   | 701/1070 [03:11<01:37,  3.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.43983858823776245\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1070 [03:38<01:16,  3.53it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.2932489514350891\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1070 [04:05<00:44,  3.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.1840178221464157\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 1001/1070 [04:32<00:19,  3.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.3416679799556732\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1070/1070 [04:51<00:00,  3.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9103095794392523, TRAIN LOSS : 0.26122877725234656\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:47,  7.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.26105403900146484\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:12<00:30,  8.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.571510910987854\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:23<00:18,  8.47it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.2399224489927292\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:35<00:06,  8.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.46134042739868164\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:42<00:00,  8.49it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.83      0.89      0.86      1206\n","          경제       0.87      0.82      0.84      1556\n","          사회       0.81      0.77      0.79      1841\n","        생활문화       0.91      0.92      0.91      1483\n","          세계       0.92      0.93      0.93      1907\n","         스포츠       0.96      0.98      0.97      1733\n","          정치       0.91      0.91      0.91      1688\n","\n","    accuracy                           0.89     11414\n","   macro avg       0.89      0.89      0.89     11414\n","weighted avg       0.89      0.89      0.89     11414\n","\n","[0.88888889 0.82069409 0.77403585 0.91908294 0.92973256 0.98268898\n"," 0.91232227]\n","VALID ACC : 0.8895216400911162, VALID LOSS : 0.3268872746566431\n","{'epoch': 3, 'train_loss': 0.26122877725234656, 'train_acc': 0.9103095794392523, 'valid_acc': 0.8895216400911162, 'val_loss': 0.3268872746566431, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 5\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1070 [00:00<05:06,  3.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.20013853907585144\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1070 [00:27<04:16,  3.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.23081769049167633\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1070 [00:54<04:05,  3.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.07677746564149857\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1070 [01:21<03:19,  3.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.3880153000354767\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1070 [01:48<03:11,  3.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.26795658469200134\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1070 [02:15<02:28,  3.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.3022311329841614\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1070 [02:43<02:14,  3.47it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.21874606609344482\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  66%|██████▌   | 701/1070 [03:10<01:35,  3.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.48655596375465393\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1070 [03:37<01:16,  3.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.40102896094322205\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1070 [04:04<00:43,  3.84it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.38018593192100525\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 1001/1070 [04:31<00:19,  3.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.43848294019699097\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1070/1070 [04:50<00:00,  3.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.920268691588785, TRAIN LOSS : 0.2357242724588402\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:46,  7.73it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.2688038945198059\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:12<00:29,  8.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.5649189352989197\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:23<00:17,  8.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.21057026088237762\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:35<00:06,  8.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.4975096881389618\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:41<00:00,  8.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.84      0.87      0.85      1206\n","          경제       0.84      0.86      0.85      1556\n","          사회       0.82      0.75      0.79      1841\n","        생활문화       0.91      0.92      0.91      1483\n","          세계       0.94      0.92      0.93      1907\n","         스포츠       0.94      0.99      0.97      1733\n","          정치       0.91      0.91      0.91      1688\n","\n","    accuracy                           0.89     11414\n","   macro avg       0.89      0.89      0.89     11414\n","weighted avg       0.89      0.89      0.89     11414\n","\n","[0.8747927  0.85604113 0.75339489 0.91773432 0.91662297 0.99249856\n"," 0.91291469]\n","VALID ACC : 0.8887331347468022, VALID LOSS : 0.33437672239683924\n","{'epoch': 4, 'train_loss': 0.2357242724588402, 'train_acc': 0.920268691588785, 'valid_acc': 0.8887331347468022, 'val_loss': 0.33437672239683924, 'learning_rate': 5e-06}\n","Start Training: Epoch 6\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/1070 [00:00<04:24,  4.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.07821807265281677\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1070 [00:26<04:01,  4.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.12870796024799347\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1070 [00:51<03:57,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.2336970865726471\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1070 [01:17<03:11,  4.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.17257925868034363\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1070 [01:43<03:01,  3.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.31757134199142456\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1070 [02:09<02:23,  3.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.1319105625152588\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1070 [02:35<02:07,  3.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.09844201803207397\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  66%|██████▌   | 701/1070 [03:01<01:32,  4.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.1906103640794754\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1070 [03:27<01:12,  3.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.3411015272140503\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1070 [03:53<00:41,  4.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.27223262190818787\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 1001/1070 [04:19<00:18,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.05931410938501358\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1070/1070 [04:37<00:00,  3.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9286799065420561, TRAIN LOSS : 0.21256912543708198\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:45,  7.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.2495255470275879\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:12<00:30,  8.37it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.5842236876487732\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:23<00:18,  8.44it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.21228793263435364\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:35<00:06,  8.73it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.47876212000846863\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:41<00:00,  8.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.86      0.80      0.83      1206\n","          경제       0.81      0.87      0.84      1556\n","          사회       0.83      0.75      0.79      1841\n","        생활문화       0.90      0.93      0.91      1483\n","          세계       0.92      0.93      0.93      1907\n","         스포츠       0.96      0.98      0.97      1733\n","          정치       0.91      0.91      0.91      1688\n","\n","    accuracy                           0.88     11414\n","   macro avg       0.88      0.88      0.88     11414\n","weighted avg       0.88      0.88      0.88     11414\n","\n","[0.79767828 0.87082262 0.7501358  0.92919757 0.93392764 0.98153491\n"," 0.91409953]\n","VALID ACC : 0.8849658314350797, VALID LOSS : 0.3487992270560074\n","{'epoch': 5, 'train_loss': 0.21256912543708198, 'train_acc': 0.9286799065420561, 'valid_acc': 0.8849658314350797, 'val_loss': 0.3487992270560074, 'learning_rate': 5e-06}\n","Start Training: Epoch 7\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/1070 [00:00<04:42,  3.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.17315013706684113\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1070 [00:27<04:12,  3.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.13584107160568237\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1070 [00:54<04:04,  3.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.13291071355342865\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1070 [01:21<03:19,  3.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.2208179086446762\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1070 [01:48<03:10,  3.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.14392049610614777\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1070 [02:15<02:27,  3.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.17343643307685852\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1070 [02:42<02:14,  3.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.14124780893325806\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  66%|██████▌   | 701/1070 [03:09<01:35,  3.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.06527316570281982\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1070 [03:36<01:15,  3.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.07727815210819244\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1070 [04:03<00:44,  3.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.30106961727142334\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  94%|█████████▎| 1001/1070 [04:30<00:19,  3.55it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.3479098081588745\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1070/1070 [04:49<00:00,  3.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9356308411214953, TRAIN LOSS : 0.19220554011691118\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:43,  8.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.254646360874176\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:11<00:29,  8.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.5746532678604126\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:23<00:18,  8.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.2230914682149887\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:35<00:06,  8.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.48914751410484314\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:41<00:00,  8.58it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.82      0.89      0.85      1206\n","          경제       0.88      0.81      0.84      1556\n","          사회       0.81      0.77      0.79      1841\n","        생활문화       0.91      0.92      0.91      1483\n","          세계       0.93      0.93      0.93      1907\n","         스포츠       0.96      0.98      0.97      1733\n","          정치       0.90      0.92      0.91      1688\n","\n","    accuracy                           0.89     11414\n","   macro avg       0.89      0.89      0.89     11414\n","weighted avg       0.89      0.89      0.89     11414\n","\n","[0.89220564 0.80719794 0.77403585 0.91908294 0.92606188 0.97864974\n"," 0.92239336]\n","VALID ACC : 0.8882950762221833, VALID LOSS : 0.34659200112390165\n","{'epoch': 6, 'train_loss': 0.19220554011691118, 'train_acc': 0.9356308411214953, 'valid_acc': 0.8882950762221833, 'val_loss': 0.34659200112390165, 'learning_rate': 5e-06}\n","EarlyStopping counter: 3 out of 3\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'roberta.embeddings.token_label_type_embeddings.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Start Training: Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1071 [00:00<04:45,  3.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 2.011697292327881\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:26<04:02,  4.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 1.86729097366333\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:51<03:54,  3.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 1.667733073234558\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:17<03:12,  4.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 1.0697885751724243\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:43<03:01,  3.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.5147751569747925\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:09<02:23,  3.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.3598838150501251\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:36<02:12,  3.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.2786266505718231\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:02<01:35,  3.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.6092551946640015\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:29<01:16,  3.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.14196470379829407\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [03:56<00:43,  3.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.3024614453315735\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:23<00:19,  3.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.24816471338272095\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:42<00:00,  3.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.7306445489325663, TRAIN LOSS : 0.8196467636666156\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:43,  8.19it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.4394378364086151\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:11<00:29,  8.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.32838672399520874\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:23<00:18,  8.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.22700577974319458\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:34<00:06,  8.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.8544970750808716\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:41<00:00,  8.67it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.81      0.89      0.85      1206\n","          경제       0.85      0.86      0.86      1556\n","          사회       0.84      0.72      0.78      1840\n","        생활문화       0.89      0.90      0.90      1483\n","          세계       0.93      0.93      0.93      1907\n","         스포츠       0.95      0.98      0.97      1733\n","          정치       0.91      0.92      0.92      1688\n","\n","    accuracy                           0.89     11413\n","   macro avg       0.88      0.89      0.88     11413\n","weighted avg       0.89      0.89      0.89     11413\n","\n","[0.89137645 0.86246787 0.72445652 0.90289953 0.93235448 0.98384305\n"," 0.92061611]\n","VALID ACC : 0.8872338561289758, VALID LOSS : 0.34241701034875976\n","{'epoch': 0, 'train_loss': 0.8196467636666156, 'train_acc': 0.7306445489325663, 'valid_acc': 0.8872338561289758, 'val_loss': 0.34241701034875976, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1071 [00:00<05:00,  3.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.38881027698516846\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:26<04:11,  3.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.26414620876312256\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:53<04:02,  3.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.18084309995174408\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:20<03:17,  3.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.340743750333786\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:46<03:09,  3.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.3121517598628998\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:13<02:26,  3.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.2928623855113983\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:40<02:13,  3.53it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.20472727715969086\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:06<01:35,  3.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.2882542908191681\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:33<01:15,  3.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.14634399116039276\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [04:00<00:44,  3.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.4649355709552765\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:26<00:19,  3.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.33446964621543884\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:45<00:00,  3.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.8877661283256915, TRAIN LOSS : 0.3368049801147285\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:43,  8.25it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.39960891008377075\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:10<00:26,  9.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.27555304765701294\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:21<00:16,  9.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.19534491002559662\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:31<00:05,  9.75it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.8539961576461792\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:37<00:00,  9.52it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.78      0.93      0.85      1206\n","          경제       0.85      0.88      0.86      1556\n","          사회       0.87      0.71      0.79      1840\n","        생활문화       0.91      0.90      0.91      1483\n","          세계       0.95      0.91      0.93      1907\n","         스포츠       0.96      0.98      0.97      1733\n","          정치       0.89      0.95      0.92      1688\n","\n","    accuracy                           0.89     11413\n","   macro avg       0.89      0.89      0.89     11413\n","weighted avg       0.89      0.89      0.89     11413\n","\n","[0.93283582 0.8785347  0.71467391 0.90289953 0.90928159 0.97980381\n"," 0.94668246]\n","VALID ACC : 0.8916148251993341, VALID LOSS : 0.32714289643329564\n","{'epoch': 1, 'train_loss': 0.3368049801147285, 'train_acc': 0.8877661283256915, 'valid_acc': 0.8916148251993341, 'val_loss': 0.32714289643329564, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1071 [00:00<05:12,  3.42it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.20079943537712097\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:26<04:01,  4.02it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.264199823141098\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:51<03:55,  3.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.20617496967315674\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:17<03:13,  3.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.3948046863079071\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:43<03:00,  3.71it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.1816052794456482\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:09<02:22,  4.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.43873924016952515\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:35<02:07,  3.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.3981655240058899\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:01<01:31,  4.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.2714623510837555\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:27<01:13,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.2696036398410797\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [03:53<00:42,  4.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.11617325246334076\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:18<00:19,  3.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.4885365068912506\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:37<00:00,  3.87it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9016967962384276, TRAIN LOSS : 0.29161693844822495\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:40,  8.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.272814005613327\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:10<00:26,  9.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.1684400588274002\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:20<00:16,  9.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.11708660423755646\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:31<00:05,  9.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.8125895261764526\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:37<00:00,  9.63it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.83      0.88      0.86      1206\n","          경제       0.88      0.86      0.87      1556\n","          사회       0.83      0.77      0.80      1840\n","        생활문화       0.87      0.93      0.90      1483\n","          세계       0.93      0.93      0.93      1907\n","         스포츠       0.97      0.97      0.97      1733\n","          정치       0.92      0.91      0.92      1688\n","\n","    accuracy                           0.89     11413\n","   macro avg       0.89      0.89      0.89     11413\n","weighted avg       0.89      0.89      0.89     11413\n","\n","[0.88308458 0.85732648 0.76847826 0.93324343 0.9318301  0.97230237\n"," 0.90936019]\n","VALID ACC : 0.8931919740646631, VALID LOSS : 0.3153270325292142\n","{'epoch': 2, 'train_loss': 0.29161693844822495, 'train_acc': 0.9016967962384276, 'valid_acc': 0.8931919740646631, 'val_loss': 0.3153270325292142, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 4\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1071 [00:00<04:49,  3.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.23872295022010803\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:25<04:00,  4.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.2081095576286316\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:51<03:58,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.19956590235233307\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:17<03:11,  4.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.32256096601486206\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:43<03:01,  3.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.31400179862976074\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:09<02:23,  3.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.34991613030433655\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:35<02:07,  3.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.20671376585960388\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:01<01:33,  3.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.15964755415916443\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:27<01:13,  3.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.42951077222824097\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [03:53<00:42,  4.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.031138285994529724\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:19<00:19,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.2787253260612488\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:37<00:00,  3.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.911918460325341, TRAIN LOSS : 0.25777963976907076\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:40,  8.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.2929902970790863\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:10<00:26,  9.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.1997509002685547\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:21<00:16,  9.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.11968564242124557\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:31<00:05,  9.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.852005124092102\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:37<00:00,  9.62it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.83      0.89      0.86      1206\n","          경제       0.90      0.84      0.87      1556\n","          사회       0.82      0.79      0.81      1840\n","        생활문화       0.91      0.90      0.90      1483\n","          세계       0.94      0.93      0.94      1907\n","         스포츠       0.97      0.98      0.97      1733\n","          정치       0.90      0.95      0.92      1688\n","\n","    accuracy                           0.90     11413\n","   macro avg       0.89      0.90      0.90     11413\n","weighted avg       0.90      0.90      0.90     11413\n","\n","[0.89303483 0.84254499 0.79184783 0.89683075 0.93078133 0.97691864\n"," 0.94727488]\n","VALID ACC : 0.8973977043722071, VALID LOSS : 0.309980202461032\n","{'epoch': 3, 'train_loss': 0.25777963976907076, 'train_acc': 0.911918460325341, 'valid_acc': 0.8973977043722071, 'val_loss': 0.309980202461032, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 5\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1071 [00:00<04:50,  3.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.14769025146961212\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:26<04:02,  3.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.32245275378227234\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:52<03:56,  3.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.2724378705024719\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:17<03:10,  4.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.41679584980010986\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:43<03:03,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.15574781596660614\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:09<02:23,  3.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.2938837707042694\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:35<02:08,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.09974413365125656\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:01<01:32,  4.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.318057656288147\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:27<01:13,  3.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.19388903677463531\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [03:53<00:42,  3.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.08511732518672943\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:19<00:19,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.1592005342245102\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:37<00:00,  3.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9207382961946204, TRAIN LOSS : 0.23088826343980656\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:40,  8.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.29302242398262024\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:10<00:26,  9.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.23566125333309174\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:21<00:16,  9.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.10341760516166687\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:31<00:05,  9.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.8696642518043518\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:37<00:00,  9.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.82      0.89      0.85      1206\n","          경제       0.87      0.86      0.86      1556\n","          사회       0.84      0.78      0.81      1840\n","        생활문화       0.91      0.90      0.91      1483\n","          세계       0.93      0.94      0.93      1907\n","         스포츠       0.97      0.98      0.97      1733\n","          정치       0.91      0.94      0.92      1688\n","\n","    accuracy                           0.90     11413\n","   macro avg       0.89      0.90      0.89     11413\n","weighted avg       0.90      0.90      0.90     11413\n","\n","[0.88971808 0.85539846 0.78097826 0.90020229 0.93759832 0.9763416\n"," 0.93720379]\n","VALID ACC : 0.8970472268465784, VALID LOSS : 0.31336467074496405\n","{'epoch': 4, 'train_loss': 0.23088826343980656, 'train_acc': 0.9207382961946204, 'valid_acc': 0.8970472268465784, 'val_loss': 0.31336467074496405, 'learning_rate': 5e-06}\n","Start Training: Epoch 6\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/1071 [00:00<04:40,  3.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.1989719122648239\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:26<04:01,  4.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.04147595167160034\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:52<03:57,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.08660531789064407\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:18<03:13,  3.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.09026269614696503\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:44<03:01,  3.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.1830822229385376\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:10<02:22,  4.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.31393176317214966\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:36<02:08,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.1488390415906906\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:02<01:32,  4.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.19219085574150085\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:28<01:14,  3.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.10926555842161179\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [03:54<00:42,  3.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.08337664604187012\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:20<00:19,  3.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.06361791491508484\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:38<00:00,  3.85it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9292368797640256, TRAIN LOSS : 0.20637796753860949\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:43,  8.22it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.3853490948677063\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:11<00:28,  9.07it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.20078718662261963\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:22<00:17,  8.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.0868975818157196\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:33<00:06,  8.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.8593953251838684\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:39<00:00,  9.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.80      0.91      0.85      1206\n","          경제       0.87      0.86      0.86      1556\n","          사회       0.85      0.76      0.80      1840\n","        생활문화       0.90      0.90      0.90      1483\n","          세계       0.94      0.93      0.93      1907\n","         스포츠       0.96      0.98      0.97      1733\n","          정치       0.91      0.94      0.92      1688\n","\n","    accuracy                           0.90     11413\n","   macro avg       0.89      0.90      0.89     11413\n","weighted avg       0.90      0.90      0.89     11413\n","\n","[0.90547264 0.85796915 0.76032609 0.90357384 0.92763503 0.98211194\n"," 0.93720379]\n","VALID ACC : 0.8953824585998423, VALID LOSS : 0.3239997281074649\n","{'epoch': 5, 'train_loss': 0.20637796753860949, 'train_acc': 0.9292368797640256, 'valid_acc': 0.8953824585998423, 'val_loss': 0.3239997281074649, 'learning_rate': 5e-06}\n","Start Training: Epoch 7\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/1071 [00:00<04:34,  3.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.10597226023674011\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:26<04:06,  3.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.34959253668785095\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:53<04:02,  3.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.4616169333457947\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:19<03:15,  3.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.29821041226387024\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:45<03:04,  3.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.1092957928776741\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:12<02:25,  3.91it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.19568048417568207\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:38<02:09,  3.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.15043991804122925\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:05<01:35,  3.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.14038242399692535\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:31<01:14,  3.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.21593181788921356\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [03:58<00:43,  3.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.12268584221601486\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:24<00:19,  3.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.28645384311676025\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:43<00:00,  3.78it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9367717064338075, TRAIN LOSS : 0.18300905829664219\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:41,  8.56it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.24714875221252441\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:11<00:27,  9.16it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.1858624666929245\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:22<00:16,  9.25it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.057170569896698\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:33<00:06,  8.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.8044257164001465\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:39<00:00,  9.10it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.84      0.85      0.84      1206\n","          경제       0.86      0.86      0.86      1556\n","          사회       0.85      0.75      0.80      1840\n","        생활문화       0.86      0.93      0.90      1483\n","          세계       0.94      0.93      0.93      1907\n","         스포츠       0.96      0.98      0.97      1733\n","          정치       0.90      0.94      0.92      1688\n","\n","    accuracy                           0.89     11413\n","   macro avg       0.89      0.89      0.89     11413\n","weighted avg       0.89      0.89      0.89     11413\n","\n","[0.85157546 0.8592545  0.75271739 0.93189481 0.92815941 0.9763416\n"," 0.94075829]\n","VALID ACC : 0.8920529221063699, VALID LOSS : 0.33677611201062424\n","{'epoch': 6, 'train_loss': 0.18300905829664219, 'train_acc': 0.9367717064338075, 'valid_acc': 0.8920529221063699, 'val_loss': 0.33677611201062424, 'learning_rate': 5e-06}\n","EarlyStopping counter: 3 out of 3\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'roberta.embeddings.token_label_type_embeddings.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Start Training: Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1071 [00:00<04:50,  3.68it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 2.025378942489624\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:26<04:08,  3.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 1.9193154573440552\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:53<03:58,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 1.491222620010376\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:19<03:15,  3.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.9973284006118774\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:45<03:04,  3.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.8198277354240417\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:11<02:25,  3.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.5063483119010925\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:38<02:10,  3.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.4718568027019501\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:04<01:33,  3.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.3906151354312897\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:30<01:14,  3.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.2590884268283844\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [03:57<00:43,  3.92it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.41168075799942017\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:23<00:19,  3.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.49864816665649414\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:41<00:00,  3.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.7397564323471861, TRAIN LOSS : 0.8024374575855908\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:42,  8.45it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.06449031084775925\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:10<00:26,  9.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.15431812405586243\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:21<00:16,  9.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.4515983462333679\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:32<00:05,  9.39it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.4867757260799408\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:37<00:00,  9.41it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.82      0.87      0.84      1206\n","          경제       0.88      0.80      0.84      1555\n","          사회       0.81      0.75      0.77      1840\n","        생활문화       0.86      0.92      0.89      1484\n","          세계       0.90      0.95      0.92      1907\n","         스포츠       0.97      0.96      0.97      1733\n","          정치       0.91      0.92      0.92      1688\n","\n","    accuracy                           0.88     11413\n","   macro avg       0.88      0.88      0.88     11413\n","weighted avg       0.88      0.88      0.88     11413\n","\n","[0.86650083 0.79935691 0.74673913 0.92318059 0.94651285 0.96133872\n"," 0.9200237 ]\n","VALID ACC : 0.881100499430474, VALID LOSS : 0.3698283586160642\n","{'epoch': 0, 'train_loss': 0.8024374575855908, 'train_acc': 0.7397564323471861, 'valid_acc': 0.881100499430474, 'val_loss': 0.3698283586160642, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 2\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1071 [00:00<05:05,  3.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.7656599283218384\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:26<04:03,  3.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.452780157327652\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:52<03:58,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.7009800672531128\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:19<03:14,  3.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.3499804437160492\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:45<03:03,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.45262411236763\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:11<02:24,  3.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.35173144936561584\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:37<02:09,  3.64it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.45261484384536743\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:03<01:33,  3.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.24917776882648468\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:30<01:14,  3.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.21840696036815643\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [03:56<00:43,  3.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.2171553671360016\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:23<00:19,  3.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.21689262986183167\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:41<00:00,  3.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.8882918139073042, TRAIN LOSS : 0.34197937472124285\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:42,  8.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.05059010162949562\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:11<00:27,  9.27it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.19919824600219727\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:22<00:17,  9.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.310247540473938\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:32<00:05,  9.30it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.3719213902950287\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:38<00:00,  9.18it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.82      0.89      0.85      1206\n","          경제       0.83      0.86      0.84      1555\n","          사회       0.84      0.73      0.78      1840\n","        생활문화       0.90      0.90      0.90      1484\n","          세계       0.94      0.93      0.93      1907\n","         스포츠       0.96      0.98      0.97      1733\n","          정치       0.89      0.95      0.92      1688\n","\n","    accuracy                           0.89     11413\n","   macro avg       0.88      0.89      0.89     11413\n","weighted avg       0.89      0.89      0.89     11413\n","\n","[0.88640133 0.85787781 0.72717391 0.9009434  0.92553749 0.97691864\n"," 0.94609005]\n","VALID ACC : 0.8878471917988259, VALID LOSS : 0.33419121316626293\n","{'epoch': 1, 'train_loss': 0.34197937472124285, 'train_acc': 0.8882918139073042, 'valid_acc': 0.8878471917988259, 'val_loss': 0.33419121316626293, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 3\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1071 [00:00<05:07,  3.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.31693336367607117\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:26<04:06,  3.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.1362704038619995\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:53<04:00,  3.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.14066681265830994\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:19<03:14,  3.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.4211514890193939\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:45<03:05,  3.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.18880979716777802\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:11<02:23,  3.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.31071752309799194\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:38<02:08,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.2407471388578415\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:04<01:33,  3.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.3446093201637268\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:30<01:13,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.22504377365112305\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [03:56<00:43,  3.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.36210137605667114\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:22<00:19,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.12724074721336365\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:40<00:00,  3.81it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9016383867293596, TRAIN LOSS : 0.2936445356778849\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:41,  8.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.018291473388671875\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:10<00:27,  9.23it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.18381302058696747\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:21<00:17,  9.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.42263686656951904\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:32<00:05,  9.22it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.49149203300476074\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:38<00:00,  9.23it/s]"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.76      0.95      0.84      1206\n","          경제       0.92      0.77      0.83      1555\n","          사회       0.84      0.73      0.78      1840\n","        생활문화       0.90      0.90      0.90      1484\n","          세계       0.90      0.95      0.92      1907\n","         스포츠       0.96      0.98      0.97      1733\n","          정치       0.91      0.93      0.92      1688\n","\n","    accuracy                           0.89     11413\n","   macro avg       0.88      0.89      0.88     11413\n","weighted avg       0.89      0.89      0.88     11413\n","\n","[0.95190713 0.76720257 0.72608696 0.90229111 0.95070792 0.98268898\n"," 0.9306872 ]\n","VALID ACC : 0.8852186103566109, VALID LOSS : 0.3435448925023904\n","{'epoch': 2, 'train_loss': 0.2936445356778849, 'train_acc': 0.9016383867293596, 'valid_acc': 0.8852186103566109, 'val_loss': 0.3435448925023904, 'learning_rate': 5e-06}\n","Start Training: Epoch 4\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/1071 [00:00<04:35,  3.88it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.2342461496591568\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:26<04:03,  3.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.40047094225883484\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:52<03:59,  3.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.3262191712856293\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:18<03:14,  3.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.18826644122600555\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:45<03:03,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.11617396026849747\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:11<02:23,  3.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.3786877691745758\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:37<02:10,  3.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.3044520616531372\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:03<01:33,  3.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.4839288592338562\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:29<01:15,  3.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.2928539514541626\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [03:56<00:42,  3.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.24013692140579224\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:22<00:19,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.18386630713939667\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:40<00:00,  3.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9116556175345346, TRAIN LOSS : 0.26267410819283943\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:42,  8.37it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.02192993275821209\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:11<00:28,  9.10it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.17874906957149506\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:21<00:16,  9.44it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.41030099987983704\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:32<00:05,  9.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.3463136553764343\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:38<00:00,  9.36it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.80      0.92      0.86      1206\n","          경제       0.87      0.82      0.85      1555\n","          사회       0.84      0.76      0.80      1840\n","        생활문화       0.90      0.90      0.90      1484\n","          세계       0.92      0.94      0.93      1907\n","         스포츠       0.97      0.98      0.97      1733\n","          정치       0.92      0.93      0.92      1688\n","\n","    accuracy                           0.89     11413\n","   macro avg       0.89      0.89      0.89     11413\n","weighted avg       0.89      0.89      0.89     11413\n","\n","[0.92205638 0.8244373  0.75543478 0.90498652 0.94336654 0.97576457\n"," 0.92535545]\n","VALID ACC : 0.8918776833435555, VALID LOSS : 0.32521629958030057\n","{'epoch': 3, 'train_loss': 0.26267410819283943, 'train_acc': 0.9116556175345346, 'valid_acc': 0.8918776833435555, 'val_loss': 0.32521629958030057, 'learning_rate': 5e-06}\n","saving model ...\n","Start Training: Epoch 5\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   0%|          | 1/1071 [00:00<05:12,  3.42it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.07042724639177322\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:26<04:05,  3.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.36542925238609314\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:52<04:00,  3.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.21754741668701172\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:18<03:12,  4.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.19995367527008057\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:44<03:02,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.16895711421966553\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:10<02:23,  3.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.24794648587703705\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:37<02:07,  3.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.18233586847782135\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:03<01:33,  3.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.3138408064842224\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:29<01:13,  3.67it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.3402853012084961\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [03:55<00:42,  4.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.23787088692188263\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:21<00:19,  3.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.06535185873508453\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:39<00:00,  3.83it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.918839987149908, TRAIN LOSS : 0.23817503455605438\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:42,  8.41it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.03271256014704704\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:10<00:26,  9.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.17623934149742126\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:21<00:16,  9.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.43842411041259766\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:32<00:05,  9.27it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.5203626155853271\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:38<00:00,  9.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.78      0.94      0.85      1206\n","          경제       0.89      0.79      0.84      1555\n","          사회       0.85      0.73      0.79      1840\n","        생활문화       0.89      0.92      0.90      1484\n","          세계       0.90      0.95      0.92      1907\n","         스포츠       0.97      0.97      0.97      1733\n","          정치       0.92      0.93      0.92      1688\n","\n","    accuracy                           0.89     11413\n","   macro avg       0.89      0.89      0.89     11413\n","weighted avg       0.89      0.89      0.89     11413\n","\n","[0.94444444 0.79421222 0.73315217 0.91509434 0.94965915 0.96999423\n"," 0.92654028]\n","VALID ACC : 0.8881976693244545, VALID LOSS : 0.34292031455972866\n","{'epoch': 4, 'train_loss': 0.23817503455605438, 'train_acc': 0.918839987149908, 'valid_acc': 0.8881976693244545, 'val_loss': 0.34292031455972866, 'learning_rate': 5e-06}\n","Start Training: Epoch 6\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/1071 [00:00<04:34,  3.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.10002041608095169\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:26<04:06,  3.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.1736917495727539\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:52<03:58,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.13739527761936188\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:18<03:13,  3.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.2265651524066925\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:44<03:05,  3.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.4125451147556305\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:11<02:22,  3.99it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.14777129888534546\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:37<02:10,  3.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.07137890160083771\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:03<01:33,  3.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.12298361957073212\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:29<01:13,  3.66it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.4013804495334625\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [03:55<00:42,  3.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.17768095433712006\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:21<00:19,  3.65it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.20581647753715515\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:40<00:00,  3.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9279518705645279, TRAIN LOSS : 0.21307345527901647\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:42,  8.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.01734730787575245\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:10<00:27,  9.16it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.18288657069206238\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:21<00:16,  9.27it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.38393527269363403\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:32<00:05,  9.38it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.40464794635772705\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:38<00:00,  9.25it/s]"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.83      0.87      0.85      1206\n","          경제       0.81      0.87      0.84      1555\n","          사회       0.85      0.72      0.78      1840\n","        생활문화       0.92      0.89      0.91      1484\n","          세계       0.90      0.95      0.92      1907\n","         스포츠       0.97      0.98      0.97      1733\n","          정치       0.92      0.92      0.92      1688\n","\n","    accuracy                           0.89     11413\n","   macro avg       0.88      0.89      0.88     11413\n","weighted avg       0.89      0.89      0.89     11413\n","\n","[0.86981758 0.87459807 0.71956522 0.89487871 0.94861038 0.97749567\n"," 0.92061611]\n","VALID ACC : 0.8865329010777184, VALID LOSS : 0.3507954023115817\n","{'epoch': 5, 'train_loss': 0.21307345527901647, 'train_acc': 0.9279518705645279, 'valid_acc': 0.8865329010777184, 'val_loss': 0.3507954023115817, 'learning_rate': 5e-06}\n","Start Training: Epoch 7\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/1071 [00:00<04:31,  3.93it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 0.029858697205781937\n"],"name":"stdout"},{"output_type":"stream","text":["Training:   9%|▉         | 101/1071 [00:26<04:06,  3.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 100 Loss: 0.12058883905410767\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  19%|█▉        | 201/1071 [00:52<04:01,  3.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 200 Loss: 0.4237842559814453\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  28%|██▊       | 301/1071 [01:18<03:14,  3.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 300 Loss: 0.13857698440551758\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  37%|███▋      | 401/1071 [01:45<03:04,  3.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 400 Loss: 0.2731251120567322\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  47%|████▋     | 501/1071 [02:11<02:23,  3.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 500 Loss: 0.07605326920747757\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  56%|█████▌    | 601/1071 [02:37<02:10,  3.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 600 Loss: 0.25701674818992615\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  65%|██████▌   | 701/1071 [03:04<01:34,  3.94it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 700 Loss: 0.28287389874458313\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  75%|███████▍  | 801/1071 [03:30<01:14,  3.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 800 Loss: 0.11168372631072998\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  84%|████████▍ | 901/1071 [03:56<00:42,  3.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 900 Loss: 0.25524625182151794\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  93%|█████████▎| 1001/1071 [04:23<00:19,  3.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 1000 Loss: 0.14013470709323883\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 1071/1071 [04:41<00:00,  3.80it/s]"],"name":"stderr"},{"output_type":"stream","text":["TRAIN ACC : 0.9348149878800268, TRAIN LOSS : 0.1930846900263511\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Training:   0%|          | 1/357 [00:00<00:44,  7.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 0 Loss: 0.014652907848358154\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  29%|██▊       | 102/357 [00:11<00:27,  9.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 100 Loss: 0.1423112154006958\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  57%|█████▋    | 202/357 [00:22<00:16,  9.17it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 200 Loss: 0.4159090518951416\n"],"name":"stdout"},{"output_type":"stream","text":["Training:  85%|████████▍ | 302/357 [00:32<00:05,  9.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["Validation steps: 300 Loss: 0.31723037362098694\n"],"name":"stdout"},{"output_type":"stream","text":["Training: 100%|██████████| 357/357 [00:38<00:00,  9.20it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","        IT과학       0.84      0.86      0.85      1206\n","          경제       0.86      0.82      0.84      1555\n","          사회       0.80      0.78      0.79      1840\n","        생활문화       0.93      0.88      0.90      1484\n","          세계       0.90      0.95      0.92      1907\n","         스포츠       0.96      0.98      0.97      1733\n","          정치       0.91      0.93      0.92      1688\n","\n","    accuracy                           0.89     11413\n","   macro avg       0.89      0.88      0.88     11413\n","weighted avg       0.89      0.89      0.89     11413\n","\n","[0.85820896 0.82186495 0.78043478 0.87533693 0.94756162 0.98038084\n"," 0.93009479]\n","VALID ACC : 0.8870586173661614, VALID LOSS : 0.3513739476350843\n","{'epoch': 6, 'train_loss': 0.1930846900263511, 'train_acc': 0.9348149878800268, 'valid_acc': 0.8870586173661614, 'val_loss': 0.3513739476350843, 'learning_rate': 5e-06}\n","EarlyStopping counter: 3 out of 3\n","************************************************** auc_avg **************************************************\n","0.8932405220647388\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YfVIY2z0kZZx","executionInfo":{"status":"ok","timestamp":1627591680532,"user_tz":-540,"elapsed":588,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["torch.cuda.empty_cache()"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txpJN5vFPWCq"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"hUWUggp_PQqy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627591833318,"user_tz":-540,"elapsed":152788,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"d27b5fdb-0f00-44e2-a1dc-a49738a3f741"},"source":["def inference_main():\n","    args = parse_args()\n","    args['vocab'] = make_vocab(args)\n","    args.model_name = \"temp\"\n","    preprocess = Preprocess(args)\n","    preprocess.load_test_data()\n","    test_data = preprocess.test_data\n","\n","    print(f\"size of test data : {len(test_data)}\")\n","    torch.cuda.empty_cache()\n","    # del model\n","    inference(args, test_data)\n","\n","inference_main()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["============ READ VOCABS ============\n","category 0 reading end, size : 972\n","category 1 reading end, size : 972\n","category 2 reading end, size : 1388\n","category 3 reading end, size : 1875\n","category 4 reading end, size : 1530\n","category 5 reading end, size : 1604\n","category 6 reading end, size : 1897\n","size of test data : 9131\n","Loading Model from: /content/drive/MyDrive/KLUE_TC/models/vocab20/temp_1.pt\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'roberta.embeddings.token_label_type_embeddings.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Loading Model from: /content/drive/MyDrive/KLUE_TC/models/vocab20/temp_1.pt ...Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["Inferencing: 100%|██████████| 286/286 [00:31<00:00,  9.14it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["writing prediction : /content/drive/MyDrive/KLUE_TC/output/vocab50/output_1.csv\n","Loading Model from: /content/drive/MyDrive/KLUE_TC/models/vocab20/temp_2.pt\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'roberta.embeddings.token_label_type_embeddings.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Loading Model from: /content/drive/MyDrive/KLUE_TC/models/vocab20/temp_2.pt ...Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["Inferencing: 100%|██████████| 286/286 [00:31<00:00,  8.99it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["writing prediction : /content/drive/MyDrive/KLUE_TC/output/vocab50/output_2.csv\n","Loading Model from: /content/drive/MyDrive/KLUE_TC/models/vocab20/temp_3.pt\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'roberta.embeddings.token_label_type_embeddings.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Loading Model from: /content/drive/MyDrive/KLUE_TC/models/vocab20/temp_3.pt ...Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["Inferencing: 100%|██████████| 286/286 [00:31<00:00,  9.20it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["writing prediction : /content/drive/MyDrive/KLUE_TC/output/vocab50/output_3.csv\n","Loading Model from: /content/drive/MyDrive/KLUE_TC/models/vocab20/temp_4.pt\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'roberta.embeddings.token_label_type_embeddings.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Loading Model from: /content/drive/MyDrive/KLUE_TC/models/vocab20/temp_4.pt ...Finished.\n"],"name":"stdout"},{"output_type":"stream","text":["Inferencing: 100%|██████████| 286/286 [00:30<00:00,  9.24it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["writing prediction : /content/drive/MyDrive/KLUE_TC/output/vocab50/output_4.csv\n","writing prediction : /content/drive/MyDrive/KLUE_TC/output/vocab50/output_softvote.csv\n"],"name":"stdout"}]}]}
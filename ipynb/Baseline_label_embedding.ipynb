{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Baseline_label_embedding.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1evHUeHIafnkc54Zs-LmIuqOYzJYSuMUH","authorship_tag":"ABX9TyMaF3ce4lhrmpuKyNQ5AbOw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbWmvczoKOwQ","executionInfo":{"status":"ok","timestamp":1627563222222,"user_tz":-540,"elapsed":398,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"d1189fc2-d18a-4a1f-f09e-ed40dc02a3ff"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Jul 29 12:53:41 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   47C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4kthgVPqr5VC"},"source":["## Directory 설정, 구글 드라이브 import"]},{"cell_type":"code","metadata":{"id":"FOkMqa8hrHl_"},"source":["cur_dir = '/content/drive/MyDrive/KLUE_TC'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jx_d93v9rzQI"},"source":["## Utils"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2KbbvLPitR1N","executionInfo":{"status":"ok","timestamp":1627563249098,"user_tz":-540,"elapsed":26627,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"c30dafef-47cf-410e-8c89-ce58b20a860f"},"source":["!pip install adamp\n","!pip install git+https://github.com/GY-Jeong/transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting adamp\n","  Downloading adamp-0.3.0.tar.gz (5.1 kB)\n","Building wheels for collected packages: adamp\n","  Building wheel for adamp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for adamp: filename=adamp-0.3.0-py3-none-any.whl size=5998 sha256=2dbc3f8d9ed35db8d2e1262dce8a4ff58201b23b7efa22b1ec0f26ee950ac6d6\n","  Stored in directory: /root/.cache/pip/wheels/bb/95/21/ced2d2cb9944e3a72e58fece7958973eed3fd8d0aeb6e2e450\n","Successfully built adamp\n","Installing collected packages: adamp\n","Successfully installed adamp-0.3.0\n","Collecting git+https://github.com/GY-Jeong/transformers\n","  Cloning https://github.com/GY-Jeong/transformers to /tmp/pip-req-build-9ndvdrrk\n","  Running command git clone -q https://github.com/GY-Jeong/transformers /tmp/pip-req-build-9ndvdrrk\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 16.4 MB/s \n","\u001b[?25hCollecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.6.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (4.41.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 43.8 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0.dev0) (21.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers==4.10.0.dev0) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.10.0.dev0) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0.dev0) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0.dev0) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0.dev0) (1.15.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.10.0.dev0-py3-none-any.whl size=2624020 sha256=1b1e1f0c04b0f638ee0c740880df2df6b593f1d9ba4ad36222ff62b0de185d3f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-da4v36um/wheels/e3/55/b9/7bc34594c5d9f4b3d1851c8a8a630cf1eaaf13795ff8388c33\n","Successfully built transformers\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0.dev0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6oadxEi9rZ7x","executionInfo":{"status":"ok","timestamp":1627567956904,"user_tz":-540,"elapsed":489,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import os\n","import random\n","import torch\n","import numpy as np\n","from torch import nn\n","\n","from torch.optim import Adam, AdamW, SGD\n","from adamp import AdamP\n","from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR, ExponentialLR, CosineAnnealingWarmRestarts\n","\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup\n","\n","\n","def set_seeds(seed=42):\n","    # 랜덤 시드를 설정하여 매 코드를 실행할 때마다 동일한 결과를 얻게 합니다.\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","def save_checkpoint(state, model_dir, model_filename):\n","    print('saving model ...')\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","    #torch.save(state, os.path.join(model_dir, model_filename))\n","    torch.save(state, os.path.join(model_filename))\n","\n","\n","def get_optimizer(model, args):\n","    if args.optimizer == 'adam':\n","        optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'adamW':\n","        optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'adamP':\n","        optimizer = AdamP(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n","    elif args.optimizer == 'SGD':\n","        optimizer = SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n","\n","    # 모든 parameter들의 grad값을 0으로 초기화\n","    optimizer.zero_grad()\n","\n","    return optimizer\n","\n","\n","def get_scheduler(optimizer, args):\n","    if args.scheduler == 'plateau':\n","        scheduler = ReduceLROnPlateau(optimizer, patience=args.plateau_patience, factor=args.plateau_factor, mode='max',\n","                                      verbose=True)\n","    elif args.scheduler == 'linear_warmup':\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n","                                                    num_training_steps=args.total_steps)\n","    elif args.scheduler == 'step_lr':\n","        scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n","    elif args.scheduler == 'exp_lr':\n","        scheduler = ExponentialLR(optimizer, gamma=args.gamma)\n","    elif args.scheduler == 'cosine_annealing':\n","        scheduler = CosineAnnealingLR(optimizer, T_max=args.t_max, eta_min=args.eta_min)\n","    elif args.scheduler == 'cosine_annealing_warmstart':\n","        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=args.T_0, T_mult=args.T_mult, eta_min=args.eta_min,\n","                                                last_epoch=-1)\n","\n","    return scheduler\n","\n","\n","def update_params(loss, model, optimizer, batch_idx, max_len, args):\n","    if args.gradient_accumulation:\n","        # normalize loss to account for batch accumulation\n","        loss = loss / args.accum_iter \n","\n","        # backward pass\n","        loss.backward()\n","\n","        # weights update\n","        if ((batch_idx + 1) % args.accum_iter == 0) or (batch_idx + 1 == max_len):\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n","            optimizer.step()\n","            optimizer.zero_grad()\n","    else:\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","\n","def load_tokenizer(args):\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.tokenizer_name\n","        if args.tokenizer_name\n","        else args.model_name_or_path,\n","        use_fast=True,\n","    )\n","\n","    return tokenizer\n","\n","\n","def load_model(args, model_name=None):\n","    if not model_name:\n","        model_name = args.model_name\n","    model_path = os.path.join(args.model_dir, model_name)\n","    print(\"Loading Model from:\", model_path)\n","    load_state = torch.load(model_path)\n","    #load_state = torch.load(model_name)\n","\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        args.config_name\n","        if args.config_name\n","        else args.model_name_or_path,\n","    )\n","\n","    config.num_labels = 7\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","    )\n","\n","    model.classifier = nn.Sequential(\n","        nn.Linear(1024, 1024),\n","        nn.Dropout(p=0.3, inplace=False),\n","        nn.Linear(1024, 512),\n","        nn.Dropout(p=0.3, inplace=False),\n","        nn.Linear(512, 7),\n","    )\n","\n","\n","    model.load_state_dict(load_state['state_dict'], strict=True)\n","\n","    model = model.to(args.device)\n","\n","    print(\"Loading Model from:\", model_path, \"...Finished.\")\n","\n","    return model\n","\n","\n","def get_model(args):\n","    # Load pretrained model and tokenizer\n","    config = AutoConfig.from_pretrained(\n","        args.config_name\n","        if args.config_name\n","        else args.model_name_or_path,\n","    )\n","\n","    config.num_labels = 7\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","    )\n","\n","    model.classifier = nn.Sequential(\n","        nn.Linear(1024, 1024),\n","        nn.Dropout(p=0.4, inplace=False),\n","        nn.Linear(1024, 512),\n","        nn.Dropout(p=0.4, inplace=False),\n","        nn.Linear(512, 7),\n","    )\n","\n","    #model.classifier.dropout = nn.Dropout(p=0.3, inplace = False)\n","\n","    model = model.to(args.device)\n","\n","    return model\n","\n","\n","def get_loaders(args, train, valid, is_inference=False):\n","    pin_memory = True\n","    train_loader, valid_loader = None, None\n","\n","    if is_inference:\n","        test_dataset = YNAT_dataset(args, valid, is_inference)\n","        test_loader = torch.utils.data.DataLoader(test_dataset, num_workers=args.num_workers, shuffle=False,\n","                                                  batch_size=args.batch_size, pin_memory=pin_memory)\n","        return test_loader\n","\n","    if train is not None:\n","        train_dataset = YNAT_dataset(args, train, is_inference)\n","        train_loader = torch.utils.data.DataLoader(train_dataset, num_workers=args.num_workers, shuffle=True,\n","                                                   batch_size=args.batch_size, pin_memory=pin_memory)\n","    if valid is not None:\n","        valid_dataset = YNAT_dataset(args, valid, is_inference)\n","        valid_loader = torch.utils.data.DataLoader(valid_dataset, num_workers=args.num_workers, shuffle=False,\n","                                                   batch_size=args.batch_size, pin_memory=pin_memory)\n","\n","    return train_loader, valid_loader\n","\n","\n","# loss계산하고 parameter update!\n","def compute_loss(preds, targets, args):\n","    \"\"\"\n","    Args :\n","        preds   : (batch_size, max_seq_len)\n","        targets : (batch_size, max_seq_len)\n","    \"\"\"\n","    # print(preds, targets)\n","    loss = get_criterion(preds, targets, args)\n","    # 마지막 시퀀스에 대한 값만 loss 계산\n","    # loss = loss[:, -1]\n","    # loss = torch.mean(loss)\n","    return loss\n","\n","\n","def get_criterion(pred, target, args):\n","    if args.criterion == 'BCE':\n","        loss = nn.BCELoss(reduction=\"none\")\n","    elif args.criterion == \"BCELogit\":\n","        loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n","    elif args.criterion == \"MSE\":\n","        loss = nn.MSELoss(reduction=\"none\")\n","    elif args.criterion == \"L1\":\n","        loss = nn.L1Loss(reduction=\"none\")\n","    elif args.criterion == \"CE\":\n","        #weights = [1,1,2,1,1,1,1] #as class distribution\n","        #class_weights = torch.FloatTensor(weights).cuda()\n","        #loss = nn.CrossEntropyLoss(weight=class_weights)\n","        loss = nn.CrossEntropyLoss()\n","    # NLL, CrossEntropy not available\n","    return loss(pred, target)\n","\n","\n","def make_vocab(args):\n","    print(\"============ READ VOCABS ============\")\n","    vocabs = []\n","    for i in range(7):\n","        vocab = set()\n","        f = open(args.vocab_dir + str(i) + '.txt', 'r')\n","        while True:\n","            line = f.readline()\n","            if not line: break\n","            vocab.add(line[:-1])\n","        f.close()\n","        vocabs.append(vocab)\n","        print(f\"category {i} reading end, size : {len(vocab)}\")\n","    return vocabs"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mLM-aadds6H9"},"source":["## Dataloader"]},{"cell_type":"code","metadata":{"id":"2s9RxMi7rlfb","executionInfo":{"status":"ok","timestamp":1627567957313,"user_tz":-540,"elapsed":3,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import os\n","import torch\n","import pandas as pd\n","\n","\n","class Preprocess:\n","    def __init__(self, args):\n","        self.args = args\n","        self.train_data = None\n","        self.test_data = None\n","\n","    def load_data(self, file_name):\n","        csv_file_name = os.path.join(self.args.data_dir, file_name)\n","        df = pd.read_csv(csv_file_name)\n","        #del df['Unnamed: 0']\n","        return df.values\n","\n","    def load_train_data(self):\n","        self.train_data = self.load_data('train_data.csv')\n","\n","    def load_test_data(self):\n","        self.test_data = self.load_data('test_data.csv')\n","\n","\n","class YNAT_dataset(torch.utils.data.Dataset):\n","    def __init__(self, args, data, is_inference):\n","        self.args = args\n","        self.data = data\n","        self.is_inference = is_inference\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        row = self.data[index]\n","        element = [row[i] for i in range(len(row))]\n","        #print(type(row))\n","        # np.array -> torch.tensor 형변환\n","        #for i, col in enumerate(row):\n","        #    if type(col) == str:\n","        #        pass\n","        #    else:\n","        #        row[i] = torch.tensor(col)\n","\n","        return element\n","\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2g9iLEBtDnJ"},"source":["## Trainer"]},{"cell_type":"code","metadata":{"id":"Xq3sntmNtErb","executionInfo":{"status":"ok","timestamp":1627567958028,"user_tz":-540,"elapsed":718,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["from sklearn.metrics import accuracy_score\n","from torch.nn.functional import one_hot\n","from tqdm import tqdm\n","from sklearn import metrics\n","\n","\n","def run(args, tokenizer, train_data, valid_data, cv_count):\n","    train_loader, valid_loader = get_loaders(args, train_data, valid_data)\n","\n","    # only when using warmup scheduler\n","    # args.total_steps = int(len(train_loader.dataset) / args.batch_size) * args.n_epochs\n","    # args.warmup_steps = int(args.total_steps * args.warmup_ratio)\n","\n","    model = get_model(args)\n","    optimizer = get_optimizer(model, args)\n","    scheduler = get_scheduler(optimizer, args)\n","\n","    best_acc = -1\n","    early_stopping_counter = 0\n","    for epoch in range(args.n_epochs):\n","\n","        print(f\"Start Training: Epoch {epoch + 1}\")\n","\n","        if not args.cv_strategy:\n","            model_name = args.run_name\n","        else:\n","            model_name = f\"{args.run_name.split('.pt')[0]}_{cv_count}.pt\"\n","\n","        # TRAIN\n","        train_acc, train_loss = train(args, model, tokenizer, train_loader, optimizer)\n","\n","        # VALID\n","        acc, val_loss = validate(args, model, tokenizer, valid_loader)\n","\n","        # TODO: model save or early stopping\n","        if args.scheduler == 'plateau':\n","            last_lr = optimizer.param_groups[0]['lr']\n","        else:\n","            last_lr = scheduler.get_last_lr()[0]\n","\n","        print({\"epoch\": epoch, \"train_loss\": train_loss, \"train_acc\": train_acc,\n","                   \"valid_acc\": acc, \"val_loss\": val_loss, \"learning_rate\": last_lr})\n","\n","        if acc > best_acc:\n","            best_acc = acc\n","            # torch.nn.DataParallel로 감싸진 경우 원래의 model을 가져옵니다.\n","            model_to_save = model.module if hasattr(model, 'module') else model\n","            save_checkpoint({\n","                'epoch': epoch + 1,\n","                'state_dict': model_to_save.state_dict(),\n","            },\n","                args.model_dir, model_name,\n","            )\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","            if early_stopping_counter >= args.patience:\n","                print(f'EarlyStopping counter: {early_stopping_counter} out of {args.patience}')\n","                break\n","\n","        # scheduler\n","        if args.scheduler == 'plateau':\n","            scheduler.step(best_acc)\n","        else:\n","            scheduler.step()\n","\n","    return best_acc\n","\n","\n","def inference(args, test_data):\n","    # ckpt_file_names = []\n","    all_fold_preds = []\n","    all_fold_argmax_preds = []\n","\n","    if not args.cv_strategy:\n","        ckpt_file_names = [args.model_name]\n","    else:\n","        ckpt_file_names = [f\"{args.model_name.split('.pt')[0]}_{i + 1}.pt\" for i in range(args.fold_num)]\n","\n","    tokenizer = load_tokenizer(args)\n","\n","    for fold_idx, ckpt in enumerate(ckpt_file_names):\n","        model = load_model(args, ckpt)\n","        model.eval()\n","        test_loader = get_loaders(args, None, test_data, True)\n","\n","        total_preds = []\n","        total_argmax_preds = []\n","        total_ids = []\n","\n","        for step, batch in tqdm(enumerate(test_loader), desc='Inferencing', total=len(test_loader)):\n","            idx, text = batch\n","            tokenized_examples = tokenizer(\n","                text,\n","                max_length=args.max_seq_len,\n","                padding=\"max_length\",\n","                return_tensors=\"pt\"\n","            ).to(args.device)\n","\n","            token_label_type_ids = []\n","            for row in tokenized_examples['input_ids']:\n","                temp = []\n","                row = tokenizer.convert_ids_to_tokens(row)\n","                for token in row:\n","                    for idx, element in enumerate(args.vocab):\n","                        if token in element:\n","                            temp.append(idx+1)\n","                            break\n","                    else:\n","                        temp.append(0)\n","                #print(temp)\n","                token_label_type_ids.append(temp)\n","\n","            token_label_type_ids = torch.tensor(token_label_type_ids).to(args.device)\n","            tokenized_examples['token_label_type_ids'] = token_label_type_ids\n","\n","            preds = model(**tokenized_examples)\n","            logits = preds['logits']\n","            logits = logits[:,0,:]\n","            argmax_logits = torch.argmax(logits, dim=1)\n","\n","            if args.device == 'cuda':\n","                argmax_preds = argmax_logits.to('cpu').detach().numpy()\n","                preds = logits.to('cpu').detach().numpy()\n","                token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","            else:  # cpu\n","                argmax_preds = argmax_logits.detach().numpy()\n","                preds = logits.detach().numpy()\n","                token_label_type_ids = token_label_type_ids.detach().numpy()\n","\n","            total_preds += list(preds)\n","            total_argmax_preds += list(argmax_preds)\n","            total_ids += list(idx)\n","\n","        all_fold_preds.append(total_preds)\n","        all_fold_argmax_preds.append(total_argmax_preds)\n","\n","        output_file_name = \"output.csv\" if not args.cv_strategy else f\"output_{fold_idx + 1}.csv\"\n","        write_path = os.path.join(args.output_dir, output_file_name)\n","        if not os.path.exists(args.output_dir):\n","            os.makedirs(args.output_dir)\n","        with open(write_path, 'w', encoding='utf8') as w:\n","            print(\"writing prediction : {}\".format(write_path))\n","            w.write(\"index,topic_idx\\n\")\n","            for index, p in zip(total_ids, total_argmax_preds):\n","                w.write('{},{}\\n'.format(index, p))\n","\n","    if len(all_fold_preds) > 1:\n","        # Soft voting ensemble\n","        votes = np.sum(all_fold_preds, axis=0)\n","        votes = np.argmax(votes, axis=1)\n","\n","        write_path = os.path.join(args.output_dir, \"output_softvote.csv\")\n","        #write_path = \"output_softvote.csv\"\n","        if not os.path.exists(args.output_dir):\n","            os.makedirs(args.output_dir)\n","        with open(write_path, 'w', encoding='utf8') as w:\n","            print(\"writing prediction : {}\".format(write_path))\n","            w.write(\"index,topic_idx\\n\")\n","            for id, p in zip(total_ids, votes):\n","                w.write('{},{}\\n'.format(id, p))\n","\n","\n","def train(args, model, tokenizer, train_loader, optimizer):\n","    model.train()\n","\n","    total_preds = []\n","    total_targets = []\n","    losses = []\n","    for step, batch in tqdm(enumerate(train_loader), desc='Training', total=len(train_loader)):\n","        idx, text, label = batch\n","        label = label.to(args.device)\n","        # print(idx[:10])\n","        # print(text[:10])\n","        # print(label[:10])\n","        tokenized_examples = tokenizer(\n","            text,\n","            max_length=args.max_seq_len,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        ).to(args.device)\n","        \n","        # tokenize\n","        # 모델의 입력으로\n","        # label은 one-hot?\n","        # loss 주고\n","        # argmax를 golden\n","\n","        token_label_type_ids = []\n","        for row in tokenized_examples['input_ids']:\n","            temp = []\n","            row = tokenizer.convert_ids_to_tokens(row)\n","            for token in row:\n","                for idx, element in enumerate(args.vocab):\n","                    if token in element:\n","                        temp.append(idx+1)\n","                        break\n","                else:\n","                    temp.append(0)\n","            #print(temp)\n","            token_label_type_ids.append(temp)\n","\n","        token_label_type_ids = torch.tensor(token_label_type_ids).to(args.device)\n","        tokenized_examples['token_label_type_ids'] = token_label_type_ids\n","\n","        preds = model(**tokenized_examples)\n","        logits = preds['logits']\n","        logits = logits[:,0,:]\n","        softmax_logits = nn.Softmax(dim=1)(logits)\n","        argmax_logits = torch.argmax(logits, dim=1)\n","\n","        # one_hot_logits = one_hot(argmax_logits, num_classes=7).float()\n","        # print(one_hot(argmax_logits, num_classes=7).type(torch.FloatTensor))\n","        loss = compute_loss(logits,\n","                            label, args)\n","\n","        # print(loss)\n","\n","        update_params(loss, model, optimizer, step, len(train_loader), args)\n","\n","        if step % args.log_steps == 0:\n","            print(f\"Training steps: {step} Loss: {str(loss.item())}\")\n","\n","        if args.device == 'cuda':\n","            argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","            label = label.to('cpu').detach().numpy()\n","            loss = loss.to('cpu').detach().numpy()\n","            token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","        else:  # cpu\n","            argmax_logits = argmax_logits.detach().numpy()\n","            label = label.detach().numpy()\n","            loss = loss.detach().numpy()\n","            token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","\n","        total_preds.append(argmax_logits)\n","        total_targets.append(label)\n","        losses.append(loss)\n","\n","    total_preds = np.concatenate(total_preds)\n","    total_targets = np.concatenate(total_targets)\n","\n","    # Train AUC / ACC\n","    acc = accuracy_score(total_targets, total_preds)\n","    loss_avg = sum(losses) / len(losses)\n","    print(f'TRAIN ACC : {acc}, TRAIN LOSS : {loss_avg}')\n","    return acc, loss_avg\n","\n","\n","def validate(args, model, tokenizer, valid_loader):\n","    model.eval()\n","\n","    total_preds = []\n","    total_targets = []\n","    losses = []\n","    for step, batch in tqdm(enumerate(valid_loader), desc='Training', total=len(valid_loader)):\n","        idx, text, label = batch\n","        label = label.to(args.device)\n","        tokenized_examples = tokenizer(\n","            text,\n","            max_length=args.max_seq_len,\n","            padding=\"max_length\",\n","            return_tensors=\"pt\"\n","        ).to(args.device)\n","\n","        # tokenize\n","        # 모델의 입력으로\n","        # label은 one-hot?\n","        # loss 주고\n","        # argmax를 golden\n","        token_label_type_ids = []\n","        for row in tokenized_examples['input_ids']:\n","            temp = []\n","            row = tokenizer.convert_ids_to_tokens(row)\n","            for token in row:\n","                for idx, element in enumerate(args.vocab):\n","                    if token in element:\n","                        temp.append(idx+1)\n","                        break\n","                else:\n","                    temp.append(0)\n","            #print(temp)\n","            token_label_type_ids.append(temp)\n","\n","        token_label_type_ids = torch.tensor(token_label_type_ids).to(args.device)\n","        tokenized_examples['token_label_type_ids'] = token_label_type_ids\n","\n","        preds = model(**tokenized_examples)\n","        logits = preds['logits']\n","        logits = logits[:,0,:]\n","        softmax_logits = nn.Softmax(dim=1)(logits)\n","        argmax_logits = torch.argmax(logits, dim=1)\n","\n","        # one_hot_logits = one_hot(argmax_logits, num_classes=7).float()\n","        # print(one_hot(argmax_logits, num_classes=7).type(torch.FloatTensor))\n","        loss = compute_loss(logits,\n","                            label, args)\n","\n","        if step % args.log_steps == 0:\n","            print(f\"Validation steps: {step} Loss: {str(loss.item())}\")\n","\n","        if args.device == 'cuda':\n","            argmax_logits = argmax_logits.to('cpu').detach().numpy()\n","            label = label.to('cpu').detach().numpy()\n","            loss = loss.to('cpu').detach().numpy()\n","            token_label_type_ids = token_label_type_ids.to('cpu').detach().numpy()\n","        else:  # cpu\n","            argmax_logits = argmax_logits.detach().numpy()\n","            label = label.detach().numpy()\n","            loss = loss.detach().numpy()\n","            token_label_type_ids = token_label_type_ids.detach().numpy()\n","\n","        total_preds.append(argmax_logits)\n","        total_targets.append(label)\n","        losses.append(loss)\n","\n","    total_preds = np.concatenate(total_preds)\n","    total_targets = np.concatenate(total_targets)\n","\n","    # Train AUC / ACC\n","    target_names = ['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치']\n","    print(metrics.classification_report(total_targets, total_preds, target_names=target_names))\n","    matrix = metrics.confusion_matrix(total_targets, total_preds)\n","    print(matrix.diagonal()/matrix.sum(axis=1))\n","\n","    acc = accuracy_score(total_targets, total_preds)\n","    loss_avg = sum(losses) / len(losses)\n","    print(f'VALID ACC : {acc}, VALID LOSS : {loss_avg}')\n","    return acc, loss_avg\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5CS14MP9tFCQ"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"K9DdutwStF4B","executionInfo":{"status":"ok","timestamp":1627567958514,"user_tz":-540,"elapsed":488,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import torch\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","from datetime import datetime\n","from pytz import timezone\n","\n","\n","def main(args):\n","    if not args.run_name:\n","        args.run_name = datetime.now(timezone(\"Asia/Seoul\")).strftime(\"%Y-%m-%d-%H:%M:%S\")\n","\n","    set_seeds(args.seed)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    args.device = device\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.tokenizer_name\n","        if args.tokenizer_name\n","        else args.model_name_or_path,\n","        use_fast=True,\n","    )\n","\n","    preprocess = Preprocess(args)\n","    preprocess.load_train_data()\n","    train_data_origin = preprocess.train_data\n","\n","    print(f\"Size of train data : {len(train_data_origin)}\")\n","    # print(f\"size of test data : {len(test_data)}\")\n","\n","    if args.cv_strategy == 'random':\n","        kf = KFold(n_splits=args.fold_num, shuffle=True)\n","        splits = kf.split(X=train_data_origin)\n","    else:\n","        # default\n","        # 여기 각 label로 바꿔야됨\n","        train_labels = [sequence[-1] for sequence in train_data_origin]\n","        skf = StratifiedKFold(n_splits=args.fold_num, shuffle=True)\n","        splits = skf.split(X=train_data_origin, y=train_labels)\n","\n","    acc_avg = 0\n","    for fold_num, (train_index, valid_index) in enumerate(splits):\n","        train_data = train_data_origin[train_index]\n","        valid_data = train_data_origin[valid_index]\n","        best_acc = run(args, tokenizer, train_data, valid_data, fold_num + 1)\n","\n","        if not args.cv_strategy:\n","            break\n","\n","        acc_avg += best_acc\n","\n","    if args.cv_strategy:\n","        acc_avg /= args.fold_num\n","\n","        print(\"*\" * 50, 'auc_avg', \"*\" * 50)\n","        print(acc_avg)\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1EgudubtKjJ"},"source":["## Run"]},{"cell_type":"code","metadata":{"id":"_xupDv9YtKGf","executionInfo":{"status":"ok","timestamp":1627568026545,"user_tz":-540,"elapsed":498,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["import argparse\n","import easydict\n","\n","def parse_args():\n","    args = easydict.EasyDict({'run_name' : 'temp',\n","                             'seed':42,\n","                             'device' :'cuda',\n","                             'data_dir': cur_dir + '/data/open/',\n","                             'model_dir' : '/content/drive/MyDrive/KLUE_TC/models/',\n","                             'transformers_dir' : '/content/drive/MyDrive/KLUE_TC/transformers/',\n","                             'model_name_or_path' : 'klue/roberta-large',\n","                             'config_name' : None,\n","                             'tokenizer_name' : None,\n","                             'output_dir' : '/content/drive/MyDrive/KLUE_TC/output/0729',\n","                             'vocab_dir' : '/content/drive/MyDrive/KLUE_TC/data/vocab/',\n","                             \n","                             'accum_iter' : 8,\n","                             'gradient_accumulation' : True,\n","\n","                             'cv_strategy' : 'stratified',\n","                             'fold_num' : 2,\n","\n","                             'num_workers' : 1,\n","\n","                             # 훈련\n","                             'n_epochs' : 5,\n","                             'batch_size' : 32,\n","                             'lr' : 5e-6,\n","                             'clip_grad' : 15,\n","                             'patience' : 5,\n","                             'max_seq_len' : 40,\n","\n","                             # Optimizer\n","                             'optimizer' : 'adamP',\n","\n","                             # Optimizer-parameters\n","                             'weight_decay' : 0.05,\n","                             'momentum' : 0.9,\n","\n","                             # Scheduler\n","                             'scheduler' : 'step_lr',\n","\n","                             # Scheduler-parameters\n","                             # plateau\n","                             'plateau_patience' : 10,\n","                             'plateau_factor' : 0.5,\n","                              \n","                             't_max' : 10,\n","                             'T_0' : 10,\n","                             'T_mult' : 2,\n","                             '--eta_min' : 0.01,\n","\n","                             # linear_warmup\n","                             'warmup_ratio' : 0.3,\n","\n","                             # Step LR\n","                             'step_size' : 50,\n","                             'gamma' : 0.1,\n","\n","                             'criterion' : 'CE',\n","\n","                             'log_steps' : 100})\n","    \n","    return args"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"YKuE65Ct3d34","executionInfo":{"status":"error","timestamp":1627567965363,"user_tz":-540,"elapsed":6852,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"7550801b-2dcb-45d5-8d1d-cdd19726e26e"},"source":["if __name__ == '__main__':\n","    args = parse_args()\n","    args['vocab'] = make_vocab(args)\n","    main(args)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["============ READ VOCABS ============\n","category 0 reading end, size : 313\n","category 1 reading end, size : 312\n","category 2 reading end, size : 393\n","category 3 reading end, size : 435\n","category 4 reading end, size : 374\n","category 5 reading end, size : 354\n","category 6 reading end, size : 304\n","Size of train data : 45654\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.embeddings.token_label_type_embeddings.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["Start Training: Epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","Training:   0%|          | 0/1071 [00:00<?, ?it/s]\u001b[A\u001b[A\n","\n","Training:   0%|          | 1/1071 [00:00<08:18,  2.14it/s]\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Training steps: 0 Loss: 1.9813740253448486\n"],"name":"stdout"},{"output_type":"stream","text":["\rTraining:   0%|          | 1/1071 [00:00<12:52,  1.39it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-ba7420a4f986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-104db43ca62c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_origin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data_origin\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-df36d92143c0>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args, tokenizer, train_data, valid_data, cv_count)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# TRAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# VALID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-df36d92143c0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, tokenizer, train_loader, optimizer)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-4866b5dd771d>\u001b[0m in \u001b[0;36mupdate_params\u001b[0;34m(loss, model, optimizer, batch_idx, max_len, args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# weights update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.90 GiB total capacity; 14.93 GiB already allocated; 27.75 MiB free; 14.97 GiB reserved in total by PyTorch)"]}]},{"cell_type":"code","metadata":{"id":"YfVIY2z0kZZx","executionInfo":{"status":"aborted","timestamp":1627567965360,"user_tz":-540,"elapsed":11,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}}},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txpJN5vFPWCq"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"hUWUggp_PQqy","colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"status":"error","timestamp":1627568037813,"user_tz":-540,"elapsed":4801,"user":{"displayName":"정근영","photoUrl":"","userId":"04776964382205030605"}},"outputId":"3a5e04ed-fec4-48db-fe6b-2a8787902a56"},"source":["def inference_main():\n","    args = parse_args()\n","    args.model_name = \"temp\"\n","    preprocess = Preprocess(args)\n","    preprocess.load_test_data()\n","    test_data = preprocess.test_data\n","\n","    print(f\"size of test data : {len(test_data)}\")\n","    torch.cuda.empty_cache()\n","    # del model\n","    inference(args, test_data)\n","\n","inference_main()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["size of test data : 9131\n","Loading Model from: /content/drive/MyDrive/KLUE_TC/models/temp_1.pt\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-c8c4afe5f9b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0minference_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-18-c8c4afe5f9b7>\u001b[0m in \u001b[0;36minference_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# del model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0minference_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-df36d92143c0>\u001b[0m in \u001b[0;36minference\u001b[0;34m(args, test_data)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_file_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-4866b5dd771d>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(args, model_name)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading Model from:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mload_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;31m#load_state = torch.load(model_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;31m# del _CudaBase.__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 15.90 GiB total capacity; 14.91 GiB already allocated; 53.75 MiB free; 14.94 GiB reserved in total by PyTorch)"]}]}]}